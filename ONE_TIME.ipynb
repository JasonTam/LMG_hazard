{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 580\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/afs/ee.cooper.edu/user/t/a/tam8/documents/ml_misc/semi_supervised/')\n",
    "import coreg\n",
    "reload(coreg)\n",
    "import rasco\n",
    "reload(rasco)\n",
    "import trireg\n",
    "reload(trireg)\n",
    "sys.path.append('/afs/ee.cooper.edu/user/t/a/tam8/documents/ml_misc/ordinal/')\n",
    "import simple\n",
    "reload(simple)\n",
    "sys.path.append('/afs/ee.cooper.edu/user/t/a/tam8/documents/ml_misc/ensemble/')\n",
    "import stacking\n",
    "reload(stacking)\n",
    "\n",
    "sys.path.append('/afs/ee.cooper.edu/user/t/a/tam8/documents/ml_misc/neighbors/')\n",
    "import rpfnn\n",
    "reload(rpfnn)\n",
    "import ann\n",
    "reload(ann)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "\n",
    "import transformers as tforms\n",
    "reload(tforms)\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion, make_union\n",
    "\n",
    "import metrics\n",
    "reload(metrics)\n",
    "from sklearn.cross_validation import StratifiedKFold, train_test_split\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.svm import SVC, LinearSVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet, Ridge, Lasso, SGDRegressor, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier, DecisionTreeClassifier, DecisionTreeRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, MaxoutDense, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from keras.optimizers import Adadelta, Adagrad, Adam, RMSprop\n",
    "from keras.layers.advanced_activations import ParametricSoftplus, PReLU\n",
    "\n",
    "from svmlight_loader import dump_svmlight_file, load_svmlight_file\n",
    "\n",
    "from collections import Counter\n",
    "import minirank as mr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from time import time\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "def wgmean(x, w):\n",
    "    return np.exp(np.sum(w*np.log(x), axis=1) / np.sum(w, axis=1))\n",
    "\n",
    "import logging \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data le instead\n",
    "# train_pd  = pd.read_pickle('saved/train_pd_l_enc.p')\n",
    "# test_pd  = pd.read_pickle('saved/test_pd_l_enc.p')\n",
    "\n",
    "train_pd  = pd.read_pickle('saved/train_pd_le_and_oh_enc.p')\n",
    "test_pd  = pd.read_pickle('saved/test_pd_le_and_oh_enc.p')\n",
    "\n",
    "labels = pd.read_pickle('saved/labels.p')\n",
    "test_ind = pickle.load(open('saved/test_ind.p'))\n",
    "y_binned = np.load('saved/y_binned.npy')\n",
    "\n",
    "# drop_cols = ['T1_V10', 'T1_V13', 'T2_V7', 'T2_V10']\n",
    "drop_cols = []\n",
    "# Lets not drop anything\n",
    "\n",
    "# drop_cols = train_pd.columns[fi < 0.01]\n",
    "\n",
    "for col in drop_cols:\n",
    "    train_pd.drop(col, axis=1, inplace=True)\n",
    "    test_pd.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50999, 2)\n",
      "(51000, 2)\n",
      "(50999, 2)\n",
      "(51000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_tsne2_26 = np.load('saved/X_tsne2_26important.npy')\n",
    "X_tsne2_26_train = X_tsne2_26[:len(train_pd), :]\n",
    "X_tsne2_26_test = X_tsne2_26[-len(test_pd):, :]\n",
    "\n",
    "X_tsne2 = np.load('saved/X_tsne2.npy')\n",
    "X_tsne2_train = X_tsne2[:len(train_pd), :]\n",
    "X_tsne2_test = X_tsne2[-len(test_pd):, :]\n",
    "\n",
    "print X_tsne2_train.shape\n",
    "print X_tsne2_test.shape\n",
    "print X_tsne2_26_train.shape\n",
    "print X_tsne2_26_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50999, 2000)\n",
      "(51000, 2000)\n"
     ]
    }
   ],
   "source": [
    "# mapped_train, _y = load_svmlight_file('saved/mapped2000_train.libsvm')\n",
    "# mapped_test, _y = load_svmlight_file('saved/mapped2000_test.libsvm')\n",
    "\n",
    "mapped_train, _y = load_svmlight_file('saved/mapped2000_train.libsvm')\n",
    "mapped_test, _y = load_svmlight_file('saved/mapped2000_test.libsvm')\n",
    "\n",
    "X_2000mean_train = mapped_train.todense()\n",
    "X_2000mean_test = mapped_test.todense()\n",
    "print X_2000mean_train.shape\n",
    "print X_2000mean_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (50999,)\n",
      "X_train (50999, 129)\n",
      "X_test (51000, 129)\n",
      "X_hold 50\n",
      "5\n",
      "(50999,)\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "train = np.array(train_pd)\n",
    "test = np.array(test_pd)\n",
    "\n",
    "X_train = train.astype(float)\n",
    "X_test = test.astype(float)\n",
    "y_train = np.array(labels)\n",
    "\n",
    "try:\n",
    "    X_train = np.c_[X_train, X_tsne2_26_train]\n",
    "    X_test = np.c_[X_test, X_tsne2_26_test]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    X_train = np.c_[X_train, X_2000mean]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "holdout = False\n",
    "if holdout:\n",
    "    X_train, X_hold, \\\n",
    "    y_train, y_hold, \\\n",
    "    y_binned, y_binned_hold \\\n",
    "    = train_test_split(\n",
    "        X_train, y_train, y_binned, \n",
    "        test_size=0.2, random_state=0)\n",
    "\n",
    "# \"\"\"\n",
    "pipe_x = make_pipeline(\n",
    "    make_union(\n",
    "        tforms.IdentityTformer(),\n",
    "    ),\n",
    "#     StandardScaler(),\n",
    ")\n",
    "pipe_y = make_pipeline(\n",
    "    tforms.IdentityTformer(),    \n",
    ")\n",
    "pipe_x.fit(np.r_[X_train, X_test])\n",
    "pipe_y.fit(y_train)\n",
    "\n",
    "X_train = pipe_x.transform(X_train)\n",
    "X_test = pipe_x.transform(X_test)\n",
    "try:\n",
    "    X_hold = pipe_x.transform(X_hold)\n",
    "except:\n",
    "    pass\n",
    "# y_train = pipe_y.fit_transform(y_train)\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "print 'y_train', y_train.shape\n",
    "print 'X_train', X_train.shape\n",
    "print 'X_test', X_test.shape\n",
    "try:\n",
    "    print 'X_hold', X_hold.shape\n",
    "except:\n",
    "    pass\n",
    "print len(np.unique(y_train))\n",
    "print len(np.unique(y_binned))\n",
    "print y_binned.shape\n",
    "print type(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50999, 2129)\n",
      "(51000, 2129)\n"
     ]
    }
   ],
   "source": [
    "X_nn_train = np.c_[X_2000mean_train, X_train]\n",
    "X_nn_test = np.c_[X_2000mean_test, X_test]\n",
    "print X_nn_train.shape\n",
    "print X_nn_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFINE THE MODELS\n",
    "## BASE MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN (using sofia kmeans cluster distance maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_feats = X_nn_train.shape[1]\n",
    "drop_prob = 0.6\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "hidden_size = 1024\n",
    "model.add(Dense(n_feats, hidden_size))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "model.add(BatchNormalization((hidden_size,)))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, hidden_size))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "model.add(BatchNormalization((hidden_size,)))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, hidden_size))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "model.add(BatchNormalization((hidden_size,)))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, hidden_size))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "model.add(BatchNormalization((hidden_size,)))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, 1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "loss_type = 'msle'\n",
    "\n",
    "opt = RMSprop(lr=0.0005, rho=0.75, epsilon=1e-6)\n",
    "\n",
    "nn_reg = KerasRegressor(model, optimizer=opt, loss=loss_type,\n",
    "                        train_batch_size=1024*4, test_batch_size=1024*16,\n",
    "                        nb_epoch=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBM XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbm_reg = xgb.XGBRegressor(\n",
    "    objective=\"reg:linear\",\n",
    "    n_estimators=900,\n",
    "    learning_rate=0.005,\n",
    "#     gamma=0.0,\n",
    "    max_depth=9,\n",
    "    min_child_weight=6,\n",
    "#     max_delta_step=10,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "#     scale_pos_weight=1.0,\n",
    "    base_score=0.5,\n",
    "    nthread=7,\n",
    "    seed=322,\n",
    "    silent=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_reg = RandomForestRegressor(n_jobs=-1, **{'max_leaf_nodes': None, \n",
    "                                         'bootstrap': False, \n",
    "                                         'min_samples_leaf': 8, \n",
    "                                         'n_estimators': 150, \n",
    "                                         'min_samples_split': 8, \n",
    "                                         'min_weight_fraction_leaf': 0.25, \n",
    "                                         'max_features': 'sqrt', \n",
    "                                         'max_depth': 12} )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other crappy regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_reg = LinearRegression()\n",
    "ridge_reg = Ridge()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom weird stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rasco:RASCO Init\n"
     ]
    }
   ],
   "source": [
    "h = [GaussianNB() for _ in range(64)] + \\\n",
    "    [DecisionTreeClassifier(max_depth=6, min_samples_split=5, class_weight='auto') for _ in range(16)] + \\\n",
    "    [SGDClassifier(n_iter=20, loss='modified_huber', class_weight='auto', n_jobs=1) \n",
    "     for _ in range(8)] + \\\n",
    "    [ExtraTreeClassifier(max_depth=6, min_samples_split=6, class_weight='auto')\n",
    "     for _ in range(16)]\n",
    "\n",
    "\n",
    "rasco_mix = rasco.Rasco(h,\n",
    "                        feat_ratio=0.6,\n",
    "                       n_estimators=104, max_iters=100,\n",
    "                       verbose=True,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "ord_rasco = simple.SimpleOrdinalClassifier(rasco_mix, n_jobs=1)\n",
    "\n",
    "ord_gaussnb = simple.SimpleOrdinalClassifier(GaussianNB(), n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## META MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NN META"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_feats = 15\n",
    "drop_prob = 0.5\n",
    "\n",
    "model_meta = Sequential()\n",
    "hidden_size = 128\n",
    "model_meta.add(Dense(n_feats, hidden_size))\n",
    "model_meta.add(Dropout(0.2))\n",
    "model_meta.add(ParametricSoftplus(hidden_size))\n",
    "model_meta.add(BatchNormalization((hidden_size,)))\n",
    "model_meta.add(Dropout(drop_prob))\n",
    "model_meta.add(Dense(hidden_size, hidden_size))\n",
    "model_meta.add(ParametricSoftplus(hidden_size))\n",
    "model_meta.add(BatchNormalization((hidden_size,)))\n",
    "model_meta.add(Dropout(drop_prob))\n",
    "model_meta.add(Dense(hidden_size, hidden_size))\n",
    "model_meta.add(ParametricSoftplus(hidden_size))\n",
    "model_meta.add(BatchNormalization((hidden_size,)))\n",
    "model_meta.add(Dropout(drop_prob))\n",
    "model_meta.add(Dense(hidden_size, 1))\n",
    "model_meta.add(Activation('linear'))\n",
    "\n",
    "loss_type_meta = 'msle'\n",
    "\n",
    "opt_meta = RMSprop(lr=0.0005, rho=0.75, epsilon=1e-6)\n",
    "\n",
    "nn_meta = KerasRegressor(model=model_meta, optimizer=opt_meta, loss=loss_type_meta,\n",
    "                        train_batch_size=1024*4, test_batch_size=1024*16,\n",
    "                        nb_epoch=210)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THE STACK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-15-9ca68bd87784>, line 27)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-15-9ca68bd87784>\"\u001b[1;36m, line \u001b[1;32m27\u001b[0m\n\u001b[1;33m    'y_binned':, y_binned,\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "fh = logging.FileHandler('/tmp/lmg.log')\n",
    "\n",
    "stack = stacking.Stacking([\n",
    "        #            Ind   Output Dim\n",
    "        nn_reg,      # 0     1\n",
    "        gbm_reg,     # 1     1\n",
    "        rf_reg,      # 2     1\n",
    "        lin_reg,     # 4     1\n",
    "        ridge_reg,   # 5     1\n",
    "        ord_gaussnb, # 6     5\n",
    "        ord_rasco,   # 7     5\n",
    "    ],\n",
    "                          nn_meta,\n",
    "#                           meta_gbm,\n",
    "#                           LinearRegression(),\n",
    "                          fit_params={\n",
    "        'base0_X': 'X_nn_train',\n",
    "        'base6_y': 'y_binned',\n",
    "        'base7_y': 'y_binned',\n",
    "    },\n",
    "                          pred_params={\n",
    "        'base0_X': 'X_nn_test',\n",
    "    },\n",
    "                          extra_data={\n",
    "        'X_nn_train': X_nn_train,\n",
    "        'X_nn_test': X_nn_test,\n",
    "        'y_binned': y_binned,\n",
    "    },\n",
    "                          include_orig_feats=False,\n",
    "                          use_probs=True,\n",
    "                          cv=8,\n",
    "                          verbose=1,\n",
    "                          save_level0_out=True,\n",
    "                          log_handler=fh,\n",
    "                         )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
