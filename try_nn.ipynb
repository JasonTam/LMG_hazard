{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/afs/ee.cooper.edu/user/t/a/tam8/documents/ml_misc/semi_supervised/')\n",
    "import coreg\n",
    "reload(coreg)\n",
    "import trireg\n",
    "reload(trireg)\n",
    "sys.path.append('/afs/ee.cooper.edu/user/t/a/tam8/documents/ml_misc/ordinal/')\n",
    "import simple\n",
    "reload(simple)\n",
    "sys.path.append('/afs/ee.cooper.edu/user/t/a/tam8/documents/ml_misc/ensemble/')\n",
    "import stacking\n",
    "reload(stacking)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "\n",
    "import transformers as tforms\n",
    "reload(tforms)\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion, make_union\n",
    "\n",
    "import metrics\n",
    "reload(metrics)\n",
    "from sklearn.cross_validation import StratifiedKFold, train_test_split\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet, Ridge, Lasso, SGDRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from collections import Counter\n",
    "import minirank as mr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from time import time\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "def wgmean(x, w):\n",
    "    return np.exp(np.sum(w*np.log(x), axis=1) / np.sum(w, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data le instead\n",
    "\n",
    "# train_pd  = pd.read_pickle('saved/train_pd_l_enc.p')\n",
    "# test_pd  = pd.read_pickle('saved/test_pd_l_enc.p')\n",
    "\n",
    "train_pd  = pd.read_pickle('saved/train_pd_le_and_oh_enc.p')\n",
    "test_pd  = pd.read_pickle('saved/test_pd_le_and_oh_enc.p')\n",
    "\n",
    "labels = pd.read_pickle('saved/labels.p')\n",
    "test_ind = pickle.load(open('saved/test_ind.p'))\n",
    "y_binned = np.load('saved/y_binned.npy')\n",
    "fi = np.load('saved/feature_importances.npy')\n",
    "\n",
    "y_binned[y_binned==6] = 5\n",
    "\n",
    "drop_cols = ['T1_V10', 'T1_V13', 'T2_V7', 'T2_V10']\n",
    "# drop_cols = []\n",
    "\n",
    "# drop_cols = train_pd.columns[fi < 0.01]\n",
    "\n",
    "\n",
    "for col in drop_cols:\n",
    "    train_pd.drop(col, axis=1, inplace=True)\n",
    "    test_pd.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import dump_svmlight_file, load_svmlight_file\n",
    "mapped_train, _y = load_svmlight_file('saved/mapped2000_train.libsvm')\n",
    "\n",
    "X_2000mean = mapped_train.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data binary instead\n",
    "train_pd  = pd.read_pickle('saved/train_pd_binary_enc.p')\n",
    "test_pd  = pd.read_pickle('saved/test_pd_binary_enc.p')\n",
    "labels = pd.read_pickle('saved/labels.p')\n",
    "test_ind = pickle.load(open('saved/test_ind.p'))\n",
    "y_binned = np.load('saved/y_binned.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50999, 2)\n",
      "(51000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_tsne2 = np.load('saved/X_tsne2_26important.npy')\n",
    "X_tsne2_train = X_tsne2[:len(train_pd), :]\n",
    "X_tsne2_test = X_tsne2[-len(test_pd):, :]\n",
    "\n",
    "print X_tsne2_train.shape\n",
    "print X_tsne2_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (50999,)\n",
      "X_train (50999, 125)\n",
      "X_test (51000, 125)\n",
      "X_hold 50\n",
      "5\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "y_binned[y_binned==6] = 5\n",
    "\n",
    "train = np.array(train_pd)\n",
    "test = np.array(test_pd)\n",
    "\n",
    "X_train = train.astype(float)\n",
    "X_test = test.astype(float)\n",
    "y_train = np.array(labels)\n",
    "\n",
    "try:\n",
    "    X_train = np.c_[X_train, X_tsne2_train]\n",
    "    X_test = np.c_[X_test, X_tsne2_test]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "holdout = False\n",
    "if holdout:\n",
    "    X_train, X_hold, \\\n",
    "    y_train, y_hold, \\\n",
    "    y_binned, y_binned_hold \\\n",
    "    = train_test_split(\n",
    "        X_train, y_train, y_binned, \n",
    "        test_size=0.2, random_state=0)\n",
    "\n",
    "\n",
    "pipe_x = make_pipeline(\n",
    "    make_union(\n",
    "        tforms.IdentityTformer(),\n",
    "#         make_pipeline(AddTformer(1), BoxCoxTformer()),\n",
    "#         AnscombeTformer(),\n",
    "    ),\n",
    "    StandardScaler(),\n",
    ")\n",
    "pipe_y = make_pipeline(\n",
    "#     tforms.IdentityTformer(),\n",
    "#     tforms.BoxCoxTformer(),\n",
    "#     tforms.LogTfortforms.mer(),\n",
    "#     tforms.AnscombeTformer(),\n",
    "#     tforms.FreemanTukeyTformer(),\n",
    "#     tforms.ArcsinhTformer(),\n",
    "    StandardScaler(),\n",
    "    \n",
    ")\n",
    "pipe_x.fit(np.r_[X_train, X_test])\n",
    "\n",
    "\n",
    "X_train = pipe_x.transform(X_train)\n",
    "X_test = pipe_x.transform(X_test)\n",
    "try:\n",
    "    X_hold = pipe_x.transform(X_hold)\n",
    "except:\n",
    "    pass\n",
    "# y_train = pipe_y.fit_transform(y_train)\n",
    "\n",
    "# small_n = 5000\n",
    "# X_train = X_train[:small_n,:]\n",
    "# y_train = y_train[:small_n]\n",
    "# y_binned = y_binned[:small_n]\n",
    "\n",
    "print 'y_train', y_train.shape\n",
    "print 'X_train', X_train.shape\n",
    "print 'X_test', X_test.shape\n",
    "try:\n",
    "    print 'X_hold', X_hold.shape\n",
    "except:\n",
    "    pass\n",
    "print len(np.unique(y_train))\n",
    "print len(np.unique(y_binned))\n",
    "print type(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 580\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, MaxoutDense, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.optimizers import Adadelta, Adagrad, Adam, RMSprop\n",
    "from keras.layers.advanced_activations import ParametricSoftplus, PReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50999, 2125)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.c_[X_2000mean, X_train]\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# n_feats = X_train.shape[1]\n",
    "n_feats = X.shape[1]\n",
    "drop_prob = 0.6\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# model.add(Dense(111, 512))\n",
    "# model.add(Dropout(0.2))\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(BatchNormalization(512))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(512, 512))\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(BatchNormalization(512))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(512, 512))\n",
    "# model.add(Activation('relu'))\n",
    "# # model.add(BatchNormalization(512))\n",
    "# model.add(Dropout(0.5))\n",
    "# model.add(Dense(512, 1))\n",
    "\n",
    "hidden_size = 1024\n",
    "model.add(Dense(n_feats, hidden_size))\n",
    "model.add(Dropout(0.2))\n",
    "# model.add(Activation('hard_sigmoid'))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(PReLU(hidden_size))\n",
    "# model.add(MaxoutDense(hidden_size, hidden_size, 2))\n",
    "model.add(BatchNormalization(hidden_size))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, hidden_size))\n",
    "# model.add(Activation('hard_sigmoid'))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(PReLU(hidden_size))\n",
    "# model.add(MaxoutDense(hidden_size, hidden_size, 2))\n",
    "model.add(BatchNormalization(hidden_size))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, hidden_size))\n",
    "# model.add(Activation('hard_sigmoid'))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(PReLU(hidden_size))\n",
    "# model.add(MaxoutDense(hidden_size, hidden_size, 2))\n",
    "model.add(BatchNormalization(hidden_size))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, hidden_size))\n",
    "# model.add(Activation('hard_sigmoid'))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(PReLU(hidden_size))\n",
    "# model.add(MaxoutDense(hidden_size, hidden_size, 2))\n",
    "model.add(BatchNormalization(hidden_size))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, 1))\n",
    "\n",
    "# loss_type = 'mean_squared_error'\n",
    "# loss_type = 'mae\n",
    "loss_type = 'msle'\n",
    "\n",
    "\n",
    "# opt = Adadelta(lr=1.0, rho=0.95, epsilon=1e-6)\n",
    "# opt = Adadelta(lr=0.8, rho=0.90, epsilon=1e-6)\n",
    "# opt = Adagrad(lr=0.01, epsilon=1e-6)\n",
    "# opt = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-8, kappa=1-1e-8)\n",
    "opt = RMSprop(lr=0.0005, rho=0.75, epsilon=1e-6)\n",
    "\n",
    "# loss_type = 'mean_absolute_error'\n",
    "# model.compile(loss=loss_type, optimizer='rmsprop')\n",
    "\n",
    "# model.fit(X_train, y_train, nb_epoch=20, batch_size=1024)\n",
    "# score = model.evaluate(X_test, y_test, batch_size=1024)\n",
    "\n",
    "model.compile(loss=loss_type, optimizer=opt)\n",
    "model.save_weights('saved/nn_weights', overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0Train on 40797 samples, validate on 10202 samples\n",
      "Epoch 0\n",
      "1s - loss: 2.2885 - val_loss: 2.2418\n",
      "Epoch 1\n",
      "1s - loss: 2.2260 - val_loss: 2.1780\n",
      "Epoch 2\n",
      "1s - loss: 2.1665 - val_loss: 2.1189\n",
      "Epoch 3\n",
      "1s - loss: 2.1097 - val_loss: 2.0632\n",
      "Epoch 4\n",
      "1s - loss: 2.0547 - val_loss: 2.0098\n",
      "Epoch 5\n",
      "1s - loss: 2.0011 - val_loss: 1.9570\n",
      "Epoch 6\n",
      "1s - loss: 1.9475 - val_loss: 1.9029\n",
      "Epoch 7\n",
      "1s - loss: 1.8949 - val_loss: 1.8519\n",
      "Epoch 8\n",
      "1s - loss: 1.8421 - val_loss: 1.7994\n",
      "Epoch 9\n",
      "1s - loss: 1.7899 - val_loss: 1.7478\n",
      "Epoch 10\n",
      "1s - loss: 1.7379 - val_loss: 1.6963\n",
      "Epoch 11\n",
      "1s - loss: 1.6863 - val_loss: 1.6452\n",
      "Epoch 12\n",
      "1s - loss: 1.6367 - val_loss: 1.5956\n",
      "Epoch 13\n",
      "1s - loss: 1.5866 - val_loss: 1.5470\n",
      "Epoch 14\n",
      "1s - loss: 1.5375 - val_loss: 1.4990\n",
      "Epoch 15\n",
      "1s - loss: 1.4884 - val_loss: 1.4513\n",
      "Epoch 16\n",
      "1s - loss: 1.4420 - val_loss: 1.4067\n",
      "Epoch 17\n",
      "1s - loss: 1.3957 - val_loss: 1.3609\n",
      "Epoch 18\n",
      "1s - loss: 1.3503 - val_loss: 1.3153\n",
      "Epoch 19\n",
      "1s - loss: 1.3055 - val_loss: 1.2713\n",
      "Epoch 20\n",
      "1s - loss: 1.2625 - val_loss: 1.2293\n",
      "Epoch 21\n",
      "1s - loss: 1.2199 - val_loss: 1.1904\n",
      "Epoch 22\n",
      "1s - loss: 1.1797 - val_loss: 1.1484\n",
      "Epoch 23\n",
      "1s - loss: 1.1396 - val_loss: 1.1090\n",
      "Epoch 24\n",
      "1s - loss: 1.1012 - val_loss: 1.0717\n",
      "Epoch 25\n",
      "1s - loss: 1.0626 - val_loss: 1.0347\n",
      "Epoch 26\n",
      "1s - loss: 1.0273 - val_loss: 1.0002\n",
      "Epoch 27\n",
      "1s - loss: 0.9914 - val_loss: 0.9655\n",
      "Epoch 28\n",
      "1s - loss: 0.9574 - val_loss: 0.9328\n",
      "Epoch 29\n",
      "1s - loss: 0.9247 - val_loss: 0.9001\n",
      "Epoch 30\n",
      "1s - loss: 0.8926 - val_loss: 0.8687\n",
      "Epoch 31\n",
      "1s - loss: 0.8625 - val_loss: 0.8385\n",
      "Epoch 32\n",
      "1s - loss: 0.8331 - val_loss: 0.8110\n",
      "Epoch 33\n",
      "1s - loss: 0.8038 - val_loss: 0.7828\n",
      "Epoch 34\n",
      "1s - loss: 0.7764 - val_loss: 0.7565\n",
      "Epoch 35\n",
      "1s - loss: 0.7508 - val_loss: 0.7314\n",
      "Epoch 36\n",
      "1s - loss: 0.7260 - val_loss: 0.7062\n",
      "Epoch 37\n",
      "1s - loss: 0.7017 - val_loss: 0.6839\n",
      "Epoch 38\n",
      "1s - loss: 0.6792 - val_loss: 0.6618\n",
      "Epoch 39\n",
      "1s - loss: 0.6581 - val_loss: 0.6405\n",
      "Epoch 40\n",
      "1s - loss: 0.6368 - val_loss: 0.6207\n",
      "Epoch 41\n",
      "1s - loss: 0.6172 - val_loss: 0.6020\n",
      "Epoch 42\n",
      "1s - loss: 0.5985 - val_loss: 0.5839\n",
      "Epoch 43\n",
      "1s - loss: 0.5811 - val_loss: 0.5666\n",
      "Epoch 44\n",
      "1s - loss: 0.5644 - val_loss: 0.5514\n",
      "Epoch 45\n",
      "1s - loss: 0.5488 - val_loss: 0.5366\n",
      "Epoch 46\n",
      "1s - loss: 0.5337 - val_loss: 0.5224\n",
      "Epoch 47\n",
      "1s - loss: 0.5197 - val_loss: 0.5083\n",
      "Epoch 48\n",
      "1s - loss: 0.5077 - val_loss: 0.4964\n",
      "Epoch 49\n",
      "1s - loss: 0.4957 - val_loss: 0.4848\n",
      "Epoch 50\n",
      "1s - loss: 0.4854 - val_loss: 0.4748\n",
      "Epoch 51\n",
      "1s - loss: 0.4746 - val_loss: 0.4655\n",
      "Epoch 52\n",
      "1s - loss: 0.4657 - val_loss: 0.4556\n",
      "Epoch 53\n",
      "1s - loss: 0.4564 - val_loss: 0.4482\n",
      "Epoch 54\n",
      "1s - loss: 0.4494 - val_loss: 0.4402\n",
      "Epoch 55\n",
      "1s - loss: 0.4423 - val_loss: 0.4343\n",
      "Epoch 56\n",
      "1s - loss: 0.4353 - val_loss: 0.4282\n",
      "Epoch 57\n",
      "1s - loss: 0.4303 - val_loss: 0.4230\n",
      "Epoch 58\n",
      "1s - loss: 0.4257 - val_loss: 0.4188\n",
      "Epoch 59\n",
      "1s - loss: 0.4210 - val_loss: 0.4153\n",
      "Epoch 60\n",
      "1s - loss: 0.4190 - val_loss: 0.4117\n",
      "Epoch 61\n",
      "1s - loss: 0.4145 - val_loss: 0.4095\n",
      "Epoch 62\n",
      "1s - loss: 0.4136 - val_loss: 0.4073\n",
      "Epoch 63\n",
      "1s - loss: 0.4115 - val_loss: 0.4058\n",
      "Epoch 64\n",
      "1s - loss: 0.4104 - val_loss: 0.4048\n",
      "Epoch 65\n",
      "1s - loss: 0.4082 - val_loss: 0.4036\n",
      "Epoch 66\n",
      "1s - loss: 0.4074 - val_loss: 0.4030\n",
      "Epoch 67\n",
      "1s - loss: 0.4078 - val_loss: 0.4024\n",
      "Epoch 68\n",
      "1s - loss: 0.4078 - val_loss: 0.4023\n",
      "Epoch 69\n",
      "1s - loss: 0.4066 - val_loss: 0.4017\n",
      "Epoch 70\n",
      "1s - loss: 0.4069 - val_loss: 0.4015\n",
      "Epoch 71\n",
      "1s - loss: 0.4068 - val_loss: 0.4014\n",
      "Epoch 72\n",
      "1s - loss: 0.4066 - val_loss: 0.4011\n",
      "Epoch 73\n",
      "1s - loss: 0.4060 - val_loss: 0.4008\n",
      "Epoch 74\n",
      "1s - loss: 0.4058 - val_loss: 0.4006\n",
      "Epoch 75\n",
      "1s - loss: 0.4062 - val_loss: 0.4007\n",
      "Epoch 76\n",
      "1s - loss: 0.4058 - val_loss: 0.4005\n",
      "Epoch 77\n",
      "1s - loss: 0.4047 - val_loss: 0.4000\n",
      "Epoch 78\n",
      "1s - loss: 0.4056 - val_loss: 0.4001\n",
      "Epoch 79\n",
      "1s - loss: 0.4064 - val_loss: 0.4000\n",
      "Epoch 80\n",
      "1s - loss: 0.4065 - val_loss: 0.3996\n",
      "Epoch 81\n",
      "1s - loss: 0.4057 - val_loss: 0.3997\n",
      "Epoch 82\n",
      "1s - loss: 0.4059 - val_loss: 0.3996\n",
      "Epoch 83\n",
      "1s - loss: 0.4049 - val_loss: 0.3995\n",
      "Epoch 84\n",
      "1s - loss: 0.4050 - val_loss: 0.3995\n",
      "Epoch 85\n",
      "1s - loss: 0.4051 - val_loss: 0.3996\n",
      "Epoch 86\n",
      "1s - loss: 0.4054 - val_loss: 0.3993\n",
      "Epoch 87\n",
      "1s - loss: 0.4047 - val_loss: 0.3991\n",
      "Epoch 88\n",
      "1s - loss: 0.4053 - val_loss: 0.3992\n",
      "Epoch 89\n",
      "1s - loss: 0.4053 - val_loss: 0.3992\n",
      "Epoch 90\n",
      "1s - loss: 0.4050 - val_loss: 0.3990\n",
      "Epoch 91\n",
      "1s - loss: 0.4041 - val_loss: 0.3990\n",
      "Epoch 92\n",
      "1s - loss: 0.4051 - val_loss: 0.3990\n",
      "Epoch 93\n",
      "1s - loss: 0.4045 - val_loss: 0.3988\n",
      "Epoch 94\n",
      "1s - loss: 0.4035 - val_loss: 0.3989\n",
      "Epoch 95\n",
      "1s - loss: 0.4032 - val_loss: 0.3987\n",
      "Epoch 96\n",
      "1s - loss: 0.4046 - val_loss: 0.3987\n",
      "Epoch 97\n",
      "1s - loss: 0.4038 - val_loss: 0.3988\n",
      "Epoch 98\n",
      "1s - loss: 0.4049 - val_loss: 0.3988\n",
      "Epoch 99\n",
      "1s - loss: 0.4031 - val_loss: 0.3985\n",
      "Epoch 100\n",
      "1s - loss: 0.4036 - val_loss: 0.3984\n",
      "Epoch 101\n",
      "1s - loss: 0.4050 - val_loss: 0.3983\n",
      "Epoch 102\n",
      "1s - loss: 0.4040 - val_loss: 0.3986\n",
      "Epoch 103\n",
      "1s - loss: 0.4037 - val_loss: 0.3983\n",
      "Epoch 104\n",
      "1s - loss: 0.4034 - val_loss: 0.3983\n",
      "Epoch 105\n",
      "1s - loss: 0.4031 - val_loss: 0.3984\n",
      "Epoch 106\n",
      "1s - loss: 0.4031 - val_loss: 0.3982\n",
      "Epoch 107\n",
      "1s - loss: 0.4031 - val_loss: 0.3981\n",
      "Epoch 108\n",
      "1s - loss: 0.4030 - val_loss: 0.3982\n",
      "Epoch 109\n",
      "1s - loss: 0.4043 - val_loss: 0.3980\n",
      "Epoch 110\n",
      "1s - loss: 0.4028 - val_loss: 0.3981\n",
      "Epoch 111\n",
      "1s - loss: 0.4044 - val_loss: 0.3980\n",
      "Epoch 112\n",
      "1s - loss: 0.4031 - val_loss: 0.3981\n",
      "Epoch 113\n",
      "1s - loss: 0.4037 - val_loss: 0.3978\n",
      "Epoch 114\n",
      "1s - loss: 0.4040 - val_loss: 0.3980\n",
      "Epoch 115\n",
      "1s - loss: 0.4029 - val_loss: 0.3978\n",
      "Epoch 116\n",
      "1s - loss: 0.4034 - val_loss: 0.3980\n",
      "Epoch 117\n",
      "1s - loss: 0.4029 - val_loss: 0.3980\n",
      "Epoch 118\n",
      "1s - loss: 0.4030 - val_loss: 0.3979\n",
      "Epoch 119\n",
      "1s - loss: 0.4027 - val_loss: 0.3980\n",
      "Epoch 120\n",
      "1s - loss: 0.4033 - val_loss: 0.3978\n",
      "Epoch 121\n",
      "1s - loss: 0.4037 - val_loss: 0.3977\n",
      "Epoch 122\n",
      "1s - loss: 0.4028 - val_loss: 0.3977\n",
      "Epoch 123\n",
      "1s - loss: 0.4035 - val_loss: 0.3976\n",
      "Epoch 124\n",
      "1s - loss: 0.4029 - val_loss: 0.3978\n",
      "Epoch 125\n",
      "1s - loss: 0.4015 - val_loss: 0.3977\n",
      "Epoch 126\n",
      "1s - loss: 0.4030 - val_loss: 0.3976\n",
      "Epoch 127\n",
      "1s - loss: 0.4032 - val_loss: 0.3977\n",
      "Epoch 128\n",
      "1s - loss: 0.4023 - val_loss: 0.3976\n",
      "Epoch 129\n",
      "1s - loss: 0.4010 - val_loss: 0.3975\n",
      "Epoch 130\n",
      "1s - loss: 0.4017 - val_loss: 0.3974\n",
      "Epoch 131\n",
      "1s - loss: 0.4027 - val_loss: 0.3975\n",
      "Epoch 132\n",
      "1s - loss: 0.4021 - val_loss: 0.3975\n",
      "Epoch 133\n",
      "1s - loss: 0.4019 - val_loss: 0.3975\n",
      "Epoch 134\n",
      "1s - loss: 0.4026 - val_loss: 0.3975\n",
      "Epoch 135\n",
      "1s - loss: 0.4024 - val_loss: 0.3973\n",
      "Epoch 136\n",
      "1s - loss: 0.4023 - val_loss: 0.3974\n",
      "Epoch 137\n",
      "1s - loss: 0.4025 - val_loss: 0.3976\n",
      "Epoch 138\n",
      "1s - loss: 0.4022 - val_loss: 0.3975\n",
      "Epoch 139\n",
      "1s - loss: 0.4027 - val_loss: 0.3974\n",
      "Epoch 140\n",
      "1s - loss: 0.4027 - val_loss: 0.3974\n",
      "Epoch 141\n",
      "1s - loss: 0.4015 - val_loss: 0.3973\n",
      "Epoch 142\n",
      "1s - loss: 0.4023 - val_loss: 0.3973\n",
      "Epoch 143\n",
      "1s - loss: 0.4028 - val_loss: 0.3973\n",
      "Epoch 144\n",
      "1s - loss: 0.4028 - val_loss: 0.3975\n",
      "Epoch 145\n",
      "1s - loss: 0.4030 - val_loss: 0.3973\n",
      "Epoch 146\n",
      "1s - loss: 0.4021 - val_loss: 0.3971\n",
      "Epoch 147\n",
      "1s - loss: 0.4027 - val_loss: 0.3972\n",
      "Epoch 148\n",
      "1s - loss: 0.4018 - val_loss: 0.3969\n",
      "Epoch 149\n",
      "1s - loss: 0.4017 - val_loss: 0.3970\n",
      "Epoch 150\n",
      "1s - loss: 0.4018 - val_loss: 0.3970\n",
      "Epoch 151\n",
      "1s - loss: 0.4018 - val_loss: 0.3971\n",
      "Epoch 152\n",
      "1s - loss: 0.4020 - val_loss: 0.3969\n",
      "Epoch 153\n",
      "1s - loss: 0.4016 - val_loss: 0.3970\n",
      "Epoch 154\n",
      "1s - loss: 0.4021 - val_loss: 0.3970\n",
      "Epoch 155\n",
      "1s - loss: 0.4019 - val_loss: 0.3970\n",
      "Epoch 156\n",
      "1s - loss: 0.4022 - val_loss: 0.3969\n",
      "Epoch 157\n",
      "1s - loss: 0.4023 - val_loss: 0.3970\n",
      "Epoch 158\n",
      "1s - loss: 0.4022 - val_loss: 0.3970\n",
      "Epoch 159\n",
      "1s - loss: 0.4016 - val_loss: 0.3970\n",
      "Epoch 160\n",
      "1s - loss: 0.4020 - val_loss: 0.3971\n",
      "Epoch 161\n",
      "1s - loss: 0.4017 - val_loss: 0.3970\n",
      "Epoch 162\n",
      "1s - loss: 0.4012 - val_loss: 0.3970\n",
      "Epoch 163\n",
      "1s - loss: 0.4018 - val_loss: 0.3971\n",
      "Epoch 164\n",
      "1s - loss: 0.4016 - val_loss: 0.3970\n",
      "Epoch 165\n",
      "1s - loss: 0.4015 - val_loss: 0.3970\n",
      "Epoch 166\n",
      "1s - loss: 0.4005 - val_loss: 0.3970\n",
      "Epoch 167\n",
      "1s - loss: 0.4003 - val_loss: 0.3968\n",
      "Epoch 168\n",
      "1s - loss: 0.4005 - val_loss: 0.3969\n",
      "Epoch 169\n",
      "1s - loss: 0.4009 - val_loss: 0.3967\n",
      "Epoch 170\n",
      "1s - loss: 0.4008 - val_loss: 0.3967\n",
      "Epoch 171\n",
      "1s - loss: 0.4010 - val_loss: 0.3967\n",
      "Epoch 172\n",
      "1s - loss: 0.4010 - val_loss: 0.3967\n",
      "Epoch 173\n",
      "1s - loss: 0.4007 - val_loss: 0.3967\n",
      "Epoch 174\n",
      "1s - loss: 0.4011 - val_loss: 0.3966\n",
      "Epoch 175\n",
      "1s - loss: 0.4021 - val_loss: 0.3965\n",
      "Epoch 176\n",
      "1s - loss: 0.4013 - val_loss: 0.3965\n",
      "Epoch 177\n",
      "1s - loss: 0.4016 - val_loss: 0.3967\n",
      "Epoch 178\n",
      "1s - loss: 0.4001 - val_loss: 0.3965\n",
      "Epoch 179\n",
      "1s - loss: 0.4013 - val_loss: 0.3966\n",
      "Epoch 180\n",
      "1s - loss: 0.4012 - val_loss: 0.3966\n",
      "Epoch 181\n",
      "1s - loss: 0.4009 - val_loss: 0.3969\n",
      "Epoch 182\n",
      "1s - loss: 0.4013 - val_loss: 0.3966\n",
      "Epoch 183\n",
      "1s - loss: 0.4002 - val_loss: 0.3966\n",
      "Epoch 184\n",
      "1s - loss: 0.4008 - val_loss: 0.3967\n",
      "Epoch 185\n",
      "1s - loss: 0.4009 - val_loss: 0.3967\n",
      "Epoch 186\n",
      "1s - loss: 0.4010 - val_loss: 0.3966\n",
      "Epoch 187\n",
      "1s - loss: 0.4007 - val_loss: 0.3965\n",
      "Epoch 188\n",
      "1s - loss: 0.4003 - val_loss: 0.3965\n",
      "Epoch 189\n",
      "1s - loss: 0.4002 - val_loss: 0.3965\n",
      "Epoch 190\n",
      "1s - loss: 0.4006 - val_loss: 0.3965\n",
      "Epoch 191\n",
      "1s - loss: 0.4013 - val_loss: 0.3966\n",
      "Epoch 192\n",
      "1s - loss: 0.4006 - val_loss: 0.3965\n",
      "Epoch 193\n",
      "1s - loss: 0.4005 - val_loss: 0.3966\n",
      "Epoch 194\n",
      "1s - loss: 0.4009 - val_loss: 0.3965\n",
      "Epoch 195\n",
      "1s - loss: 0.4002 - val_loss: 0.3966\n",
      "Epoch 196\n",
      "1s - loss: 0.4009 - val_loss: 0.3966\n",
      "Epoch 197\n",
      "1s - loss: 0.4007 - val_loss: 0.3965\n",
      "Epoch 198\n",
      "1s - loss: 0.4010 - val_loss: 0.3965\n",
      "Epoch 199\n",
      "1s - loss: 0.3998 - val_loss: 0.3965\n",
      "Epoch 200\n",
      "1s - loss: 0.3995 - val_loss: 0.3966\n",
      "Epoch 201\n",
      "1s - loss: 0.4013 - val_loss: 0.3964\n",
      "Epoch 202\n",
      "1s - loss: 0.4007 - val_loss: 0.3965\n",
      "Epoch 203\n",
      "1s - loss: 0.4005 - val_loss: 0.3964\n",
      "Epoch 204\n",
      "1s - loss: 0.3993 - val_loss: 0.3964\n",
      "Epoch 205\n",
      "1s - loss: 0.4009 - val_loss: 0.3963\n",
      "Epoch 206\n",
      "1s - loss: 0.4003 - val_loss: 0.3962\n",
      "Epoch 207\n",
      "1s - loss: 0.4004 - val_loss: 0.3963\n",
      "Epoch 208\n",
      "1s - loss: 0.4003 - val_loss: 0.3963\n",
      "Epoch 209\n",
      "1s - loss: 0.4002 - val_loss: 0.3963\n",
      "Epoch 210\n",
      "1s - loss: 0.3997 - val_loss: 0.3963\n",
      "Epoch 211\n",
      "1s - loss: 0.4000 - val_loss: 0.3962\n",
      "Epoch 212\n",
      "1s - loss: 0.4002 - val_loss: 0.3964\n",
      "Epoch 213\n",
      "1s - loss: 0.3998 - val_loss: 0.3961\n",
      "Epoch 214\n",
      "1s - loss: 0.4001 - val_loss: 0.3963\n",
      "Epoch 215\n",
      "1s - loss: 0.4010 - val_loss: 0.3962\n",
      "Epoch 216\n",
      "1s - loss: 0.3997 - val_loss: 0.3962\n",
      "Epoch 217\n",
      "1s - loss: 0.3987 - val_loss: 0.3962\n",
      "Epoch 218\n",
      "1s - loss: 0.3991 - val_loss: 0.3962\n",
      "Epoch 219\n",
      "1s - loss: 0.3994 - val_loss: 0.3962\n",
      "Epoch 220\n",
      "1s - loss: 0.3999 - val_loss: 0.3962\n",
      "Epoch 221\n",
      "1s - loss: 0.3990 - val_loss: 0.3961\n",
      "Epoch 222\n",
      "1s - loss: 0.4000 - val_loss: 0.3961\n",
      "Epoch 223\n",
      "1s - loss: 0.4006 - val_loss: 0.3960\n",
      "Epoch 224\n",
      "1s - loss: 0.3991 - val_loss: 0.3960\n",
      "Epoch 225\n",
      "1s - loss: 0.4001 - val_loss: 0.3962\n",
      "Epoch 226\n",
      "1s - loss: 0.3988 - val_loss: 0.3959\n",
      "Epoch 227\n",
      "1s - loss: 0.3998 - val_loss: 0.3960\n",
      "Epoch 228\n",
      "1s - loss: 0.3998 - val_loss: 0.3960\n",
      "Epoch 229\n",
      "1s - loss: 0.3994 - val_loss: 0.3960\n",
      "Epoch 230\n",
      "1s - loss: 0.4004 - val_loss: 0.3960\n",
      "Epoch 231\n",
      "1s - loss: 0.3998 - val_loss: 0.3960\n",
      "Epoch 232\n",
      "1s - loss: 0.3996 - val_loss: 0.3960\n",
      "Epoch 233\n",
      "1s - loss: 0.3992 - val_loss: 0.3959\n",
      "Epoch 234\n",
      "1s - loss: 0.3993 - val_loss: 0.3959\n",
      "Epoch 235\n",
      "1s - loss: 0.3996 - val_loss: 0.3958\n",
      "Epoch 236\n",
      "1s - loss: 0.3994 - val_loss: 0.3958\n",
      "Epoch 237\n",
      "1s - loss: 0.3990 - val_loss: 0.3958\n",
      "Epoch 238\n",
      "1s - loss: 0.3989 - val_loss: 0.3957\n",
      "Epoch 239\n",
      "1s - loss: 0.3990 - val_loss: 0.3957\n",
      "Epoch 240\n",
      "1s - loss: 0.3989 - val_loss: 0.3957\n",
      "Epoch 241\n",
      "1s - loss: 0.3999 - val_loss: 0.3957\n",
      "Epoch 242\n",
      "1s - loss: 0.3996 - val_loss: 0.3957\n",
      "Epoch 243\n",
      "1s - loss: 0.3993 - val_loss: 0.3958\n",
      "Epoch 244\n",
      "1s - loss: 0.3995 - val_loss: 0.3957\n",
      "Epoch 245\n",
      "1s - loss: 0.3993 - val_loss: 0.3957\n",
      "Epoch 246\n",
      "1s - loss: 0.3986 - val_loss: 0.3958\n",
      "Epoch 247\n",
      "1s - loss: 0.3999 - val_loss: 0.3959\n",
      "Epoch 248\n",
      "1s - loss: 0.3985 - val_loss: 0.3957\n",
      "Epoch 249\n",
      "1s - loss: 0.3992 - val_loss: 0.3958\n",
      "Epoch 250\n",
      "1s - loss: 0.3994 - val_loss: 0.3957\n",
      "Epoch 251\n",
      "1s - loss: 0.3977 - val_loss: 0.3958\n",
      "Epoch 252\n",
      "1s - loss: 0.3986 - val_loss: 0.3956\n",
      "Epoch 253\n",
      "1s - loss: 0.3990 - val_loss: 0.3956\n",
      "Epoch 254\n",
      "1s - loss: 0.3991 - val_loss: 0.3957\n",
      "Epoch 255\n",
      "1s - loss: 0.3988 - val_loss: 0.3957\n",
      "Epoch 256\n",
      "1s - loss: 0.4003 - val_loss: 0.3957\n",
      "Epoch 257\n",
      "1s - loss: 0.3993 - val_loss: 0.3956\n",
      "Epoch 258\n",
      "1s - loss: 0.3997 - val_loss: 0.3957\n",
      "Epoch 259\n",
      "1s - loss: 0.3985 - val_loss: 0.3957\n",
      "Epoch 260\n",
      "1s - loss: 0.3994 - val_loss: 0.3957\n",
      "Epoch 261\n",
      "1s - loss: 0.3985 - val_loss: 0.3957\n",
      "Epoch 262\n",
      "1s - loss: 0.3996 - val_loss: 0.3958\n",
      "Epoch 263\n",
      "1s - loss: 0.3993 - val_loss: 0.3956\n",
      "Epoch 264\n",
      "1s - loss: 0.3983 - val_loss: 0.3956\n",
      "Epoch 265\n",
      "1s - loss: 0.3985 - val_loss: 0.3955\n",
      "Epoch 266\n",
      "1s - loss: 0.3989 - val_loss: 0.3958\n",
      "Epoch 267\n",
      "1s - loss: 0.3975 - val_loss: 0.3955\n",
      "Epoch 268\n",
      "1s - loss: 0.3992 - val_loss: 0.3956\n",
      "Epoch 269\n",
      "1s - loss: 0.3984 - val_loss: 0.3956\n",
      "Epoch 270\n",
      "1s - loss: 0.3985 - val_loss: 0.3956\n",
      "Epoch 271\n",
      "1s - loss: 0.3987 - val_loss: 0.3957\n",
      "Epoch 272\n",
      "1s - loss: 0.3987 - val_loss: 0.3956\n",
      "Epoch 273\n",
      "1s - loss: 0.3988 - val_loss: 0.3956\n",
      "Epoch 274\n",
      "1s - loss: 0.3984 - val_loss: 0.3955\n",
      "Epoch 275\n",
      "1s - loss: 0.3996 - val_loss: 0.3956\n",
      "Epoch 276\n",
      "1s - loss: 0.3989 - val_loss: 0.3956\n",
      "Epoch 277\n",
      "1s - loss: 0.3984 - val_loss: 0.3955\n",
      "Epoch 278\n",
      "1s - loss: 0.3980 - val_loss: 0.3955\n",
      "Epoch 279\n",
      "1s - loss: 0.3982 - val_loss: 0.3955\n",
      "Epoch 280\n",
      "1s - loss: 0.3989 - val_loss: 0.3954\n",
      "Epoch 281\n",
      "1s - loss: 0.3978 - val_loss: 0.3955\n",
      "Epoch 282\n",
      "1s - loss: 0.3993 - val_loss: 0.3954\n",
      "Epoch 283\n",
      "1s - loss: 0.3985 - val_loss: 0.3953\n",
      "Epoch 284\n",
      "1s - loss: 0.3978 - val_loss: 0.3954\n",
      "Epoch 285\n",
      "1s - loss: 0.3981 - val_loss: 0.3954\n",
      "Epoch 286\n",
      "1s - loss: 0.3989 - val_loss: 0.3955\n",
      "Epoch 287\n",
      "1s - loss: 0.3989 - val_loss: 0.3953\n",
      "Epoch 288\n",
      "1s - loss: 0.3986 - val_loss: 0.3954\n",
      "Epoch 289\n",
      "1s - loss: 0.3979 - val_loss: 0.3956\n",
      "Epoch 290\n",
      "1s - loss: 0.3979 - val_loss: 0.3953\n",
      "Epoch 291\n",
      "1s - loss: 0.3983 - val_loss: 0.3954\n",
      "Epoch 292\n",
      "1s - loss: 0.3997 - val_loss: 0.3953\n",
      "Epoch 293\n",
      "1s - loss: 0.3976 - val_loss: 0.3954\n",
      "Epoch 294\n",
      "1s - loss: 0.3974 - val_loss: 0.3954\n",
      "Epoch 295\n",
      "1s - loss: 0.3973 - val_loss: 0.3952\n",
      "Epoch 296\n",
      "1s - loss: 0.3979 - val_loss: 0.3952\n",
      "Epoch 297\n",
      "1s - loss: 0.3979 - val_loss: 0.3952\n",
      "Epoch 298\n",
      "1s - loss: 0.3983 - val_loss: 0.3952\n",
      "Epoch 299\n",
      "1s - loss: 0.3979 - val_loss: 0.3952\n",
      "Epoch 300\n",
      "1s - loss: 0.3983 - val_loss: 0.3952\n",
      "Epoch 301\n",
      "1s - loss: 0.3979 - val_loss: 0.3952\n",
      "Epoch 302\n",
      "1s - loss: 0.3982 - val_loss: 0.3952\n",
      "Epoch 303\n",
      "1s - loss: 0.3980 - val_loss: 0.3952\n",
      "Epoch 304\n",
      "1s - loss: 0.3981 - val_loss: 0.3952\n",
      "Epoch 305\n",
      "1s - loss: 0.3983 - val_loss: 0.3951\n",
      "Epoch 306\n",
      "1s - loss: 0.3971 - val_loss: 0.3953\n",
      "Epoch 307\n",
      "1s - loss: 0.3982 - val_loss: 0.3952\n",
      "Epoch 308\n",
      "1s - loss: 0.3980 - val_loss: 0.3952\n",
      "Epoch 309\n",
      "1s - loss: 0.3976 - val_loss: 0.3952\n",
      "Epoch 310\n",
      "1s - loss: 0.3985 - val_loss: 0.3952\n",
      "Epoch 311\n",
      "1s - loss: 0.3983 - val_loss: 0.3952\n",
      "Epoch 312\n",
      "1s - loss: 0.3970 - val_loss: 0.3952\n",
      "Epoch 313\n",
      "1s - loss: 0.3966 - val_loss: 0.3952\n",
      "Epoch 314\n",
      "1s - loss: 0.3976 - val_loss: 0.3952\n",
      "Epoch 315\n",
      "1s - loss: 0.3975 - val_loss: 0.3952\n",
      "Epoch 316\n",
      "1s - loss: 0.3984 - val_loss: 0.3952\n",
      "Epoch 317\n",
      "1s - loss: 0.3976 - val_loss: 0.3950\n",
      "Epoch 318\n",
      "1s - loss: 0.3969 - val_loss: 0.3951\n",
      "Epoch 319\n",
      "1s - loss: 0.3971 - val_loss: 0.3953\n",
      "Epoch 320\n",
      "1s - loss: 0.3963 - val_loss: 0.3951\n",
      "Epoch 321\n",
      "1s - loss: 0.3975 - val_loss: 0.3951\n",
      "Epoch 322\n",
      "1s - loss: 0.3991 - val_loss: 0.3952\n",
      "Epoch 323\n",
      "1s - loss: 0.3981 - val_loss: 0.3952\n",
      "Epoch 324\n",
      "1s - loss: 0.3977 - val_loss: 0.3951\n",
      "Epoch 325\n",
      "1s - loss: 0.3978 - val_loss: 0.3951\n",
      "Epoch 326\n",
      "1s - loss: 0.3974 - val_loss: 0.3951\n",
      "Epoch 327\n",
      "1s - loss: 0.3978 - val_loss: 0.3952\n",
      "Epoch 328\n",
      "1s - loss: 0.3981 - val_loss: 0.3952\n",
      "Epoch 329\n",
      "1s - loss: 0.3962 - val_loss: 0.3953\n",
      "Epoch 330\n",
      "1s - loss: 0.3974 - val_loss: 0.3951\n",
      "Epoch 331\n",
      "1s - loss: 0.3974 - val_loss: 0.3950\n",
      "Epoch 332\n",
      "1s - loss: 0.3970 - val_loss: 0.3951\n",
      "Epoch 333\n",
      "1s - loss: 0.3973 - val_loss: 0.3950\n",
      "Epoch 334\n",
      "1s - loss: 0.3973 - val_loss: 0.3948\n",
      "Epoch 335\n",
      "1s - loss: 0.3966 - val_loss: 0.3950\n",
      "Epoch 336\n",
      "1s - loss: 0.3975 - val_loss: 0.3949\n",
      "Epoch 337\n",
      "1s - loss: 0.3972 - val_loss: 0.3949\n",
      "Epoch 338\n",
      "1s - loss: 0.3978 - val_loss: 0.3949\n",
      "Epoch 339\n",
      "1s - loss: 0.3968 - val_loss: 0.3949\n",
      "Epoch 340\n",
      "1s - loss: 0.3970 - val_loss: 0.3949\n",
      "Epoch 341\n",
      "1s - loss: 0.3973 - val_loss: 0.3949\n",
      "Epoch 342\n",
      "1s - loss: 0.3970 - val_loss: 0.3949\n",
      "Epoch 343\n",
      "1s - loss: 0.3974 - val_loss: 0.3949\n",
      "Epoch 344\n",
      "1s - loss: 0.3971 - val_loss: 0.3949\n",
      "Epoch 345\n",
      "1s - loss: 0.3978 - val_loss: 0.3949\n",
      "Epoch 346\n",
      "1s - loss: 0.3953 - val_loss: 0.3950\n",
      "Epoch 347\n",
      "1s - loss: 0.3974 - val_loss: 0.3949\n",
      "Epoch 348\n",
      "1s - loss: 0.3967 - val_loss: 0.3949\n",
      "Epoch 349\n",
      "1s - loss: 0.3968 - val_loss: 0.3949\n",
      "Epoch 350\n",
      "1s - loss: 0.3966 - val_loss: 0.3949\n",
      "Epoch 351\n",
      "1s - loss: 0.3967 - val_loss: 0.3949\n",
      "Epoch 352\n",
      "1s - loss: 0.3966 - val_loss: 0.3949\n",
      "Epoch 353\n",
      "1s - loss: 0.3979 - val_loss: 0.3948\n",
      "Epoch 354\n",
      "1s - loss: 0.3971 - val_loss: 0.3948\n",
      "Epoch 355\n",
      "1s - loss: 0.3968 - val_loss: 0.3950\n",
      "Epoch 356\n",
      "1s - loss: 0.3970 - val_loss: 0.3950\n",
      "Epoch 357\n",
      "1s - loss: 0.3970 - val_loss: 0.3948\n",
      "Epoch 358\n",
      "1s - loss: 0.3979 - val_loss: 0.3949\n",
      "Epoch 359\n",
      "1s - loss: 0.3976 - val_loss: 0.3950\n",
      "Epoch 360\n",
      "1s - loss: 0.3962 - val_loss: 0.3949\n",
      "Epoch 361\n",
      "1s - loss: 0.3960 - val_loss: 0.3950\n",
      "Epoch 362\n",
      "1s - loss: 0.3962 - val_loss: 0.3948\n",
      "Epoch 363\n",
      "1s - loss: 0.3975 - val_loss: 0.3948\n",
      "Epoch 364\n",
      "1s - loss: 0.3962 - val_loss: 0.3948\n",
      "Epoch 365\n",
      "1s - loss: 0.3963 - val_loss: 0.3949\n",
      "Epoch 366\n",
      "1s - loss: 0.3968 - val_loss: 0.3949\n",
      "Epoch 367\n",
      "1s - loss: 0.3958 - val_loss: 0.3948\n",
      "Epoch 368\n",
      "1s - loss: 0.3962 - val_loss: 0.3948\n",
      "Epoch 369\n",
      "1s - loss: 0.3973 - val_loss: 0.3948\n",
      "Epoch 370\n",
      "1s - loss: 0.3956 - val_loss: 0.3948\n",
      "Epoch 371\n",
      "1s - loss: 0.3957 - val_loss: 0.3948\n",
      "Epoch 372\n",
      "1s - loss: 0.3957 - val_loss: 0.3947\n",
      "Epoch 373\n",
      "1s - loss: 0.3962 - val_loss: 0.3948\n",
      "Epoch 374\n",
      "1s - loss: 0.3967 - val_loss: 0.3948\n",
      "Epoch 375\n",
      "1s - loss: 0.3964 - val_loss: 0.3947\n",
      "Epoch 376\n",
      "1s - loss: 0.3963 - val_loss: 0.3947\n",
      "Epoch 377\n",
      "1s - loss: 0.3965 - val_loss: 0.3947\n",
      "Epoch 378\n",
      "1s - loss: 0.3959 - val_loss: 0.3947\n",
      "Epoch 379\n",
      "1s - loss: 0.3968 - val_loss: 0.3948\n",
      "Epoch 380\n",
      "1s - loss: 0.3958 - val_loss: 0.3948\n",
      "Epoch 381\n",
      "1s - loss: 0.3974 - val_loss: 0.3948\n",
      "Epoch 382\n",
      "1s - loss: 0.3962 - val_loss: 0.3948\n",
      "Epoch 383\n",
      "1s - loss: 0.3957 - val_loss: 0.3948\n",
      "Epoch 384\n",
      "1s - loss: 0.3966 - val_loss: 0.3946\n",
      "Epoch 385\n",
      "1s - loss: 0.3959 - val_loss: 0.3946\n",
      "Epoch 386\n",
      "1s - loss: 0.3967 - val_loss: 0.3946\n",
      "Epoch 387\n",
      "1s - loss: 0.3962 - val_loss: 0.3945\n",
      "Epoch 388\n",
      "1s - loss: 0.3965 - val_loss: 0.3946\n",
      "Epoch 389\n",
      "1s - loss: 0.3961 - val_loss: 0.3945\n",
      "Epoch 390\n",
      "1s - loss: 0.3967 - val_loss: 0.3946\n",
      "Epoch 391\n",
      "1s - loss: 0.3958 - val_loss: 0.3946\n",
      "Epoch 392\n",
      "1s - loss: 0.3959 - val_loss: 0.3946\n",
      "Epoch 393\n",
      "1s - loss: 0.3959 - val_loss: 0.3946\n",
      "Epoch 394\n",
      "1s - loss: 0.3953 - val_loss: 0.3947\n",
      "Epoch 395\n",
      "1s - loss: 0.3950 - val_loss: 0.3947\n",
      "Epoch 396\n",
      "1s - loss: 0.3965 - val_loss: 0.3947\n",
      "Epoch 397\n",
      "1s - loss: 0.3958 - val_loss: 0.3946\n",
      "Epoch 398\n",
      "1s - loss: 0.3958 - val_loss: 0.3946\n",
      "Epoch 399\n",
      "1s - loss: 0.3957 - val_loss: 0.3945\n",
      "Epoch 400\n",
      "1s - loss: 0.3958 - val_loss: 0.3946\n",
      "Epoch 401\n",
      "1s - loss: 0.3951 - val_loss: 0.3947\n",
      "Epoch 402\n",
      "1s - loss: 0.3962 - val_loss: 0.3947\n",
      "Epoch 403\n",
      "1s - loss: 0.3961 - val_loss: 0.3945\n",
      "Epoch 404\n",
      "1s - loss: 0.3959 - val_loss: 0.3946\n",
      "Epoch 405\n",
      "1s - loss: 0.3966 - val_loss: 0.3944\n",
      "Epoch 406\n",
      "1s - loss: 0.3957 - val_loss: 0.3945\n",
      "Epoch 407\n",
      "1s - loss: 0.3944 - val_loss: 0.3945\n",
      "Epoch 408\n",
      "1s - loss: 0.3957 - val_loss: 0.3945\n",
      "Epoch 409\n",
      "1s - loss: 0.3968 - val_loss: 0.3944\n",
      "Epoch 410\n",
      "1s - loss: 0.3961 - val_loss: 0.3945\n",
      "Epoch 411\n",
      "1s - loss: 0.3961 - val_loss: 0.3946\n",
      "Epoch 412\n",
      "1s - loss: 0.3955 - val_loss: 0.3945\n",
      "Epoch 413\n",
      "1s - loss: 0.3963 - val_loss: 0.3945\n",
      "Epoch 414\n",
      "1s - loss: 0.3948 - val_loss: 0.3943\n",
      "Epoch 415\n",
      "1s - loss: 0.3963 - val_loss: 0.3944\n",
      "Epoch 416\n",
      "1s - loss: 0.3951 - val_loss: 0.3944\n",
      "Epoch 417\n",
      "1s - loss: 0.3948 - val_loss: 0.3944\n",
      "Epoch 418\n",
      "1s - loss: 0.3950 - val_loss: 0.3945\n",
      "Epoch 419\n",
      "1s - loss: 0.3961 - val_loss: 0.3945\n",
      "Epoch 420\n",
      "1s - loss: 0.3952 - val_loss: 0.3946\n",
      "Epoch 421\n",
      "1s - loss: 0.3953 - val_loss: 0.3944\n",
      "Epoch 422\n",
      "1s - loss: 0.3956 - val_loss: 0.3945\n",
      "Epoch 423\n",
      "1s - loss: 0.3953 - val_loss: 0.3945\n",
      "Epoch 424\n",
      "1s - loss: 0.3957 - val_loss: 0.3944\n",
      "Epoch 425\n",
      "1s - loss: 0.3949 - val_loss: 0.3944\n",
      "Epoch 426\n",
      "1s - loss: 0.3959 - val_loss: 0.3943\n",
      "Epoch 427\n",
      "1s - loss: 0.3955 - val_loss: 0.3943\n",
      "Epoch 428\n",
      "1s - loss: 0.3959 - val_loss: 0.3944\n",
      "Epoch 429\n",
      "1s - loss: 0.3952 - val_loss: 0.3942\n",
      "Epoch 430\n",
      "1s - loss: 0.3952 - val_loss: 0.3945\n",
      "Epoch 431\n",
      "1s - loss: 0.3955 - val_loss: 0.3944\n",
      "Epoch 432\n",
      "1s - loss: 0.3952 - val_loss: 0.3942\n",
      "Epoch 433\n",
      "1s - loss: 0.3948 - val_loss: 0.3944\n",
      "Epoch 434\n",
      "1s - loss: 0.3951 - val_loss: 0.3943\n",
      "Epoch 435\n",
      "1s - loss: 0.3946 - val_loss: 0.3943\n",
      "Epoch 436\n",
      "1s - loss: 0.3949 - val_loss: 0.3944\n",
      "Epoch 437\n",
      "1s - loss: 0.3951 - val_loss: 0.3944\n",
      "Epoch 438\n",
      "1s - loss: 0.3949 - val_loss: 0.3943\n",
      "Epoch 439\n",
      "1s - loss: 0.3946 - val_loss: 0.3943\n",
      "Epoch 440\n",
      "1s - loss: 0.3946 - val_loss: 0.3943\n",
      "Epoch 441\n",
      "1s - loss: 0.3960 - val_loss: 0.3943\n",
      "Epoch 442\n",
      "1s - loss: 0.3957 - val_loss: 0.3943\n",
      "Epoch 443\n",
      "1s - loss: 0.3936 - val_loss: 0.3944\n",
      "Epoch 444\n",
      "1s - loss: 0.3953 - val_loss: 0.3944\n",
      "Epoch 445\n",
      "1s - loss: 0.3952 - val_loss: 0.3945\n",
      "Epoch 446\n",
      "1s - loss: 0.3941 - val_loss: 0.3943\n",
      "Epoch 447\n",
      "1s - loss: 0.3952 - val_loss: 0.3943\n",
      "Epoch 448\n",
      "1s - loss: 0.3953 - val_loss: 0.3943\n",
      "Epoch 449\n",
      "1s - loss: 0.3951 - val_loss: 0.3943\n",
      "Epoch 450\n",
      "1s - loss: 0.3947 - val_loss: 0.3943\n",
      "Epoch 451\n",
      "1s - loss: 0.3948 - val_loss: 0.3943\n",
      "Epoch 452\n",
      "1s - loss: 0.3952 - val_loss: 0.3943\n",
      "Epoch 453\n",
      "1s - loss: 0.3950 - val_loss: 0.3943\n",
      "Epoch 454\n",
      "1s - loss: 0.3949 - val_loss: 0.3942\n",
      "Epoch 455\n",
      "1s - loss: 0.3947 - val_loss: 0.3943\n",
      "Epoch 456\n",
      "1s - loss: 0.3956 - val_loss: 0.3942\n",
      "Epoch 457\n",
      "1s - loss: 0.3950 - val_loss: 0.3943\n",
      "Epoch 458\n",
      "1s - loss: 0.3949 - val_loss: 0.3944\n",
      "Epoch 459\n",
      "1s - loss: 0.3954 - val_loss: 0.3944\n",
      "Epoch 460\n",
      "1s - loss: 0.3951 - val_loss: 0.3944\n",
      "Epoch 461\n",
      "1s - loss: 0.3953 - val_loss: 0.3943\n",
      "Epoch 462\n",
      "1s - loss: 0.3953 - val_loss: 0.3943\n",
      "Epoch 463\n",
      "1s - loss: 0.3945 - val_loss: 0.3943\n",
      "Epoch 464\n",
      "1s - loss: 0.3954 - val_loss: 0.3943\n",
      "Epoch 465\n",
      "1s - loss: 0.3942 - val_loss: 0.3944\n",
      "Epoch 466\n",
      "1s - loss: 0.3948 - val_loss: 0.3943\n",
      "Epoch 467\n",
      "1s - loss: 0.3945 - val_loss: 0.3942\n",
      "Epoch 468\n",
      "1s - loss: 0.3942 - val_loss: 0.3942\n",
      "Epoch 469\n",
      "1s - loss: 0.3939 - val_loss: 0.3942\n",
      "Epoch 470\n",
      "1s - loss: 0.3941 - val_loss: 0.3941\n",
      "Epoch 471\n",
      "1s - loss: 0.3944 - val_loss: 0.3942\n",
      "Epoch 472\n",
      "1s - loss: 0.3947 - val_loss: 0.3942\n",
      "Epoch 473\n",
      "1s - loss: 0.3947 - val_loss: 0.3942\n",
      "Epoch 474\n",
      "1s - loss: 0.3938 - val_loss: 0.3941\n",
      "Epoch 475\n",
      "1s - loss: 0.3944 - val_loss: 0.3940\n",
      "Epoch 476\n",
      "1s - loss: 0.3935 - val_loss: 0.3941\n",
      "Epoch 477\n",
      "1s - loss: 0.3956 - val_loss: 0.3942\n",
      "Epoch 478\n",
      "1s - loss: 0.3941 - val_loss: 0.3941\n",
      "Epoch 479\n",
      "1s - loss: 0.3942 - val_loss: 0.3941\n",
      "Epoch 480\n",
      "1s - loss: 0.3951 - val_loss: 0.3942\n",
      "Epoch 481\n",
      "1s - loss: 0.3927 - val_loss: 0.3941\n",
      "Epoch 482\n",
      "1s - loss: 0.3930 - val_loss: 0.3942\n",
      "Epoch 483\n",
      "1s - loss: 0.3950 - val_loss: 0.3942\n",
      "Epoch 484\n",
      "1s - loss: 0.3944 - val_loss: 0.3942\n",
      "Epoch 485\n",
      "1s - loss: 0.3942 - val_loss: 0.3942\n",
      "Epoch 486\n",
      "1s - loss: 0.3937 - val_loss: 0.3942\n",
      "Epoch 487\n",
      "1s - loss: 0.3942 - val_loss: 0.3942\n",
      "Epoch 488\n",
      "1s - loss: 0.3944 - val_loss: 0.3941\n",
      "Epoch 489\n",
      "1s - loss: 0.3944 - val_loss: 0.3940\n",
      "Epoch 490\n",
      "1s - loss: 0.3931 - val_loss: 0.3941\n",
      "Epoch 491\n",
      "1s - loss: 0.3939 - val_loss: 0.3942\n",
      "Epoch 492\n",
      "1s - loss: 0.3950 - val_loss: 0.3942\n",
      "Epoch 493\n",
      "1s - loss: 0.3942 - val_loss: 0.3942\n",
      "Epoch 494\n",
      "1s - loss: 0.3946 - val_loss: 0.3942\n",
      "Epoch 495\n",
      "1s - loss: 0.3932 - val_loss: 0.3943\n",
      "Epoch 496\n",
      "1s - loss: 0.3943 - val_loss: 0.3942\n",
      "Epoch 497\n",
      "1s - loss: 0.3938 - val_loss: 0.3943\n",
      "Epoch 498\n",
      "1s - loss: 0.3943 - val_loss: 0.3942\n",
      "Epoch 499\n",
      "1s - loss: 0.3934 - val_loss: 0.3941\n",
      " Train time: 725.185 s\tScore 0.36071\n",
      "done\n",
      "0.360712842162\n"
     ]
    }
   ],
   "source": [
    "# clf = simple.SimpleOrdinalClassifier(nn)\n",
    "\n",
    "# clf = KerasClassifier(model=model, optimizer='rmsprop', loss=loss_type)\n",
    "\n",
    "losses = []\n",
    "from keras.callbacks import Callback\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        pass\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        self.losses.append(logs.get('val_loss'))\n",
    "# clf = ElasticNet()\n",
    "\n",
    "scores = []\n",
    "scores_base = []\n",
    "n_reps = 1\n",
    "k = 5\n",
    "for reps in range(n_reps):\n",
    "    skf = StratifiedKFold(y_binned, n_folds=k,\n",
    "                          shuffle=True,\n",
    "                          random_state=np.random.randint(0,100))\n",
    "    for ii, (train, valid) in enumerate(skf):\n",
    "        model.load_weights('saved/nn_weights')\n",
    "        \n",
    "        \n",
    "        history = LossHistory()   # for keras\n",
    "        print 'Fold %d' % ii,\n",
    "#         X_train_k = X_train[train]\n",
    "#         X_valid_k = X_train[valid]\n",
    "        X_train_k = X[train]\n",
    "        X_valid_k = X[valid]\n",
    "        y_train_k = y_train[train]\n",
    "        y_valid_k = y_train[valid]\n",
    "#         y_train_k = y_binned[train]\n",
    "#         y_valid_k = y_binned[valid]\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        tic = time()\n",
    "#         clf.fit(X_train_k, y_train_k)\n",
    "        \n",
    "#         clf.fit(X_train_k, y_train_k, batch_size=16384, nb_epoch=200)\n",
    "    \n",
    "        model.fit(X_train_k, y_train_k, \n",
    "                  nb_epoch=500, batch_size=1024*16,\n",
    "                  validation_data=(X_valid_k, y_valid_k),\n",
    "                  callbacks=[history],\n",
    "                  verbose=2)\n",
    "        \n",
    "        toc = time() - tic\n",
    "        print 'Train time: %2.3f s\\t' % toc, \n",
    "\n",
    "#         valid_preds = clf.predict(X_valid_k)\n",
    "#         valid_preds = clf.predict_proba(X_valid_k, batch_size=16384).flatten()\n",
    "        valid_preds = model.predict_proba(X_valid_k, batch_size=16384, verbose=0).flatten()\n",
    "        \n",
    "#         score = metrics.gini(y_valid_k, valid_preds)\n",
    "        score = metrics.normalized_gini(y_valid_k, valid_preds)\n",
    "        \n",
    "        print 'Score %1.5f' % score\n",
    "        scores.append(score)\n",
    "        \n",
    "        losses.append(history.losses)\n",
    "        \n",
    "        break\n",
    "        \n",
    "print \"done\"\n",
    "print np.array(scores).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.352331569833\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.39, 0.45)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIwAAAJQCAYAAADsev/BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1d174df590>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "print np.array(scores).mean()\n",
    "plt.plot(np.mean(losses, axis=0))\n",
    "# plt.ylim([0.88, 0.94])\n",
    "# plt.ylim([0.1, 0.12])\n",
    "# plt.ylim([14.5, 15.5])\n",
    "plt.ylim([0.39, 0.45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.72351503  3.22279859  2.74496603  4.32277489  3.4688468   4.8109026\n",
      "  2.28649879  2.8852489   2.24739313  2.19923973]\n",
      "[ 1  1  5 15  1 14  1  1  4  7]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': 'mean_absolute_error',\n",
       " 'model': <keras.models.Sequential at 0x7f72ef18dbd0>,\n",
       " 'optimizer': 'rmsprop'}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0.345 @ 512 maxout\n",
    "# 0.365419697668  1000 clusters & original feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Predict Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.compile(loss=loss_type, optimizer='rmsprop')\n",
    "model.fit(X_train, y_train, \n",
    "          nb_epoch=10, batch_size=16384,\n",
    "          callbacks=[history],\n",
    "          verbose=0)\n",
    "\n",
    "preds = model.predict_proba(X_test, batch_size=16384, verbose=0).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(51000,)\n",
      "(51000, 111)\n"
     ]
    }
   ],
   "source": [
    "print preds.shape\n",
    "print X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = pd.DataFrame({\"Id\": test_ind, \"Hazard\": preds})\n",
    "preds = preds.set_index('Id')\n",
    "preds.to_csv('submissions/keras_benchmark.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
