{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/afs/ee.cooper.edu/user/t/a/tam8/documents/ml_misc/semi_supervised/')\n",
    "import coreg\n",
    "reload(coreg)\n",
    "import rasco\n",
    "reload(rasco)\n",
    "import trireg\n",
    "reload(trireg)\n",
    "sys.path.append('/afs/ee.cooper.edu/user/t/a/tam8/documents/ml_misc/ordinal/')\n",
    "import simple\n",
    "reload(simple)\n",
    "sys.path.append('/afs/ee.cooper.edu/user/t/a/tam8/documents/ml_misc/ensemble/')\n",
    "import stacking\n",
    "reload(stacking)\n",
    "\n",
    "sys.path.append('/afs/ee.cooper.edu/user/t/a/tam8/documents/ml_misc/neighbors/')\n",
    "import rpfnn\n",
    "reload(rpfnn)\n",
    "import ann\n",
    "reload(ann)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import xgboost as xgb\n",
    "\n",
    "import transformers as tforms\n",
    "reload(tforms)\n",
    "from sklearn.pipeline import Pipeline, make_pipeline, FeatureUnion, make_union\n",
    "\n",
    "import metrics\n",
    "reload(metrics)\n",
    "from sklearn.cross_validation import StratifiedKFold, train_test_split\n",
    "\n",
    "from sklearn.base import clone\n",
    "\n",
    "from sklearn.svm import SVC, LinearSVC, SVR\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, ElasticNet, Ridge, Lasso, SGDRegressor, SGDClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier, DecisionTreeClassifier, DecisionTreeRegressor\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "\n",
    "from sklearn.lda import LDA\n",
    "from sklearn.qda import QDA\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "from collections import Counter\n",
    "import minirank as mr\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from time import time\n",
    "\n",
    "import cPickle as pickle\n",
    "\n",
    "def wgmean(x, w):\n",
    "    return np.exp(np.sum(w*np.log(x), axis=1) / np.sum(w, axis=1))\n",
    "\n",
    "import logging \n",
    "fh = logging.FileHandler('/tmp/lmg.log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data\n",
    "train_pd  = pd.read_pickle('saved/train_pd_enc.p')\n",
    "test_pd  = pd.read_pickle('saved/test_pd_enc.p')\n",
    "labels = pd.read_pickle('saved/labels.p')\n",
    "test_ind = pickle.load(open('saved/test_ind.p'))\n",
    "y_binned = np.load('saved/y_binned.npy')\n",
    "\n",
    "fi = np.load('saved/feature_importances.npy')\n",
    "y_binned[y_binned==6] = 5\n",
    "\n",
    "drop_cols = ['T1_V10', 'T1_V13', 'T2_V7', 'T2_V10']\n",
    "# drop_cols = train_pd.columns[fi < 0.01]\n",
    "\n",
    "\n",
    "for col in drop_cols:\n",
    "    train_pd.drop(col, axis=1, inplace=True)\n",
    "    test_pd.drop(col, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data custom\n",
    "train_pd  = pd.read_pickle('saved/train_pd_custom.p')\n",
    "test_pd  = pd.read_pickle('saved/test_pd_custom.p')\n",
    "\n",
    "labels = pd.read_pickle('saved/labels.p')\n",
    "test_ind = pickle.load(open('saved/test_ind.p'))\n",
    "y_binned = np.load('saved/y_binned.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load Data le instead\n",
    "# train_pd  = pd.read_pickle('saved/train_pd_l_enc.p')\n",
    "# test_pd  = pd.read_pickle('saved/test_pd_l_enc.p')\n",
    "\n",
    "train_pd  = pd.read_pickle('saved/train_pd_le_and_oh_enc.p')\n",
    "test_pd  = pd.read_pickle('saved/test_pd_le_and_oh_enc.p')\n",
    "\n",
    "labels = pd.read_pickle('saved/labels.p')\n",
    "test_ind = pickle.load(open('saved/test_ind.p'))\n",
    "y_binned = np.load('saved/y_binned.npy')\n",
    "fi = np.load('saved/feature_importances.npy')\n",
    "\n",
    "y_binned[y_binned==6] = 5\n",
    "\n",
    "drop_cols = ['T1_V10', 'T1_V13', 'T2_V7', 'T2_V10']\n",
    "# drop_cols = []\n",
    "\n",
    "# drop_cols = train_pd.columns[fi < 0.01]\n",
    "\n",
    "\n",
    "for col in drop_cols:\n",
    "    train_pd.drop(col, axis=1, inplace=True)\n",
    "    test_pd.drop(col, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Data binary instead\n",
    "train_pd  = pd.read_pickle('saved/train_pd_binary_enc.p')\n",
    "test_pd  = pd.read_pickle('saved/test_pd_binary_enc.p')\n",
    "labels = pd.read_pickle('saved/labels.p')\n",
    "test_ind = pickle.load(open('saved/test_ind.p'))\n",
    "y_binned = np.load('saved/y_binned.npy')\n",
    "\n",
    "y_binned[y_binned==6] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50999, 2)\n",
      "(51000, 2)\n",
      "(50999, 2)\n",
      "(51000, 2)\n"
     ]
    }
   ],
   "source": [
    "X_tsne2_26 = np.load('saved/X_tsne2_26important.npy')\n",
    "X_tsne2_26_train = X_tsne2_26[:len(train_pd), :]\n",
    "X_tsne2_26_test = X_tsne2_26[-len(test_pd):, :]\n",
    "\n",
    "X_tsne2 = np.load('saved/X_tsne2.npy')\n",
    "X_tsne2_train = X_tsne2[:len(train_pd), :]\n",
    "X_tsne2_test = X_tsne2[-len(test_pd):, :]\n",
    "\n",
    "print X_tsne2_train.shape\n",
    "print X_tsne2_test.shape\n",
    "print X_tsne2_26_train.shape\n",
    "print X_tsne2_26_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50999, 38) (51000, 38)\n"
     ]
    }
   ],
   "source": [
    "# X_fa10_train = np.load('saved/X_fa10_train.npy')\n",
    "# X_fa10_test = np.load('saved/X_fa10_test.npy')\n",
    "# print X_fa10_train.shape, X_fa10_test.shape\n",
    "\n",
    "X_fa40_train = np.load('saved/X_fa40_train.npy')\n",
    "X_fa40_test = np.load('saved/X_fa40_test.npy')\n",
    "print X_fa40_train.shape, X_fa40_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train (50999,)\n",
      "X_train (50999, 79)\n",
      "X_test (51000, 79)\n",
      "X_hold 50\n",
      "5\n",
      "<type 'numpy.float64'>\n"
     ]
    }
   ],
   "source": [
    "train = np.array(train_pd)\n",
    "test = np.array(test_pd)\n",
    "\n",
    "X_train = train.astype(float)\n",
    "X_test = test.astype(float)\n",
    "y_train = np.array(labels)\n",
    "\n",
    "try:\n",
    "    X_train = np.c_[X_train, X_tsne2_26_train]\n",
    "    X_test = np.c_[X_test, X_tsne2_26_test]\n",
    "#     X_train = X_tsne2_26_train\n",
    "#     X_test = X_tsne2_26_test\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    X_train = np.c_[X_train, X_3000mean]\n",
    "except:\n",
    "    pass\n",
    "\n",
    "holdout = False\n",
    "if holdout:\n",
    "    X_train, X_hold, \\\n",
    "    y_train, y_hold, \\\n",
    "    y_binned, y_binned_hold \\\n",
    "    = train_test_split(\n",
    "        X_train, y_train, y_binned, \n",
    "        test_size=0.2, random_state=0)\n",
    "\n",
    "# \"\"\"\n",
    "pipe_x = make_pipeline(\n",
    "    make_union(\n",
    "        tforms.IdentityTformer(),\n",
    "#         make_pipeline(AddTformer(1), BoxCoxTformer()),\n",
    "#         AnscombeTformer(),\n",
    "    ),\n",
    "#     StandardScaler(),\n",
    ")\n",
    "pipe_y = make_pipeline(\n",
    "    tforms.IdentityTformer(),\n",
    "#     tforms.BoxCoxTformer(),\n",
    "#     tforms.LogTformer(),\n",
    "#     tforms.AnscombeTformer(),\n",
    "#     tforms.FreemanTukeyTformer(),\n",
    "#     tforms.ArcsinhTformer(),\n",
    "#     StandardScaler(),\n",
    "    \n",
    ")\n",
    "pipe_x.fit(np.r_[X_train, X_test])\n",
    "pipe_y.fit(y_train)\n",
    "\n",
    "X_train = pipe_x.transform(X_train)\n",
    "X_test = pipe_x.transform(X_test)\n",
    "try:\n",
    "    X_hold = pipe_x.transform(X_hold)\n",
    "except:\n",
    "    pass\n",
    "# y_train = pipe_y.fit_transform(y_train)\n",
    "\n",
    "# \"\"\"\n",
    "\n",
    "# small_n = 5000\n",
    "# X_train = X_train[:small_n,:]\n",
    "# y_train = y_train[:small_n]\n",
    "# y_binned = y_binned[:small_n]\n",
    "\n",
    "print 'y_train', y_train.shape\n",
    "print 'X_train', X_train.shape\n",
    "print 'X_test', X_test.shape\n",
    "try:\n",
    "    print 'X_hold', X_hold.shape\n",
    "except:\n",
    "    pass\n",
    "print len(np.unique(y_train))\n",
    "print len(np.unique(y_binned))\n",
    "print type(X_train[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.datasets import dump_svmlight_file, load_svmlight_file\n",
    "from svmlight_loader import dump_svmlight_file, load_svmlight_file\n",
    "mapped_train, _y = load_svmlight_file('saved/mapped2000_train.libsvm')\n",
    "\n",
    "X_2000mean = mapped_train.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50999, 2125)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_nn = np.c_[X_2000mean, X_train]\n",
    "X_nn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keras\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout, MaxoutDense, Reshape\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.wrappers.scikit_learn import KerasClassifier, KerasRegressor\n",
    "from keras.optimizers import Adadelta, Adagrad, Adam, RMSprop\n",
    "from keras.layers.advanced_activations import ParametricSoftplus, PReLU\n",
    "\n",
    "n_feats = X_nn.shape[1]\n",
    "drop_prob = 0.6\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "\n",
    "hidden_size = 1024\n",
    "model.add(Dense(n_feats, hidden_size))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "model.add(BatchNormalization((hidden_size,)))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, hidden_size))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "model.add(BatchNormalization((hidden_size,)))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, hidden_size))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "model.add(BatchNormalization((hidden_size,)))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, hidden_size))\n",
    "model.add(ParametricSoftplus(hidden_size))\n",
    "model.add(BatchNormalization((hidden_size,)))\n",
    "model.add(Dropout(drop_prob))\n",
    "model.add(Dense(hidden_size, 1))\n",
    "model.add(Activation('linear'))\n",
    "\n",
    "loss_type = 'msle'\n",
    "\n",
    "opt = RMSprop(lr=0.0005, rho=0.75, epsilon=1e-6)\n",
    "\n",
    "# model.compile(loss=loss_type, optimizer=opt)\n",
    "# model.save_weights('saved/nn_weights', overwrite=True)\n",
    "\n",
    "clf_nn = KerasRegressor(model, optimizer=opt, loss=loss_type,\n",
    "                       train_batch_size=1024*4, test_batch_size=1024*16,\n",
    "                       nb_epoch=500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# GBM models\n",
    "\n",
    "gbm = xgb.XGBRegressor(\n",
    "    objective=\"reg:linear\",\n",
    "    n_estimators=900,\n",
    "    learning_rate=0.005,\n",
    "#     gamma=0.0,\n",
    "    max_depth=9,\n",
    "    min_child_weight=6,\n",
    "#     max_delta_step=10,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "#     scale_pos_weight=1.0,\n",
    "    base_score=0.5,\n",
    "    nthread=7,\n",
    "    seed=np.random.randint(0,100),\n",
    "    silent=True,\n",
    ")\n",
    "\n",
    "meta_gbm = xgb.XGBRegressor(\n",
    "    objective=\"reg:linear\",\n",
    "    n_estimators=750,\n",
    "    learning_rate=0.005,\n",
    "#     gamma=0.0,\n",
    "    max_depth=9,\n",
    "    min_child_weight=6,\n",
    "#     max_delta_step=10,\n",
    "    subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "#     scale_pos_weight=1.0,\n",
    "    base_score=0.5,\n",
    "    nthread=7,\n",
    "    seed=np.random.randint(0,100),\n",
    "    silent=True,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "gbm_bin = xgb.XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "#     gamma=0.0,\n",
    "    max_depth=8,\n",
    "    min_child_weight=5,\n",
    "#     max_delta_step=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "#     scale_pos_weight=1.0,\n",
    "    base_score=0.5,\n",
    "    nthread=8,\n",
    "    seed=np.random.randint(0,100),\n",
    "    silent=True,\n",
    ")\n",
    "\n",
    "gbm_cat = xgb.XGBClassifier(\n",
    "    objective=\"binary:softmax\",\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.01,\n",
    "#     gamma=0.0,\n",
    "    max_depth=8,\n",
    "    min_child_weight=5,\n",
    "#     max_delta_step=10,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "#     scale_pos_weight=1.0,\n",
    "    base_score=0.5,\n",
    "    nthread=8,\n",
    "    seed=np.random.randint(0,100),\n",
    "    silent=True,\n",
    ")\n",
    "\n",
    "gbm_fast = xgb.XGBClassifier(\n",
    "    objective=\"binary:softmax\",\n",
    "    n_estimators=50,\n",
    "    learning_rate=0.3,\n",
    "#     gamma=0.0,\n",
    "    max_depth=8,\n",
    "    min_child_weight=5,\n",
    "#     max_delta_step=10,\n",
    "    subsample=1.0,\n",
    "    colsample_bytree=1.0,\n",
    "#     scale_pos_weight=1.0,\n",
    "    base_score=0.5,\n",
    "    nthread=8,\n",
    "    seed=np.random.randint(0,100),\n",
    "    silent=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rasco:RASCO Init\n",
      "INFO:rasco:RASCO Init\n",
      "INFO:rasco:RASCO Init\n",
      "INFO:rasco:RASCO Init\n",
      "INFO:rasco:RASCO Init\n",
      "INFO:stacker:Stacker Init\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.901985\n",
      "DEBUG:rasco:Iter #0 time: 5.80886\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.908127\n",
      "DEBUG:rasco:Iter #1 time: 5.84686\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.894073\n",
      "DEBUG:rasco:Iter #2 time: 6.03353\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.893093\n",
      "DEBUG:rasco:Iter #3 time: 5.94141\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.901929\n",
      "DEBUG:rasco:Iter #4 time: 5.91349\n",
      "DEBUG:rasco:Total time: 29.5478\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.903516\n",
      "DEBUG:rasco:Iter #0 time: 5.80821\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.914373\n",
      "DEBUG:rasco:Iter #1 time: 5.91069\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.909786\n",
      "DEBUG:rasco:Iter #2 time: 5.9186\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.920266\n",
      "DEBUG:rasco:Iter #3 time: 5.92052\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.889757\n",
      "DEBUG:rasco:Iter #4 time: 5.91841\n",
      "DEBUG:rasco:Total time: 29.4801\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.935292\n",
      "DEBUG:rasco:Iter #0 time: 5.70421\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.941838\n",
      "DEBUG:rasco:Iter #1 time: 5.81137\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.939819\n",
      "DEBUG:rasco:Iter #2 time: 5.82137\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.938244\n",
      "DEBUG:rasco:Iter #3 time: 5.83485\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.932569\n",
      "DEBUG:rasco:Iter #4 time: 5.8079\n",
      "DEBUG:rasco:Total time: 28.9834\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.959768\n",
      "DEBUG:rasco:Iter #0 time: 5.82154\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.963889\n",
      "DEBUG:rasco:Iter #1 time: 5.9184\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.961023\n",
      "DEBUG:rasco:Iter #2 time: 5.92495\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.961213\n",
      "DEBUG:rasco:Iter #3 time: 5.92018\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.962497\n",
      "DEBUG:rasco:Iter #4 time: 5.93021\n",
      "DEBUG:rasco:Total time: 29.5189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned OFF\n",
      "Fold 0 Train time: 117.654 s\tPred time: 33.352 s\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.914504\n",
      "DEBUG:rasco:Iter #0 time: 5.81148\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.903988\n",
      "DEBUG:rasco:Iter #1 time: 5.93031\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.902313\n",
      "DEBUG:rasco:Iter #2 time: 5.96014\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.910863\n",
      "DEBUG:rasco:Iter #3 time: 5.94718\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.90813\n",
      "DEBUG:rasco:Iter #4 time: 5.92928\n",
      "DEBUG:rasco:Total time: 29.582\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.908552\n",
      "DEBUG:rasco:Iter #0 time: 5.81869\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.904668\n",
      "DEBUG:rasco:Iter #1 time: 5.92255\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.910663\n",
      "DEBUG:rasco:Iter #2 time: 5.93496\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.905872\n",
      "DEBUG:rasco:Iter #3 time: 5.97166\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.898121\n",
      "DEBUG:rasco:Iter #4 time: 5.91031\n",
      "DEBUG:rasco:Total time: 29.5619\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.943556\n",
      "DEBUG:rasco:Iter #0 time: 5.81399\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.950666\n",
      "DEBUG:rasco:Iter #1 time: 5.83397\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.95037\n",
      "DEBUG:rasco:Iter #2 time: 5.9234\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.933103\n",
      "DEBUG:rasco:Iter #3 time: 5.81424\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.942533\n",
      "DEBUG:rasco:Iter #4 time: 5.81829\n",
      "DEBUG:rasco:Total time: 29.2075\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.974881\n",
      "DEBUG:rasco:Iter #0 time: 5.83509\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.973592\n",
      "DEBUG:rasco:Iter #1 time: 5.91863\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.980708\n",
      "DEBUG:rasco:Iter #2 time: 5.9237\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.969359\n",
      "DEBUG:rasco:Iter #3 time: 5.91185\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.9823\n",
      "DEBUG:rasco:Iter #4 time: 5.92807\n",
      "DEBUG:rasco:Total time: 29.521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score [0.20677959061209183]\n",
      "Score avg ensemble 0.206779590612\n",
      "Fold 1 Train time: 117.999 s\tPred time: 33.515 s\t"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.920834\n",
      "DEBUG:rasco:Iter #0 time: 5.94445\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.904558\n",
      "DEBUG:rasco:Iter #1 time: 5.93389\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.908231\n",
      "DEBUG:rasco:Iter #2 time: 5.90467\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.904302\n",
      "DEBUG:rasco:Iter #3 time: 5.9072\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.926705\n",
      "DEBUG:rasco:Iter #4 time: 6.03077\n",
      "DEBUG:rasco:Total time: 29.7246\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.915491\n",
      "DEBUG:rasco:Iter #0 time: 5.81465\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.910574\n",
      "DEBUG:rasco:Iter #1 time: 5.93479\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.902904\n",
      "DEBUG:rasco:Iter #2 time: 5.95958\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.886849\n",
      "DEBUG:rasco:Iter #3 time: 5.91066\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.907669\n",
      "DEBUG:rasco:Iter #4 time: 5.95429\n",
      "DEBUG:rasco:Total time: 29.5776\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.946115\n",
      "DEBUG:rasco:Iter #0 time: 5.84252\n",
      "DEBUG:rasco:Best candidate: pred_class=1 | Prob: 0.937545\n",
      "DEBUG:rasco:Iter #1 time: 5.93953\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.935726\n",
      "DEBUG:rasco:Iter #2 time: 5.80888\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.938617\n",
      "DEBUG:rasco:Iter #3 time: 5.91905\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.94478\n",
      "DEBUG:rasco:Iter #4 time: 5.94076\n",
      "DEBUG:rasco:Total time: 29.4544\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.973174\n",
      "DEBUG:rasco:Iter #0 time: 5.83199\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.968924\n",
      "DEBUG:rasco:Iter #1 time: 5.80665\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.97118\n",
      "DEBUG:rasco:Iter #2 time: 5.93468\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.968114\n",
      "DEBUG:rasco:Iter #3 time: 5.80552\n",
      "DEBUG:rasco:Best candidate: pred_class=0 | Prob: 0.965776\n",
      "DEBUG:rasco:Iter #4 time: 5.81932\n",
      "DEBUG:rasco:Total time: 29.2018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score [0.19520612253275124]\n",
      "Score avg ensemble 0.195206122533\n",
      "Fold 2 Train time: 118.100 s\tPred time: 33.402 s\tScore [0.20393425330556705]\n",
      "Score avg ensemble 0.203934253306\n",
      "done\n",
      "Score: [ 0.20197332]\n",
      "Base Score: 0.20197332215\n",
      "lol [-0.008757804121674033, -0.0022976210660010515, -0.01221072178961156] -0.00775538232576\n"
     ]
    }
   ],
   "source": [
    "from sklearn.base import clone\n",
    "%pdb off\n",
    "\n",
    "rasco_nb = rasco.Rasco(base_estimator=GaussianNB(),\n",
    "                       n_estimators=32, max_iters=20,\n",
    "                       verbose=True,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "rasco_dt = rasco.Rasco(base_estimator=DecisionTreeClassifier(),\n",
    "                       n_estimators=128, max_iters=20,\n",
    "                       verbose=True,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "rasco_gbm = rasco.Rasco(base_estimator=gbm_cat,\n",
    "                       n_estimators=8, max_iters=20,\n",
    "                       verbose=True,\n",
    "                      n_jobs=1)\n",
    "rasco_logistic = rasco.Rasco(base_estimator=LogisticRegression(),\n",
    "                            n_estimators=8, max_iters=10,\n",
    "                            verbose=True,\n",
    "                            n_jobs=-1)\n",
    "\n",
    "h = [GaussianNB() for _ in range(64)] + \\\n",
    "    [DecisionTreeClassifier(max_depth=6, min_samples_split=5, class_weight='auto') for _ in range(16)] + \\\n",
    "    [SGDClassifier(n_iter=20, loss='modified_huber', class_weight='auto', n_jobs=1) \n",
    "     for _ in range(8)] + \\\n",
    "    [ExtraTreeClassifier(max_depth=6, min_samples_split=6, class_weight='auto')\n",
    "     for _ in range(16)]\n",
    "\n",
    "\n",
    "rasco_mix = rasco.Rasco(h,\n",
    "                        feat_ratio=0.6,\n",
    "                       n_estimators=104, max_iters=5,\n",
    "                       verbose=True,\n",
    "                      n_jobs=-1)\n",
    "\n",
    "ord_rasco = simple.SimpleOrdinalClassifier(rasco_mix, n_jobs=1)\n",
    "\n",
    "ord_gbm_bin = simple.SimpleOrdinalClassifier(gbm_bin, n_jobs=1)\n",
    "ord_gaussnb_bin = simple.SimpleOrdinalClassifier(GaussianNB(), n_jobs=-1)\n",
    "ord_logistic_bin = simple.SimpleOrdinalClassifier(LogisticRegression(), n_jobs=-1)\n",
    "ord_rf_bin = simple.SimpleOrdinalClassifier(RandomForestClassifier(n_estimators=200, n_jobs=-1), n_jobs=1)\n",
    "ord_ada_bin = simple.SimpleOrdinalClassifier(AdaBoostClassifier(n_estimators=200), n_jobs=-1)\n",
    "ord_dt = simple.SimpleOrdinalClassifier(DecisionTreeClassifier(), n_jobs=-1)\n",
    "ord_lda = simple.SimpleOrdinalClassifier(LDA(solver='lsqr'), n_jobs=-1)\n",
    "\n",
    "# clf0 = simple.SimpleOrdinalClassifier(base_estimator_type=GaussianNB, base_estimator_params={}, n_jobs=1)\n",
    "# clf0 = simple.SimpleOrdinalClassifier(SVC(probability=True), n_jobs=8)\n",
    "# clf0 = simple.SimpleOrdinalClassifier(clf_nn, n_jobs=1)\n",
    "# clf0 = simple.SimpleOrdinalClassifier(base_estimator_type=KerasClassifier,\n",
    "#                                       base_estimator_params=nn_params,\n",
    "#                                       label_transformer=label_bin_tform,\n",
    "#                                       n_jobs=1)\n",
    "\n",
    "rf = RandomForestRegressor(n_jobs=-1, **{'max_leaf_nodes': None, \n",
    "                                         'bootstrap': False, \n",
    "                                         'min_samples_leaf': 8, \n",
    "                                         'n_estimators': 150, \n",
    "                                         'min_samples_split': 8, \n",
    "                                         'min_weight_fraction_leaf': 0.25, \n",
    "                                         'max_features': 'sqrt', \n",
    "                                         'max_depth': 12} )\n",
    "\n",
    "et = ExtraTreesRegressor(n_jobs=-1, **{'max_leaf_nodes': None, \n",
    "                                         'bootstrap': False, \n",
    "                                         'min_samples_leaf': 8, \n",
    "                                         'n_estimators': 150, \n",
    "                                         'min_samples_split': 8, \n",
    "                                         'min_weight_fraction_leaf': 0.25, \n",
    "                                         'max_features': 'sqrt', \n",
    "                                         'max_depth': 12} )\n",
    "\n",
    "stack = stacking.Stacking([\n",
    "        ord_gaussnb_bin,\n",
    "        ord_lda,\n",
    "        rf,\n",
    "        clf_nn,\n",
    "#         ord_rf_bin,\n",
    "#         ord_ada_bin,\n",
    "#         ord_logistic_bin,\n",
    "#         ord_gbm_bin,\n",
    "#         ann.ann(no_trees=50, num_neighbors=100, weights='distance'),\n",
    "#         KNeighborsRegressor(n_neighbors=20),\n",
    "\n",
    "        LinearRegression(),\n",
    "        Ridge(), \n",
    "#         BaggingRegressor(n_estimators=400, max_features=0.8, n_jobs=-1),\n",
    "#         ExtraTreesRegressor(n_estimators=400, n_jobs=-1), \n",
    "        \n",
    "#         gbm,\n",
    "    ],\n",
    "                          meta_gbm,\n",
    "#                           LinearRegression(),\n",
    "#                           gbm,\n",
    "                          fit_params={\n",
    "        'base0_y': 'y_binned',\n",
    "        'base1_y': 'y_binned',\n",
    "        'base3_X': 'X_nn_train',\n",
    "#         'base3_fit_batch_size': 1024*16,\n",
    "#         'base3_predict_batch_size': 1024*16,\n",
    "#         'base3_fit_nb_epoch': 500,\n",
    "        \n",
    "#         'base2_y': 'y_binned',\n",
    "    },\n",
    "                          pred_params={\n",
    "        'base3_X': 'X_nn_val',\n",
    "#         'base3_predict_batch_size': 1024*16,\n",
    "        \n",
    "#         'base0_weighted':True,\n",
    "#         'base1_weighted':True,\n",
    "    },\n",
    "                          include_orig_feats=False,\n",
    "                          use_probs=True,\n",
    "                          cv=8,\n",
    "                          verbose=1,\n",
    "                          save_level0_out=True,\n",
    "                          log_handler=fh,\n",
    "                         )\n",
    "# clfs = [clf0, gbm] \n",
    "# clfs = [clone(gbm), stack]\n",
    "# clfs = [LinearRegression(), stack]\n",
    "# clfs = [stack]\n",
    "# clfs = [BaggingRegressor(n_estimators=200, max_features=0.8, n_jobs=-1)]\n",
    "\n",
    "# clfs = [stack]\n",
    "clfs = [ord_rasco]\n",
    "# clfs = [rasco_mix]\n",
    "# clfs = []\n",
    "# clfs = [LinearRegression()]\n",
    "# clfs = [rf]\n",
    "\n",
    "\n",
    "\n",
    "# clfs = [rpfnn.rpfnn(leaf_size=100, no_trees=10, num_neighbors=100)]\n",
    "# clfs = [ann.ann(no_trees=50, num_neighbors=100, weights='distance')]\n",
    "\n",
    "# clfs = [ord_gaussnb_bin]\n",
    "# clfs = [ord_lda]\n",
    "# clfs = [simple.SimpleOrdinalClassifier(LDA(solver='lsqr'), n_jobs=-1)]\n",
    "# clfs = [clf1, clf2, ]\n",
    "\n",
    "\n",
    "# h = [clone(clf1), clone(clf2)]\n",
    "# clf = coreg.CoReg(h=h, T=5, verbose=True, n_jobs=1)\n",
    "\n",
    "scores = []\n",
    "scores_base = []\n",
    "lols = []\n",
    "n_reps = 1\n",
    "k = 3\n",
    "for reps in range(n_reps):\n",
    "    skf = StratifiedKFold(y_binned, n_folds=k,\n",
    "                          shuffle=True,\n",
    "                          random_state=np.random.randint(0,100))\n",
    "    for ii, (train, valid) in enumerate(skf):\n",
    "        \n",
    "        \n",
    "#         history = LossHistory()   # for keras\n",
    "        print 'Fold %d' % ii,\n",
    "        X_train_k, X_valid_k = X_train[train], X_train[valid]\n",
    "#         X_train_k, X_valid_k = X_fa40_train[train], X_fa40_train[valid]\n",
    "        y_train_k, y_valid_k = y_train[train], y_train[valid]\n",
    "        y_train_binned_k, y_valid_binned_k = y_binned[train], y_binned[valid]\n",
    "        \n",
    "        X_nn_train_k, X_nn_valid_k = X_nn[train], X_nn[valid]\n",
    "        \n",
    "        \n",
    "        X_all = np.r_[X_train_k, X_valid_k]\n",
    "        y_all = np.r_[y_train_k, np.nan*np.ones(len(y_valid_k))]\n",
    "        y_binned_all = np.r_[y_train_binned_k, np.nan*np.ones(len(y_valid_binned_k))]\n",
    "        \n",
    "        tic = time()\n",
    "#         clf.fit(X_train_k, y_train_k)\n",
    "#         clf.fit(X_all, y_all)\n",
    "\n",
    "\n",
    "        stack.extra_data['y_binned'] = y_train_binned_k\n",
    "        stack.extra_data['X_nn_train'] = X_nn_train_k\n",
    "        stack.extra_data['X_nn_val'] = X_nn_valid_k\n",
    "        \n",
    "        rasco_mix.y_val = y_valid_binned_k - 1\n",
    "        for clf in clfs:\n",
    "            clf.fit(X_all, y_binned_all-1)\n",
    "#             clf.fit(X_train_k, pipe_y.transform(y_train_k))\n",
    "#             clf.fit(X_train_k, y_train_binned_k-1)\n",
    "#             clf.fit(X_train_k, y_train_binned_k,\n",
    "#                    nb_epoch=60, batch_size=1024*16, )\n",
    "#             clf.fit(X_train_k, y_train_k, \n",
    "#                     base0_y=y_train_binned_k,\n",
    "#                     base1_y=y_train_binned_k,\n",
    "# #                     base2_y=y_train_binned_k,\n",
    "#                    )\n",
    "    \n",
    "    \n",
    "        \n",
    "        # Minirank\n",
    "#         w, theta = mr.ordinal_logistic_fit(X_train_k, y_train_k, verbose=False,\n",
    "#                                 solver='TNC')\n",
    "\n",
    "        \n",
    "        toc = time() - tic\n",
    "        print 'Train time: %2.3f s\\t' % toc,\n",
    "        tic = time()\n",
    "\n",
    "        \n",
    "        \n",
    "#         valid_preds = [clf.predict_weighted(X_valid_k,) \n",
    "#                        if hasattr(clf, 'predict_weighted') \n",
    "#                        else clf.predict(X_valid_k)\n",
    "#                        for clf in clfs]\n",
    "        \n",
    "        valid_preds = [clf.predict(X_valid_k) for clf in clfs]\n",
    "        \n",
    "#         valid_preds = [clf.predict_weighted(X_valid_k, batch_size=1024*16) \n",
    "#                if hasattr(clf, 'predict_weighted') \n",
    "#                else clf.predict(X_valid_k)\n",
    "#                for clf in clfs]\n",
    "        \n",
    "        try:\n",
    "            lol = metrics.normalized_gini(y_valid_k, \n",
    "                                          clfs[0].predict_weighted(X_valid_k, geometric=True))\n",
    "            lols.append(lol)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        # Minirank\n",
    "#         valid_preds = mr.ordinal_logistic_predict(w, theta, X_valid_k)\n",
    "        \n",
    "        valid_base_preds = np.mean([clf.predict(X_valid_k) for clf in clfs\n",
    "                                   ], \n",
    "                                   axis=0)\n",
    "\n",
    "        \n",
    "        score = [metrics.normalized_gini(y_valid_k, v) for v in valid_preds]\n",
    "        score_base = metrics.normalized_gini(y_valid_k, valid_base_preds)\n",
    "        \n",
    "        toc_pred = time() - tic\n",
    "        print 'Pred time: %2.3f s\\t' % toc_pred, \n",
    "        \n",
    "        print 'Score', score\n",
    "        print 'Score avg ensemble', score_base\n",
    "        scores.append(score)\n",
    "        scores_base.append(score_base)\n",
    "        \n",
    "#         break\n",
    "        \n",
    "        \n",
    "print \"done\"\n",
    "print 'Score:', np.array(scores).mean(axis=0)\n",
    "print 'Base Score:', np.array(scores_base).mean()\n",
    "print 'lol', lols, np.mean(lols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf = clfs[0]\n",
    "q = clf.predict_proba(X_valid_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 7, ..., 4, 3, 9])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "est = stack.base_estimators[3]\n",
    "est.config_['layers'][-1]['output_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print y_train_k.shape\n",
    "print y_train_binned_k.shape\n",
    "# gbm is ~ 0.381  @ 30 sec per fold?\n",
    "# gbm on binned labels ~ 0.378 @ 30 sec per fold... ~_~\n",
    "# ordinal nn binned is like 0.34 with stdscaling X 60 epochs\n",
    "\n",
    "# the stack is worse?????? 0.379\n",
    "# added nn\n",
    "# stack including features meta gbm 0.369\n",
    "# move gbm to base and dont includebase ests (use linreg for meta) -> 0.377\n",
    "# increase folds cv=8 -> 0.379"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: [ 0.37916189]\n",
      "Base Score: 0.379161888136\n",
      "lol [] nan\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print 'Score:', np.array(scores).mean(axis=0)\n",
    "print 'Base Score:', np.array(scores_base).mean()\n",
    "print 'lol', lols, np.mean(lols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metrics.normalized_gini(y_valid_k, clfs[0].predict_weighted(X_valid_k, geometric=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print y_pred_base_hold.shape\n",
    "print y_hold[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33998, 14)\n",
      "(33998,)\n"
     ]
    }
   ],
   "source": [
    "X_stack = stack.level0_out[0]\n",
    "y_stack = stack.level0_out[1]\n",
    "print X_stack.shape\n",
    "print y_stack.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.02278476049\n",
      "2.35284428496\n",
      "2.35945290095\n"
     ]
    }
   ],
   "source": [
    "print y_train.mean()\n",
    "print y_stack.mean()\n",
    "print y_train_k.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 120 rounds.\n",
      "[0]\ttrain-rmse:2.233049\tval-rmse:2.233330\n",
      "[1]\ttrain-rmse:2.224972\tval-rmse:2.225478\n",
      "[2]\ttrain-rmse:2.216940\tval-rmse:2.217663\n",
      "[3]\ttrain-rmse:2.208930\tval-rmse:2.209854\n",
      "[4]\ttrain-rmse:2.201055\tval-rmse:2.202188\n",
      "[5]\ttrain-rmse:2.193213\tval-rmse:2.194567\n",
      "[6]\ttrain-rmse:2.185442\tval-rmse:2.187056\n",
      "[7]\ttrain-rmse:2.177714\tval-rmse:2.179523\n",
      "[8]\ttrain-rmse:2.169972\tval-rmse:2.172019\n",
      "[9]\ttrain-rmse:2.162337\tval-rmse:2.164564\n",
      "[10]\ttrain-rmse:2.154759\tval-rmse:2.157186\n",
      "[11]\ttrain-rmse:2.147175\tval-rmse:2.149861\n",
      "[12]\ttrain-rmse:2.139671\tval-rmse:2.142588\n",
      "[13]\ttrain-rmse:2.132219\tval-rmse:2.135376\n",
      "[14]\ttrain-rmse:2.124771\tval-rmse:2.128183\n",
      "[15]\ttrain-rmse:2.117374\tval-rmse:2.121008\n",
      "[16]\ttrain-rmse:2.110065\tval-rmse:2.113907\n",
      "[17]\ttrain-rmse:2.102777\tval-rmse:2.106839\n",
      "[18]\ttrain-rmse:2.095571\tval-rmse:2.099890\n",
      "[19]\ttrain-rmse:2.088372\tval-rmse:2.092894\n",
      "[20]\ttrain-rmse:2.081231\tval-rmse:2.085951\n",
      "[21]\ttrain-rmse:2.074061\tval-rmse:2.079023\n",
      "[22]\ttrain-rmse:2.066958\tval-rmse:2.072139\n",
      "[23]\ttrain-rmse:2.059922\tval-rmse:2.065351\n",
      "[24]\ttrain-rmse:2.052948\tval-rmse:2.058593\n",
      "[25]\ttrain-rmse:2.046000\tval-rmse:2.051887\n",
      "[26]\ttrain-rmse:2.039125\tval-rmse:2.045192\n",
      "[27]\ttrain-rmse:2.032261\tval-rmse:2.038549\n",
      "[28]\ttrain-rmse:2.025515\tval-rmse:2.032025\n",
      "[29]\ttrain-rmse:2.018782\tval-rmse:2.025505\n",
      "[30]\ttrain-rmse:2.012036\tval-rmse:2.019048\n",
      "[31]\ttrain-rmse:2.005337\tval-rmse:2.012621\n",
      "[32]\ttrain-rmse:1.998740\tval-rmse:2.006221\n",
      "[33]\ttrain-rmse:1.992128\tval-rmse:1.999818\n",
      "[34]\ttrain-rmse:1.985605\tval-rmse:1.993545\n",
      "[35]\ttrain-rmse:1.979126\tval-rmse:1.987322\n",
      "[36]\ttrain-rmse:1.972642\tval-rmse:1.981073\n",
      "[37]\ttrain-rmse:1.966219\tval-rmse:1.974937\n",
      "[38]\ttrain-rmse:1.959824\tval-rmse:1.968790\n",
      "[39]\ttrain-rmse:1.953468\tval-rmse:1.962722\n",
      "[40]\ttrain-rmse:1.947179\tval-rmse:1.956670\n",
      "[41]\ttrain-rmse:1.940971\tval-rmse:1.950695\n",
      "[42]\ttrain-rmse:1.934765\tval-rmse:1.944752\n",
      "[43]\ttrain-rmse:1.928599\tval-rmse:1.938822\n",
      "[44]\ttrain-rmse:1.922475\tval-rmse:1.932956\n",
      "[45]\ttrain-rmse:1.916353\tval-rmse:1.927111\n",
      "[46]\ttrain-rmse:1.910286\tval-rmse:1.921278\n",
      "[47]\ttrain-rmse:1.904217\tval-rmse:1.915511\n",
      "[48]\ttrain-rmse:1.898194\tval-rmse:1.909757\n",
      "[49]\ttrain-rmse:1.892211\tval-rmse:1.904055\n",
      "[50]\ttrain-rmse:1.886346\tval-rmse:1.898459\n",
      "[51]\ttrain-rmse:1.880439\tval-rmse:1.892811\n",
      "[52]\ttrain-rmse:1.874576\tval-rmse:1.887172\n",
      "[53]\ttrain-rmse:1.868758\tval-rmse:1.881602\n",
      "[54]\ttrain-rmse:1.862957\tval-rmse:1.876059\n",
      "[55]\ttrain-rmse:1.857267\tval-rmse:1.870621\n",
      "[56]\ttrain-rmse:1.851536\tval-rmse:1.865200\n",
      "[57]\ttrain-rmse:1.845876\tval-rmse:1.859782\n",
      "[58]\ttrain-rmse:1.840278\tval-rmse:1.854449\n",
      "[59]\ttrain-rmse:1.834755\tval-rmse:1.849140\n",
      "[60]\ttrain-rmse:1.829211\tval-rmse:1.843852\n",
      "[61]\ttrain-rmse:1.823702\tval-rmse:1.838605\n",
      "[62]\ttrain-rmse:1.818242\tval-rmse:1.833433\n",
      "[63]\ttrain-rmse:1.812863\tval-rmse:1.828314\n",
      "[64]\ttrain-rmse:1.807478\tval-rmse:1.823200\n",
      "[65]\ttrain-rmse:1.802171\tval-rmse:1.818118\n",
      "[66]\ttrain-rmse:1.796861\tval-rmse:1.813035\n",
      "[67]\ttrain-rmse:1.791547\tval-rmse:1.808031\n",
      "[68]\ttrain-rmse:1.786327\tval-rmse:1.803082\n",
      "[69]\ttrain-rmse:1.781160\tval-rmse:1.798176\n",
      "[70]\ttrain-rmse:1.775995\tval-rmse:1.793265\n",
      "[71]\ttrain-rmse:1.770838\tval-rmse:1.788389\n",
      "[72]\ttrain-rmse:1.765732\tval-rmse:1.783497\n",
      "[73]\ttrain-rmse:1.760675\tval-rmse:1.778684\n",
      "[74]\ttrain-rmse:1.755662\tval-rmse:1.773950\n",
      "[75]\ttrain-rmse:1.750673\tval-rmse:1.769203\n",
      "[76]\ttrain-rmse:1.745729\tval-rmse:1.764562\n",
      "[77]\ttrain-rmse:1.740782\tval-rmse:1.759903\n",
      "[78]\ttrain-rmse:1.735878\tval-rmse:1.755268\n",
      "[79]\ttrain-rmse:1.731023\tval-rmse:1.750652\n",
      "[80]\ttrain-rmse:1.726188\tval-rmse:1.746070\n",
      "[81]\ttrain-rmse:1.721389\tval-rmse:1.741566\n",
      "[82]\ttrain-rmse:1.716646\tval-rmse:1.737084\n",
      "[83]\ttrain-rmse:1.711918\tval-rmse:1.732587\n",
      "[84]\ttrain-rmse:1.707255\tval-rmse:1.728195\n",
      "[85]\ttrain-rmse:1.702554\tval-rmse:1.723812\n",
      "[86]\ttrain-rmse:1.697884\tval-rmse:1.719465\n",
      "[87]\ttrain-rmse:1.693303\tval-rmse:1.715150\n",
      "[88]\ttrain-rmse:1.688712\tval-rmse:1.710850\n",
      "[89]\ttrain-rmse:1.684183\tval-rmse:1.706623\n",
      "[90]\ttrain-rmse:1.679686\tval-rmse:1.702386\n",
      "[91]\ttrain-rmse:1.675175\tval-rmse:1.698132\n",
      "[92]\ttrain-rmse:1.670770\tval-rmse:1.693976\n",
      "[93]\ttrain-rmse:1.666334\tval-rmse:1.689818\n",
      "[94]\ttrain-rmse:1.661943\tval-rmse:1.685700\n",
      "[95]\ttrain-rmse:1.657590\tval-rmse:1.681646\n",
      "[96]\ttrain-rmse:1.653232\tval-rmse:1.677592\n",
      "[97]\ttrain-rmse:1.648925\tval-rmse:1.673544\n",
      "[98]\ttrain-rmse:1.644706\tval-rmse:1.669594\n",
      "[99]\ttrain-rmse:1.640474\tval-rmse:1.665671\n",
      "[100]\ttrain-rmse:1.636225\tval-rmse:1.661667\n",
      "[101]\ttrain-rmse:1.632020\tval-rmse:1.657711\n",
      "[102]\ttrain-rmse:1.627872\tval-rmse:1.653823\n",
      "[103]\ttrain-rmse:1.623764\tval-rmse:1.649977\n",
      "[104]\ttrain-rmse:1.619663\tval-rmse:1.646201\n",
      "[105]\ttrain-rmse:1.615640\tval-rmse:1.642443\n",
      "[106]\ttrain-rmse:1.611582\tval-rmse:1.638683\n",
      "[107]\ttrain-rmse:1.607545\tval-rmse:1.634929\n",
      "[108]\ttrain-rmse:1.603536\tval-rmse:1.631194\n",
      "[109]\ttrain-rmse:1.599553\tval-rmse:1.627550\n",
      "[110]\ttrain-rmse:1.595638\tval-rmse:1.623909\n",
      "[111]\ttrain-rmse:1.591762\tval-rmse:1.620282\n",
      "[112]\ttrain-rmse:1.587900\tval-rmse:1.616681\n",
      "[113]\ttrain-rmse:1.584092\tval-rmse:1.613135\n",
      "[114]\ttrain-rmse:1.580274\tval-rmse:1.609612\n",
      "[115]\ttrain-rmse:1.576462\tval-rmse:1.606112\n",
      "[116]\ttrain-rmse:1.572774\tval-rmse:1.602653\n",
      "[117]\ttrain-rmse:1.569077\tval-rmse:1.599227\n",
      "[118]\ttrain-rmse:1.565419\tval-rmse:1.595862\n",
      "[119]\ttrain-rmse:1.561752\tval-rmse:1.592456\n",
      "[120]\ttrain-rmse:1.558128\tval-rmse:1.589114\n",
      "[121]\ttrain-rmse:1.554475\tval-rmse:1.585762\n",
      "[122]\ttrain-rmse:1.550865\tval-rmse:1.582465\n",
      "[123]\ttrain-rmse:1.547274\tval-rmse:1.579209\n",
      "[124]\ttrain-rmse:1.543745\tval-rmse:1.575969\n",
      "[125]\ttrain-rmse:1.540253\tval-rmse:1.572720\n",
      "[126]\ttrain-rmse:1.536800\tval-rmse:1.569494\n",
      "[127]\ttrain-rmse:1.533292\tval-rmse:1.566272\n",
      "[128]\ttrain-rmse:1.529855\tval-rmse:1.563090\n",
      "[129]\ttrain-rmse:1.526352\tval-rmse:1.559920\n",
      "[130]\ttrain-rmse:1.522895\tval-rmse:1.556775\n",
      "[131]\ttrain-rmse:1.519569\tval-rmse:1.553720\n",
      "[132]\ttrain-rmse:1.516235\tval-rmse:1.550626\n",
      "[133]\ttrain-rmse:1.512920\tval-rmse:1.547598\n",
      "[134]\ttrain-rmse:1.509607\tval-rmse:1.544552\n",
      "[135]\ttrain-rmse:1.506320\tval-rmse:1.541556\n",
      "[136]\ttrain-rmse:1.503111\tval-rmse:1.538576\n",
      "[137]\ttrain-rmse:1.499880\tval-rmse:1.535642\n",
      "[138]\ttrain-rmse:1.496670\tval-rmse:1.532700\n",
      "[139]\ttrain-rmse:1.493451\tval-rmse:1.529792\n",
      "[140]\ttrain-rmse:1.490291\tval-rmse:1.526947\n",
      "[141]\ttrain-rmse:1.487221\tval-rmse:1.524108\n",
      "[142]\ttrain-rmse:1.484124\tval-rmse:1.521276\n",
      "[143]\ttrain-rmse:1.481069\tval-rmse:1.518463\n",
      "[144]\ttrain-rmse:1.477977\tval-rmse:1.515684\n",
      "[145]\ttrain-rmse:1.474939\tval-rmse:1.512914\n",
      "[146]\ttrain-rmse:1.471930\tval-rmse:1.510180\n",
      "[147]\ttrain-rmse:1.468939\tval-rmse:1.507465\n",
      "[148]\ttrain-rmse:1.465964\tval-rmse:1.504774\n",
      "[149]\ttrain-rmse:1.463025\tval-rmse:1.502105\n",
      "[150]\ttrain-rmse:1.460118\tval-rmse:1.499449\n",
      "[151]\ttrain-rmse:1.457145\tval-rmse:1.496806\n",
      "[152]\ttrain-rmse:1.454225\tval-rmse:1.494209\n",
      "[153]\ttrain-rmse:1.451361\tval-rmse:1.491664\n",
      "[154]\ttrain-rmse:1.448489\tval-rmse:1.489081\n",
      "[155]\ttrain-rmse:1.445615\tval-rmse:1.486565\n",
      "[156]\ttrain-rmse:1.442804\tval-rmse:1.484031\n",
      "[157]\ttrain-rmse:1.440036\tval-rmse:1.481501\n",
      "[158]\ttrain-rmse:1.437272\tval-rmse:1.479002\n",
      "[159]\ttrain-rmse:1.434537\tval-rmse:1.476536\n",
      "[160]\ttrain-rmse:1.431830\tval-rmse:1.474049\n",
      "[161]\ttrain-rmse:1.429109\tval-rmse:1.471616\n",
      "[162]\ttrain-rmse:1.426418\tval-rmse:1.469250\n",
      "[163]\ttrain-rmse:1.423768\tval-rmse:1.466871\n",
      "[164]\ttrain-rmse:1.421127\tval-rmse:1.464579\n",
      "[165]\ttrain-rmse:1.418510\tval-rmse:1.462229\n",
      "[166]\ttrain-rmse:1.415913\tval-rmse:1.459863\n",
      "[167]\ttrain-rmse:1.413298\tval-rmse:1.457556\n",
      "[168]\ttrain-rmse:1.410658\tval-rmse:1.455291\n",
      "[169]\ttrain-rmse:1.408107\tval-rmse:1.453033\n",
      "[170]\ttrain-rmse:1.405526\tval-rmse:1.450769\n",
      "[171]\ttrain-rmse:1.402947\tval-rmse:1.448525\n",
      "[172]\ttrain-rmse:1.400463\tval-rmse:1.446317\n",
      "[173]\ttrain-rmse:1.397981\tval-rmse:1.444117\n",
      "[174]\ttrain-rmse:1.395496\tval-rmse:1.441922\n",
      "[175]\ttrain-rmse:1.393056\tval-rmse:1.439760\n",
      "[176]\ttrain-rmse:1.390635\tval-rmse:1.437642\n",
      "[177]\ttrain-rmse:1.388227\tval-rmse:1.435487\n",
      "[178]\ttrain-rmse:1.385787\tval-rmse:1.433340\n",
      "[179]\ttrain-rmse:1.383400\tval-rmse:1.431228\n",
      "[180]\ttrain-rmse:1.380991\tval-rmse:1.429179\n",
      "[181]\ttrain-rmse:1.378659\tval-rmse:1.427127\n",
      "[182]\ttrain-rmse:1.376363\tval-rmse:1.425116\n",
      "[183]\ttrain-rmse:1.374071\tval-rmse:1.423070\n",
      "[184]\ttrain-rmse:1.371851\tval-rmse:1.421062\n",
      "[185]\ttrain-rmse:1.369596\tval-rmse:1.419101\n",
      "[186]\ttrain-rmse:1.367284\tval-rmse:1.417145\n",
      "[187]\ttrain-rmse:1.365105\tval-rmse:1.415175\n",
      "[188]\ttrain-rmse:1.362895\tval-rmse:1.413239\n",
      "[189]\ttrain-rmse:1.360669\tval-rmse:1.411308\n",
      "[190]\ttrain-rmse:1.358454\tval-rmse:1.409399\n",
      "[191]\ttrain-rmse:1.356218\tval-rmse:1.407509\n",
      "[192]\ttrain-rmse:1.354076\tval-rmse:1.405636\n",
      "[193]\ttrain-rmse:1.352006\tval-rmse:1.403805\n",
      "[194]\ttrain-rmse:1.349857\tval-rmse:1.401961\n",
      "[195]\ttrain-rmse:1.347741\tval-rmse:1.400113\n",
      "[196]\ttrain-rmse:1.345706\tval-rmse:1.398288\n",
      "[197]\ttrain-rmse:1.343627\tval-rmse:1.396458\n",
      "[198]\ttrain-rmse:1.341559\tval-rmse:1.394658\n",
      "[199]\ttrain-rmse:1.339507\tval-rmse:1.392915\n",
      "[200]\ttrain-rmse:1.337481\tval-rmse:1.391125\n",
      "[201]\ttrain-rmse:1.335444\tval-rmse:1.389376\n",
      "[202]\ttrain-rmse:1.333533\tval-rmse:1.387668\n",
      "[203]\ttrain-rmse:1.331641\tval-rmse:1.385963\n",
      "[204]\ttrain-rmse:1.329701\tval-rmse:1.384322\n",
      "[205]\ttrain-rmse:1.327788\tval-rmse:1.382631\n",
      "[206]\ttrain-rmse:1.325911\tval-rmse:1.380992\n",
      "[207]\ttrain-rmse:1.323993\tval-rmse:1.379367\n",
      "[208]\ttrain-rmse:1.322092\tval-rmse:1.377749\n",
      "[209]\ttrain-rmse:1.320202\tval-rmse:1.376155\n",
      "[210]\ttrain-rmse:1.318387\tval-rmse:1.374555\n",
      "[211]\ttrain-rmse:1.316549\tval-rmse:1.372974\n",
      "[212]\ttrain-rmse:1.314739\tval-rmse:1.371401\n",
      "[213]\ttrain-rmse:1.312954\tval-rmse:1.369817\n",
      "[214]\ttrain-rmse:1.311111\tval-rmse:1.368282\n",
      "[215]\ttrain-rmse:1.309351\tval-rmse:1.366780\n",
      "[216]\ttrain-rmse:1.307535\tval-rmse:1.365245\n",
      "[217]\ttrain-rmse:1.305827\tval-rmse:1.363737\n",
      "[218]\ttrain-rmse:1.304081\tval-rmse:1.362235\n",
      "[219]\ttrain-rmse:1.302340\tval-rmse:1.360796\n",
      "[220]\ttrain-rmse:1.300631\tval-rmse:1.359325\n",
      "[221]\ttrain-rmse:1.298886\tval-rmse:1.357862\n",
      "[222]\ttrain-rmse:1.297169\tval-rmse:1.356430\n",
      "[223]\ttrain-rmse:1.295449\tval-rmse:1.354994\n",
      "[224]\ttrain-rmse:1.293755\tval-rmse:1.353587\n",
      "[225]\ttrain-rmse:1.292083\tval-rmse:1.352172\n",
      "[226]\ttrain-rmse:1.290396\tval-rmse:1.350760\n",
      "[227]\ttrain-rmse:1.288791\tval-rmse:1.349395\n",
      "[228]\ttrain-rmse:1.287173\tval-rmse:1.348032\n",
      "[229]\ttrain-rmse:1.285553\tval-rmse:1.346686\n",
      "[230]\ttrain-rmse:1.283955\tval-rmse:1.345320\n",
      "[231]\ttrain-rmse:1.282346\tval-rmse:1.343977\n",
      "[232]\ttrain-rmse:1.280760\tval-rmse:1.342648\n",
      "[233]\ttrain-rmse:1.279177\tval-rmse:1.341352\n",
      "[234]\ttrain-rmse:1.277667\tval-rmse:1.340057\n",
      "[235]\ttrain-rmse:1.276135\tval-rmse:1.338817\n",
      "[236]\ttrain-rmse:1.274602\tval-rmse:1.337503\n",
      "[237]\ttrain-rmse:1.273038\tval-rmse:1.336231\n",
      "[238]\ttrain-rmse:1.271584\tval-rmse:1.334972\n",
      "[239]\ttrain-rmse:1.270102\tval-rmse:1.333740\n",
      "[240]\ttrain-rmse:1.268560\tval-rmse:1.332514\n",
      "[241]\ttrain-rmse:1.267130\tval-rmse:1.331306\n",
      "[242]\ttrain-rmse:1.265665\tval-rmse:1.330115\n",
      "[243]\ttrain-rmse:1.264197\tval-rmse:1.328915\n",
      "[244]\ttrain-rmse:1.262764\tval-rmse:1.327740\n",
      "[245]\ttrain-rmse:1.261352\tval-rmse:1.326584\n",
      "[246]\ttrain-rmse:1.259920\tval-rmse:1.325414\n",
      "[247]\ttrain-rmse:1.258499\tval-rmse:1.324258\n",
      "[248]\ttrain-rmse:1.257135\tval-rmse:1.323135\n",
      "[249]\ttrain-rmse:1.255681\tval-rmse:1.322034\n",
      "[250]\ttrain-rmse:1.254343\tval-rmse:1.320920\n",
      "[251]\ttrain-rmse:1.252971\tval-rmse:1.319824\n",
      "[252]\ttrain-rmse:1.251607\tval-rmse:1.318726\n",
      "[253]\ttrain-rmse:1.250261\tval-rmse:1.317656\n",
      "[254]\ttrain-rmse:1.248949\tval-rmse:1.316611\n",
      "[255]\ttrain-rmse:1.247581\tval-rmse:1.315523\n",
      "[256]\ttrain-rmse:1.246248\tval-rmse:1.314448\n",
      "[257]\ttrain-rmse:1.244958\tval-rmse:1.313404\n",
      "[258]\ttrain-rmse:1.243589\tval-rmse:1.312334\n",
      "[259]\ttrain-rmse:1.242358\tval-rmse:1.311320\n",
      "[260]\ttrain-rmse:1.241107\tval-rmse:1.310283\n",
      "[261]\ttrain-rmse:1.239872\tval-rmse:1.309259\n",
      "[262]\ttrain-rmse:1.238638\tval-rmse:1.308245\n",
      "[263]\ttrain-rmse:1.237393\tval-rmse:1.307235\n",
      "[264]\ttrain-rmse:1.236166\tval-rmse:1.306249\n",
      "[265]\ttrain-rmse:1.234934\tval-rmse:1.305263\n",
      "[266]\ttrain-rmse:1.233744\tval-rmse:1.304301\n",
      "[267]\ttrain-rmse:1.232514\tval-rmse:1.303344\n",
      "[268]\ttrain-rmse:1.231345\tval-rmse:1.302405\n",
      "[269]\ttrain-rmse:1.230165\tval-rmse:1.301457\n",
      "[270]\ttrain-rmse:1.228950\tval-rmse:1.300514\n",
      "[271]\ttrain-rmse:1.227832\tval-rmse:1.299591\n",
      "[272]\ttrain-rmse:1.226707\tval-rmse:1.298676\n",
      "[273]\ttrain-rmse:1.225588\tval-rmse:1.297769\n",
      "[274]\ttrain-rmse:1.224490\tval-rmse:1.296879\n",
      "[275]\ttrain-rmse:1.223339\tval-rmse:1.295970\n",
      "[276]\ttrain-rmse:1.222218\tval-rmse:1.295116\n",
      "[277]\ttrain-rmse:1.221125\tval-rmse:1.294237\n",
      "[278]\ttrain-rmse:1.220102\tval-rmse:1.293375\n",
      "[279]\ttrain-rmse:1.219005\tval-rmse:1.292492\n",
      "[280]\ttrain-rmse:1.217935\tval-rmse:1.291630\n",
      "[281]\ttrain-rmse:1.216885\tval-rmse:1.290789\n",
      "[282]\ttrain-rmse:1.215872\tval-rmse:1.289951\n",
      "[283]\ttrain-rmse:1.214809\tval-rmse:1.289118\n",
      "[284]\ttrain-rmse:1.213734\tval-rmse:1.288309\n",
      "[285]\ttrain-rmse:1.212707\tval-rmse:1.287498\n",
      "[286]\ttrain-rmse:1.211666\tval-rmse:1.286689\n",
      "[287]\ttrain-rmse:1.210666\tval-rmse:1.285912\n",
      "[288]\ttrain-rmse:1.209581\tval-rmse:1.285124\n",
      "[289]\ttrain-rmse:1.208514\tval-rmse:1.284324\n",
      "[290]\ttrain-rmse:1.207499\tval-rmse:1.283555\n",
      "[291]\ttrain-rmse:1.206520\tval-rmse:1.282800\n",
      "[292]\ttrain-rmse:1.205510\tval-rmse:1.282056\n",
      "[293]\ttrain-rmse:1.204504\tval-rmse:1.281297\n",
      "[294]\ttrain-rmse:1.203526\tval-rmse:1.280563\n",
      "[295]\ttrain-rmse:1.202564\tval-rmse:1.279820\n",
      "[296]\ttrain-rmse:1.201562\tval-rmse:1.279090\n",
      "[297]\ttrain-rmse:1.200622\tval-rmse:1.278359\n",
      "[298]\ttrain-rmse:1.199687\tval-rmse:1.277617\n",
      "[299]\ttrain-rmse:1.198777\tval-rmse:1.276904\n",
      "[300]\ttrain-rmse:1.197849\tval-rmse:1.276196\n",
      "[301]\ttrain-rmse:1.196913\tval-rmse:1.275511\n",
      "[302]\ttrain-rmse:1.195991\tval-rmse:1.274799\n",
      "[303]\ttrain-rmse:1.195053\tval-rmse:1.274117\n",
      "[304]\ttrain-rmse:1.194123\tval-rmse:1.273434\n",
      "[305]\ttrain-rmse:1.193208\tval-rmse:1.272782\n",
      "[306]\ttrain-rmse:1.192319\tval-rmse:1.272129\n",
      "[307]\ttrain-rmse:1.191447\tval-rmse:1.271453\n",
      "[308]\ttrain-rmse:1.190541\tval-rmse:1.270820\n",
      "[309]\ttrain-rmse:1.189701\tval-rmse:1.270174\n",
      "[310]\ttrain-rmse:1.188822\tval-rmse:1.269519\n",
      "[311]\ttrain-rmse:1.187918\tval-rmse:1.268891\n",
      "[312]\ttrain-rmse:1.187092\tval-rmse:1.268279\n",
      "[313]\ttrain-rmse:1.186300\tval-rmse:1.267646\n",
      "[314]\ttrain-rmse:1.185424\tval-rmse:1.266994\n",
      "[315]\ttrain-rmse:1.184550\tval-rmse:1.266368\n",
      "[316]\ttrain-rmse:1.183675\tval-rmse:1.265764\n",
      "[317]\ttrain-rmse:1.182847\tval-rmse:1.265155\n",
      "[318]\ttrain-rmse:1.182008\tval-rmse:1.264545\n",
      "[319]\ttrain-rmse:1.181231\tval-rmse:1.263969\n",
      "[320]\ttrain-rmse:1.180366\tval-rmse:1.263414\n",
      "[321]\ttrain-rmse:1.179489\tval-rmse:1.262821\n",
      "[322]\ttrain-rmse:1.178714\tval-rmse:1.262234\n",
      "[323]\ttrain-rmse:1.177992\tval-rmse:1.261683\n",
      "[324]\ttrain-rmse:1.177229\tval-rmse:1.261110\n",
      "[325]\ttrain-rmse:1.176431\tval-rmse:1.260556\n",
      "[326]\ttrain-rmse:1.175624\tval-rmse:1.260013\n",
      "[327]\ttrain-rmse:1.174810\tval-rmse:1.259467\n",
      "[328]\ttrain-rmse:1.174024\tval-rmse:1.258906\n",
      "[329]\ttrain-rmse:1.173255\tval-rmse:1.258345\n",
      "[330]\ttrain-rmse:1.172510\tval-rmse:1.257796\n",
      "[331]\ttrain-rmse:1.171747\tval-rmse:1.257309\n",
      "[332]\ttrain-rmse:1.171021\tval-rmse:1.256795\n",
      "[333]\ttrain-rmse:1.170316\tval-rmse:1.256295\n",
      "[334]\ttrain-rmse:1.169552\tval-rmse:1.255787\n",
      "[335]\ttrain-rmse:1.168832\tval-rmse:1.255272\n",
      "[336]\ttrain-rmse:1.168174\tval-rmse:1.254774\n",
      "[337]\ttrain-rmse:1.167410\tval-rmse:1.254267\n",
      "[338]\ttrain-rmse:1.166715\tval-rmse:1.253776\n",
      "[339]\ttrain-rmse:1.166046\tval-rmse:1.253281\n",
      "[340]\ttrain-rmse:1.165382\tval-rmse:1.252806\n",
      "[341]\ttrain-rmse:1.164740\tval-rmse:1.252330\n",
      "[342]\ttrain-rmse:1.164064\tval-rmse:1.251861\n",
      "[343]\ttrain-rmse:1.163441\tval-rmse:1.251392\n",
      "[344]\ttrain-rmse:1.162749\tval-rmse:1.250951\n",
      "[345]\ttrain-rmse:1.162076\tval-rmse:1.250487\n",
      "[346]\ttrain-rmse:1.161358\tval-rmse:1.250006\n",
      "[347]\ttrain-rmse:1.160607\tval-rmse:1.249561\n",
      "[348]\ttrain-rmse:1.159957\tval-rmse:1.249112\n",
      "[349]\ttrain-rmse:1.159240\tval-rmse:1.248660\n",
      "[350]\ttrain-rmse:1.158580\tval-rmse:1.248232\n",
      "[351]\ttrain-rmse:1.157969\tval-rmse:1.247790\n",
      "[352]\ttrain-rmse:1.157276\tval-rmse:1.247353\n",
      "[353]\ttrain-rmse:1.156633\tval-rmse:1.246923\n",
      "[354]\ttrain-rmse:1.156019\tval-rmse:1.246505\n",
      "[355]\ttrain-rmse:1.155383\tval-rmse:1.246079\n",
      "[356]\ttrain-rmse:1.154745\tval-rmse:1.245666\n",
      "[357]\ttrain-rmse:1.154105\tval-rmse:1.245272\n",
      "[358]\ttrain-rmse:1.153542\tval-rmse:1.244855\n",
      "[359]\ttrain-rmse:1.153007\tval-rmse:1.244451\n",
      "[360]\ttrain-rmse:1.152410\tval-rmse:1.244027\n",
      "[361]\ttrain-rmse:1.151844\tval-rmse:1.243618\n",
      "[362]\ttrain-rmse:1.151223\tval-rmse:1.243225\n",
      "[363]\ttrain-rmse:1.150607\tval-rmse:1.242840\n",
      "[364]\ttrain-rmse:1.149958\tval-rmse:1.242456\n",
      "[365]\ttrain-rmse:1.149393\tval-rmse:1.242069\n",
      "[366]\ttrain-rmse:1.148812\tval-rmse:1.241687\n",
      "[367]\ttrain-rmse:1.148215\tval-rmse:1.241320\n",
      "[368]\ttrain-rmse:1.147645\tval-rmse:1.240953\n",
      "[369]\ttrain-rmse:1.147135\tval-rmse:1.240599\n",
      "[370]\ttrain-rmse:1.146540\tval-rmse:1.240257\n",
      "[371]\ttrain-rmse:1.145929\tval-rmse:1.239902\n",
      "[372]\ttrain-rmse:1.145338\tval-rmse:1.239543\n",
      "[373]\ttrain-rmse:1.144779\tval-rmse:1.239198\n",
      "[374]\ttrain-rmse:1.144277\tval-rmse:1.238853\n",
      "[375]\ttrain-rmse:1.143669\tval-rmse:1.238530\n",
      "[376]\ttrain-rmse:1.143135\tval-rmse:1.238187\n",
      "[377]\ttrain-rmse:1.142546\tval-rmse:1.237849\n",
      "[378]\ttrain-rmse:1.142013\tval-rmse:1.237523\n",
      "[379]\ttrain-rmse:1.141461\tval-rmse:1.237205\n",
      "[380]\ttrain-rmse:1.140918\tval-rmse:1.236884\n",
      "[381]\ttrain-rmse:1.140369\tval-rmse:1.236564\n",
      "[382]\ttrain-rmse:1.139857\tval-rmse:1.236230\n",
      "[383]\ttrain-rmse:1.139271\tval-rmse:1.235925\n",
      "[384]\ttrain-rmse:1.138764\tval-rmse:1.235612\n",
      "[385]\ttrain-rmse:1.138258\tval-rmse:1.235309\n",
      "[386]\ttrain-rmse:1.137780\tval-rmse:1.235031\n",
      "[387]\ttrain-rmse:1.137209\tval-rmse:1.234736\n",
      "[388]\ttrain-rmse:1.136649\tval-rmse:1.234446\n",
      "[389]\ttrain-rmse:1.136091\tval-rmse:1.234144\n",
      "[390]\ttrain-rmse:1.135617\tval-rmse:1.233849\n",
      "[391]\ttrain-rmse:1.135093\tval-rmse:1.233566\n",
      "[392]\ttrain-rmse:1.134644\tval-rmse:1.233269\n",
      "[393]\ttrain-rmse:1.134133\tval-rmse:1.232994\n",
      "[394]\ttrain-rmse:1.133672\tval-rmse:1.232704\n",
      "[395]\ttrain-rmse:1.133153\tval-rmse:1.232406\n",
      "[396]\ttrain-rmse:1.132713\tval-rmse:1.232124\n",
      "[397]\ttrain-rmse:1.132223\tval-rmse:1.231846\n",
      "[398]\ttrain-rmse:1.131723\tval-rmse:1.231588\n",
      "[399]\ttrain-rmse:1.131299\tval-rmse:1.231314\n",
      "[400]\ttrain-rmse:1.130899\tval-rmse:1.231027\n",
      "[401]\ttrain-rmse:1.130364\tval-rmse:1.230758\n",
      "[402]\ttrain-rmse:1.129911\tval-rmse:1.230516\n",
      "[403]\ttrain-rmse:1.129483\tval-rmse:1.230251\n",
      "[404]\ttrain-rmse:1.129067\tval-rmse:1.229984\n",
      "[405]\ttrain-rmse:1.128617\tval-rmse:1.229727\n",
      "[406]\ttrain-rmse:1.128202\tval-rmse:1.229473\n",
      "[407]\ttrain-rmse:1.127745\tval-rmse:1.229203\n",
      "[408]\ttrain-rmse:1.127317\tval-rmse:1.228961\n",
      "[409]\ttrain-rmse:1.126889\tval-rmse:1.228717\n",
      "[410]\ttrain-rmse:1.126484\tval-rmse:1.228465\n",
      "[411]\ttrain-rmse:1.126044\tval-rmse:1.228223\n",
      "[412]\ttrain-rmse:1.125559\tval-rmse:1.227995\n",
      "[413]\ttrain-rmse:1.125151\tval-rmse:1.227747\n",
      "[414]\ttrain-rmse:1.124770\tval-rmse:1.227525\n",
      "[415]\ttrain-rmse:1.124406\tval-rmse:1.227286\n",
      "[416]\ttrain-rmse:1.124022\tval-rmse:1.227065\n",
      "[417]\ttrain-rmse:1.123646\tval-rmse:1.226835\n",
      "[418]\ttrain-rmse:1.123166\tval-rmse:1.226609\n",
      "[419]\ttrain-rmse:1.122744\tval-rmse:1.226383\n",
      "[420]\ttrain-rmse:1.122384\tval-rmse:1.226156\n",
      "[421]\ttrain-rmse:1.122045\tval-rmse:1.225920\n",
      "[422]\ttrain-rmse:1.121525\tval-rmse:1.225725\n",
      "[423]\ttrain-rmse:1.121084\tval-rmse:1.225526\n",
      "[424]\ttrain-rmse:1.120697\tval-rmse:1.225334\n",
      "[425]\ttrain-rmse:1.120339\tval-rmse:1.225106\n",
      "[426]\ttrain-rmse:1.119953\tval-rmse:1.224928\n",
      "[427]\ttrain-rmse:1.119596\tval-rmse:1.224733\n",
      "[428]\ttrain-rmse:1.119252\tval-rmse:1.224528\n",
      "[429]\ttrain-rmse:1.118821\tval-rmse:1.224325\n",
      "[430]\ttrain-rmse:1.118450\tval-rmse:1.224121\n",
      "[431]\ttrain-rmse:1.118098\tval-rmse:1.223928\n",
      "[432]\ttrain-rmse:1.117741\tval-rmse:1.223722\n",
      "[433]\ttrain-rmse:1.117330\tval-rmse:1.223520\n",
      "[434]\ttrain-rmse:1.116919\tval-rmse:1.223339\n",
      "[435]\ttrain-rmse:1.116634\tval-rmse:1.223161\n",
      "[436]\ttrain-rmse:1.116281\tval-rmse:1.222970\n",
      "[437]\ttrain-rmse:1.115944\tval-rmse:1.222787\n",
      "[438]\ttrain-rmse:1.115584\tval-rmse:1.222591\n",
      "[439]\ttrain-rmse:1.115237\tval-rmse:1.222404\n",
      "[440]\ttrain-rmse:1.114823\tval-rmse:1.222234\n",
      "[441]\ttrain-rmse:1.114424\tval-rmse:1.222027\n",
      "[442]\ttrain-rmse:1.114127\tval-rmse:1.221849\n",
      "[443]\ttrain-rmse:1.113813\tval-rmse:1.221644\n",
      "[444]\ttrain-rmse:1.113505\tval-rmse:1.221480\n",
      "[445]\ttrain-rmse:1.113131\tval-rmse:1.221302\n",
      "[446]\ttrain-rmse:1.112777\tval-rmse:1.221134\n",
      "[447]\ttrain-rmse:1.112460\tval-rmse:1.220967\n",
      "[448]\ttrain-rmse:1.112100\tval-rmse:1.220797\n",
      "[449]\ttrain-rmse:1.111702\tval-rmse:1.220648\n",
      "[450]\ttrain-rmse:1.111377\tval-rmse:1.220484\n",
      "[451]\ttrain-rmse:1.111027\tval-rmse:1.220305\n",
      "[452]\ttrain-rmse:1.110707\tval-rmse:1.220133\n",
      "[453]\ttrain-rmse:1.110388\tval-rmse:1.219984\n",
      "[454]\ttrain-rmse:1.110091\tval-rmse:1.219820\n",
      "[455]\ttrain-rmse:1.109695\tval-rmse:1.219662\n",
      "[456]\ttrain-rmse:1.109371\tval-rmse:1.219508\n",
      "[457]\ttrain-rmse:1.109065\tval-rmse:1.219360\n",
      "[458]\ttrain-rmse:1.108725\tval-rmse:1.219193\n",
      "[459]\ttrain-rmse:1.108422\tval-rmse:1.219065\n",
      "[460]\ttrain-rmse:1.108102\tval-rmse:1.218932\n",
      "[461]\ttrain-rmse:1.107704\tval-rmse:1.218786\n",
      "[462]\ttrain-rmse:1.107411\tval-rmse:1.218639\n",
      "[463]\ttrain-rmse:1.107161\tval-rmse:1.218499\n",
      "[464]\ttrain-rmse:1.106820\tval-rmse:1.218356\n",
      "[465]\ttrain-rmse:1.106542\tval-rmse:1.218233\n",
      "[466]\ttrain-rmse:1.106252\tval-rmse:1.218098\n",
      "[467]\ttrain-rmse:1.105902\tval-rmse:1.217968\n",
      "[468]\ttrain-rmse:1.105620\tval-rmse:1.217834\n",
      "[469]\ttrain-rmse:1.105244\tval-rmse:1.217683\n",
      "[470]\ttrain-rmse:1.104919\tval-rmse:1.217543\n",
      "[471]\ttrain-rmse:1.104581\tval-rmse:1.217411\n",
      "[472]\ttrain-rmse:1.104312\tval-rmse:1.217284\n",
      "[473]\ttrain-rmse:1.103964\tval-rmse:1.217152\n",
      "[474]\ttrain-rmse:1.103657\tval-rmse:1.217023\n",
      "[475]\ttrain-rmse:1.103328\tval-rmse:1.216882\n",
      "[476]\ttrain-rmse:1.103034\tval-rmse:1.216768\n",
      "[477]\ttrain-rmse:1.102761\tval-rmse:1.216638\n",
      "[478]\ttrain-rmse:1.102481\tval-rmse:1.216509\n",
      "[479]\ttrain-rmse:1.102140\tval-rmse:1.216394\n",
      "[480]\ttrain-rmse:1.101819\tval-rmse:1.216264\n",
      "[481]\ttrain-rmse:1.101511\tval-rmse:1.216149\n",
      "[482]\ttrain-rmse:1.101203\tval-rmse:1.216012\n",
      "[483]\ttrain-rmse:1.100950\tval-rmse:1.215881\n",
      "[484]\ttrain-rmse:1.100695\tval-rmse:1.215765\n",
      "[485]\ttrain-rmse:1.100394\tval-rmse:1.215648\n",
      "[486]\ttrain-rmse:1.100105\tval-rmse:1.215542\n",
      "[487]\ttrain-rmse:1.099827\tval-rmse:1.215424\n",
      "[488]\ttrain-rmse:1.099518\tval-rmse:1.215321\n",
      "[489]\ttrain-rmse:1.099265\tval-rmse:1.215201\n",
      "[490]\ttrain-rmse:1.099005\tval-rmse:1.215091\n",
      "[491]\ttrain-rmse:1.098727\tval-rmse:1.214977\n",
      "[492]\ttrain-rmse:1.098510\tval-rmse:1.214875\n",
      "[493]\ttrain-rmse:1.098287\tval-rmse:1.214759\n",
      "[494]\ttrain-rmse:1.098046\tval-rmse:1.214659\n",
      "[495]\ttrain-rmse:1.097759\tval-rmse:1.214560\n",
      "[496]\ttrain-rmse:1.097533\tval-rmse:1.214445\n",
      "[497]\ttrain-rmse:1.097341\tval-rmse:1.214354\n",
      "[498]\ttrain-rmse:1.097070\tval-rmse:1.214256\n",
      "[499]\ttrain-rmse:1.096799\tval-rmse:1.214150\n",
      "[500]\ttrain-rmse:1.096573\tval-rmse:1.214056\n",
      "[501]\ttrain-rmse:1.096259\tval-rmse:1.213971\n",
      "[502]\ttrain-rmse:1.096047\tval-rmse:1.213860\n",
      "[503]\ttrain-rmse:1.095789\tval-rmse:1.213780\n",
      "[504]\ttrain-rmse:1.095558\tval-rmse:1.213675\n",
      "[505]\ttrain-rmse:1.095276\tval-rmse:1.213580\n",
      "[506]\ttrain-rmse:1.094976\tval-rmse:1.213482\n",
      "[507]\ttrain-rmse:1.094727\tval-rmse:1.213370\n",
      "[508]\ttrain-rmse:1.094512\tval-rmse:1.213283\n",
      "[509]\ttrain-rmse:1.094299\tval-rmse:1.213207\n",
      "[510]\ttrain-rmse:1.094107\tval-rmse:1.213139\n",
      "[511]\ttrain-rmse:1.093894\tval-rmse:1.213050\n",
      "[512]\ttrain-rmse:1.093663\tval-rmse:1.212935\n",
      "[513]\ttrain-rmse:1.093384\tval-rmse:1.212853\n",
      "[514]\ttrain-rmse:1.093135\tval-rmse:1.212783\n",
      "[515]\ttrain-rmse:1.092910\tval-rmse:1.212697\n",
      "[516]\ttrain-rmse:1.092628\tval-rmse:1.212616\n",
      "[517]\ttrain-rmse:1.092461\tval-rmse:1.212537\n",
      "[518]\ttrain-rmse:1.092271\tval-rmse:1.212471\n",
      "[519]\ttrain-rmse:1.091961\tval-rmse:1.212375\n",
      "[520]\ttrain-rmse:1.091667\tval-rmse:1.212305\n",
      "[521]\ttrain-rmse:1.091431\tval-rmse:1.212242\n",
      "[522]\ttrain-rmse:1.091125\tval-rmse:1.212150\n",
      "[523]\ttrain-rmse:1.090913\tval-rmse:1.212075\n",
      "[524]\ttrain-rmse:1.090654\tval-rmse:1.212003\n",
      "[525]\ttrain-rmse:1.090405\tval-rmse:1.211929\n",
      "[526]\ttrain-rmse:1.090192\tval-rmse:1.211846\n",
      "[527]\ttrain-rmse:1.089942\tval-rmse:1.211778\n",
      "[528]\ttrain-rmse:1.089745\tval-rmse:1.211700\n",
      "[529]\ttrain-rmse:1.089537\tval-rmse:1.211616\n",
      "[530]\ttrain-rmse:1.089302\tval-rmse:1.211541\n",
      "[531]\ttrain-rmse:1.089132\tval-rmse:1.211465\n",
      "[532]\ttrain-rmse:1.088843\tval-rmse:1.211417\n",
      "[533]\ttrain-rmse:1.088638\tval-rmse:1.211335\n",
      "[534]\ttrain-rmse:1.088467\tval-rmse:1.211267\n",
      "[535]\ttrain-rmse:1.088227\tval-rmse:1.211194\n",
      "[536]\ttrain-rmse:1.088023\tval-rmse:1.211135\n",
      "[537]\ttrain-rmse:1.087837\tval-rmse:1.211053\n",
      "[538]\ttrain-rmse:1.087644\tval-rmse:1.210989\n",
      "[539]\ttrain-rmse:1.087437\tval-rmse:1.210926\n",
      "[540]\ttrain-rmse:1.087226\tval-rmse:1.210843\n",
      "[541]\ttrain-rmse:1.086949\tval-rmse:1.210777\n",
      "[542]\ttrain-rmse:1.086734\tval-rmse:1.210700\n",
      "[543]\ttrain-rmse:1.086498\tval-rmse:1.210651\n",
      "[544]\ttrain-rmse:1.086297\tval-rmse:1.210584\n",
      "[545]\ttrain-rmse:1.086107\tval-rmse:1.210523\n",
      "[546]\ttrain-rmse:1.085918\tval-rmse:1.210464\n",
      "[547]\ttrain-rmse:1.085683\tval-rmse:1.210413\n",
      "[548]\ttrain-rmse:1.085467\tval-rmse:1.210351\n",
      "[549]\ttrain-rmse:1.085272\tval-rmse:1.210289\n",
      "[550]\ttrain-rmse:1.085069\tval-rmse:1.210220\n",
      "[551]\ttrain-rmse:1.084836\tval-rmse:1.210172\n",
      "[552]\ttrain-rmse:1.084663\tval-rmse:1.210108\n",
      "[553]\ttrain-rmse:1.084506\tval-rmse:1.210041\n",
      "[554]\ttrain-rmse:1.084237\tval-rmse:1.209993\n",
      "[555]\ttrain-rmse:1.084075\tval-rmse:1.209947\n",
      "[556]\ttrain-rmse:1.083884\tval-rmse:1.209898\n",
      "[557]\ttrain-rmse:1.083663\tval-rmse:1.209859\n",
      "[558]\ttrain-rmse:1.083457\tval-rmse:1.209805\n",
      "[559]\ttrain-rmse:1.083231\tval-rmse:1.209751\n",
      "[560]\ttrain-rmse:1.083047\tval-rmse:1.209679\n",
      "[561]\ttrain-rmse:1.082854\tval-rmse:1.209628\n",
      "[562]\ttrain-rmse:1.082678\tval-rmse:1.209562\n",
      "[563]\ttrain-rmse:1.082497\tval-rmse:1.209511\n",
      "[564]\ttrain-rmse:1.082303\tval-rmse:1.209445\n",
      "[565]\ttrain-rmse:1.082187\tval-rmse:1.209403\n",
      "[566]\ttrain-rmse:1.082009\tval-rmse:1.209351\n",
      "[567]\ttrain-rmse:1.081842\tval-rmse:1.209312\n",
      "[568]\ttrain-rmse:1.081634\tval-rmse:1.209260\n",
      "[569]\ttrain-rmse:1.081410\tval-rmse:1.209218\n",
      "[570]\ttrain-rmse:1.081269\tval-rmse:1.209172\n",
      "[571]\ttrain-rmse:1.081099\tval-rmse:1.209112\n",
      "[572]\ttrain-rmse:1.080848\tval-rmse:1.209060\n",
      "[573]\ttrain-rmse:1.080594\tval-rmse:1.209033\n",
      "[574]\ttrain-rmse:1.080431\tval-rmse:1.208981\n",
      "[575]\ttrain-rmse:1.080210\tval-rmse:1.208940\n",
      "[576]\ttrain-rmse:1.080026\tval-rmse:1.208898\n",
      "[577]\ttrain-rmse:1.079820\tval-rmse:1.208846\n",
      "[578]\ttrain-rmse:1.079677\tval-rmse:1.208803\n",
      "[579]\ttrain-rmse:1.079549\tval-rmse:1.208757\n",
      "[580]\ttrain-rmse:1.079294\tval-rmse:1.208712\n",
      "[581]\ttrain-rmse:1.079160\tval-rmse:1.208666\n",
      "[582]\ttrain-rmse:1.078891\tval-rmse:1.208633\n",
      "[583]\ttrain-rmse:1.078756\tval-rmse:1.208593\n",
      "[584]\ttrain-rmse:1.078599\tval-rmse:1.208546\n",
      "[585]\ttrain-rmse:1.078384\tval-rmse:1.208495\n",
      "[586]\ttrain-rmse:1.078194\tval-rmse:1.208460\n",
      "[587]\ttrain-rmse:1.078012\tval-rmse:1.208426\n",
      "[588]\ttrain-rmse:1.077790\tval-rmse:1.208387\n",
      "[589]\ttrain-rmse:1.077533\tval-rmse:1.208375\n",
      "[590]\ttrain-rmse:1.077390\tval-rmse:1.208346\n",
      "[591]\ttrain-rmse:1.077231\tval-rmse:1.208300\n",
      "[592]\ttrain-rmse:1.076985\tval-rmse:1.208262\n",
      "[593]\ttrain-rmse:1.076833\tval-rmse:1.208218\n",
      "[594]\ttrain-rmse:1.076636\tval-rmse:1.208192\n",
      "[595]\ttrain-rmse:1.076512\tval-rmse:1.208154\n",
      "[596]\ttrain-rmse:1.076277\tval-rmse:1.208120\n",
      "[597]\ttrain-rmse:1.076095\tval-rmse:1.208077\n",
      "[598]\ttrain-rmse:1.075968\tval-rmse:1.208032\n",
      "[599]\ttrain-rmse:1.075782\tval-rmse:1.207994\n",
      "[600]\ttrain-rmse:1.075633\tval-rmse:1.207949\n",
      "[601]\ttrain-rmse:1.075449\tval-rmse:1.207927\n",
      "[602]\ttrain-rmse:1.075348\tval-rmse:1.207893\n",
      "[603]\ttrain-rmse:1.075184\tval-rmse:1.207872\n",
      "[604]\ttrain-rmse:1.074864\tval-rmse:1.207834\n",
      "[605]\ttrain-rmse:1.074747\tval-rmse:1.207799\n",
      "[606]\ttrain-rmse:1.074516\tval-rmse:1.207766\n",
      "[607]\ttrain-rmse:1.074310\tval-rmse:1.207747\n",
      "[608]\ttrain-rmse:1.074124\tval-rmse:1.207735\n",
      "[609]\ttrain-rmse:1.073969\tval-rmse:1.207703\n",
      "[610]\ttrain-rmse:1.073762\tval-rmse:1.207660\n",
      "[611]\ttrain-rmse:1.073547\tval-rmse:1.207638\n",
      "[612]\ttrain-rmse:1.073340\tval-rmse:1.207607\n",
      "[613]\ttrain-rmse:1.073196\tval-rmse:1.207576\n",
      "[614]\ttrain-rmse:1.073012\tval-rmse:1.207539\n",
      "[615]\ttrain-rmse:1.072812\tval-rmse:1.207509\n",
      "[616]\ttrain-rmse:1.072693\tval-rmse:1.207486\n",
      "[617]\ttrain-rmse:1.072502\tval-rmse:1.207458\n",
      "[618]\ttrain-rmse:1.072275\tval-rmse:1.207440\n",
      "[619]\ttrain-rmse:1.072168\tval-rmse:1.207420\n",
      "[620]\ttrain-rmse:1.072010\tval-rmse:1.207397\n",
      "[621]\ttrain-rmse:1.071903\tval-rmse:1.207367\n",
      "[622]\ttrain-rmse:1.071646\tval-rmse:1.207334\n",
      "[623]\ttrain-rmse:1.071430\tval-rmse:1.207297\n",
      "[624]\ttrain-rmse:1.071307\tval-rmse:1.207258\n",
      "[625]\ttrain-rmse:1.071149\tval-rmse:1.207237\n",
      "[626]\ttrain-rmse:1.070988\tval-rmse:1.207209\n",
      "[627]\ttrain-rmse:1.070811\tval-rmse:1.207197\n",
      "[628]\ttrain-rmse:1.070701\tval-rmse:1.207177\n",
      "[629]\ttrain-rmse:1.070554\tval-rmse:1.207148\n",
      "[630]\ttrain-rmse:1.070403\tval-rmse:1.207128\n",
      "[631]\ttrain-rmse:1.070241\tval-rmse:1.207105\n",
      "[632]\ttrain-rmse:1.070107\tval-rmse:1.207087\n",
      "[633]\ttrain-rmse:1.069956\tval-rmse:1.207063\n",
      "[634]\ttrain-rmse:1.069806\tval-rmse:1.207041\n",
      "[635]\ttrain-rmse:1.069707\tval-rmse:1.207009\n",
      "[636]\ttrain-rmse:1.069594\tval-rmse:1.206977\n",
      "[637]\ttrain-rmse:1.069436\tval-rmse:1.206946\n",
      "[638]\ttrain-rmse:1.069230\tval-rmse:1.206948\n",
      "[639]\ttrain-rmse:1.069121\tval-rmse:1.206930\n",
      "[640]\ttrain-rmse:1.068997\tval-rmse:1.206904\n",
      "[641]\ttrain-rmse:1.068856\tval-rmse:1.206875\n",
      "[642]\ttrain-rmse:1.068741\tval-rmse:1.206844\n",
      "[643]\ttrain-rmse:1.068572\tval-rmse:1.206826\n",
      "[644]\ttrain-rmse:1.068483\tval-rmse:1.206809\n",
      "[645]\ttrain-rmse:1.068354\tval-rmse:1.206791\n",
      "[646]\ttrain-rmse:1.068117\tval-rmse:1.206752\n",
      "[647]\ttrain-rmse:1.067944\tval-rmse:1.206747\n",
      "[648]\ttrain-rmse:1.067839\tval-rmse:1.206712\n",
      "[649]\ttrain-rmse:1.067649\tval-rmse:1.206662\n",
      "[650]\ttrain-rmse:1.067530\tval-rmse:1.206666\n",
      "[651]\ttrain-rmse:1.067381\tval-rmse:1.206646\n",
      "[652]\ttrain-rmse:1.067156\tval-rmse:1.206617\n",
      "[653]\ttrain-rmse:1.066975\tval-rmse:1.206612\n",
      "[654]\ttrain-rmse:1.066787\tval-rmse:1.206611\n",
      "[655]\ttrain-rmse:1.066665\tval-rmse:1.206584\n",
      "[656]\ttrain-rmse:1.066566\tval-rmse:1.206564\n",
      "[657]\ttrain-rmse:1.066357\tval-rmse:1.206571\n",
      "[658]\ttrain-rmse:1.066098\tval-rmse:1.206561\n",
      "[659]\ttrain-rmse:1.065975\tval-rmse:1.206539\n",
      "[660]\ttrain-rmse:1.065756\tval-rmse:1.206512\n",
      "[661]\ttrain-rmse:1.065658\tval-rmse:1.206496\n",
      "[662]\ttrain-rmse:1.065551\tval-rmse:1.206472\n",
      "[663]\ttrain-rmse:1.065382\tval-rmse:1.206461\n",
      "[664]\ttrain-rmse:1.065211\tval-rmse:1.206454\n",
      "[665]\ttrain-rmse:1.065072\tval-rmse:1.206457\n",
      "[666]\ttrain-rmse:1.064940\tval-rmse:1.206447\n",
      "[667]\ttrain-rmse:1.064793\tval-rmse:1.206423\n",
      "[668]\ttrain-rmse:1.064635\tval-rmse:1.206410\n",
      "[669]\ttrain-rmse:1.064454\tval-rmse:1.206403\n",
      "[670]\ttrain-rmse:1.064313\tval-rmse:1.206393\n",
      "[671]\ttrain-rmse:1.064202\tval-rmse:1.206376\n",
      "[672]\ttrain-rmse:1.064051\tval-rmse:1.206356\n",
      "[673]\ttrain-rmse:1.063933\tval-rmse:1.206339\n",
      "[674]\ttrain-rmse:1.063768\tval-rmse:1.206330\n",
      "[675]\ttrain-rmse:1.063663\tval-rmse:1.206307\n",
      "[676]\ttrain-rmse:1.063536\tval-rmse:1.206314\n",
      "[677]\ttrain-rmse:1.063353\tval-rmse:1.206314\n",
      "[678]\ttrain-rmse:1.063168\tval-rmse:1.206283\n",
      "[679]\ttrain-rmse:1.062933\tval-rmse:1.206253\n",
      "[680]\ttrain-rmse:1.062837\tval-rmse:1.206243\n",
      "[681]\ttrain-rmse:1.062757\tval-rmse:1.206235\n",
      "[682]\ttrain-rmse:1.062536\tval-rmse:1.206232\n",
      "[683]\ttrain-rmse:1.062409\tval-rmse:1.206214\n",
      "[684]\ttrain-rmse:1.062300\tval-rmse:1.206204\n",
      "[685]\ttrain-rmse:1.062119\tval-rmse:1.206159\n",
      "[686]\ttrain-rmse:1.061994\tval-rmse:1.206158\n",
      "[687]\ttrain-rmse:1.061918\tval-rmse:1.206137\n",
      "[688]\ttrain-rmse:1.061746\tval-rmse:1.206131\n",
      "[689]\ttrain-rmse:1.061556\tval-rmse:1.206127\n",
      "[690]\ttrain-rmse:1.061443\tval-rmse:1.206113\n",
      "[691]\ttrain-rmse:1.061265\tval-rmse:1.206116\n",
      "[692]\ttrain-rmse:1.061174\tval-rmse:1.206103\n",
      "[693]\ttrain-rmse:1.061080\tval-rmse:1.206092\n",
      "[694]\ttrain-rmse:1.060944\tval-rmse:1.206075\n",
      "[695]\ttrain-rmse:1.060875\tval-rmse:1.206063\n",
      "[696]\ttrain-rmse:1.060681\tval-rmse:1.206042\n",
      "[697]\ttrain-rmse:1.060569\tval-rmse:1.206023\n",
      "[698]\ttrain-rmse:1.060366\tval-rmse:1.206026\n",
      "[699]\ttrain-rmse:1.060301\tval-rmse:1.206006\n",
      "[700]\ttrain-rmse:1.060068\tval-rmse:1.206023\n",
      "[701]\ttrain-rmse:1.059944\tval-rmse:1.206018\n",
      "[702]\ttrain-rmse:1.059757\tval-rmse:1.206016\n",
      "[703]\ttrain-rmse:1.059551\tval-rmse:1.206009\n",
      "[704]\ttrain-rmse:1.059508\tval-rmse:1.205991\n",
      "[705]\ttrain-rmse:1.059373\tval-rmse:1.205996\n",
      "[706]\ttrain-rmse:1.059249\tval-rmse:1.205981\n",
      "[707]\ttrain-rmse:1.059040\tval-rmse:1.205971\n",
      "[708]\ttrain-rmse:1.058971\tval-rmse:1.205952\n",
      "[709]\ttrain-rmse:1.058853\tval-rmse:1.205950\n",
      "[710]\ttrain-rmse:1.058645\tval-rmse:1.205947\n",
      "[711]\ttrain-rmse:1.058533\tval-rmse:1.205938\n",
      "[712]\ttrain-rmse:1.058452\tval-rmse:1.205922\n",
      "[713]\ttrain-rmse:1.058323\tval-rmse:1.205904\n",
      "[714]\ttrain-rmse:1.058168\tval-rmse:1.205902\n",
      "[715]\ttrain-rmse:1.058046\tval-rmse:1.205900\n",
      "[716]\ttrain-rmse:1.057941\tval-rmse:1.205891\n",
      "[717]\ttrain-rmse:1.057748\tval-rmse:1.205896\n",
      "[718]\ttrain-rmse:1.057585\tval-rmse:1.205900\n",
      "[719]\ttrain-rmse:1.057440\tval-rmse:1.205870\n",
      "[720]\ttrain-rmse:1.057371\tval-rmse:1.205854\n",
      "[721]\ttrain-rmse:1.057286\tval-rmse:1.205841\n",
      "[722]\ttrain-rmse:1.057132\tval-rmse:1.205817\n",
      "[723]\ttrain-rmse:1.056919\tval-rmse:1.205821\n",
      "[724]\ttrain-rmse:1.056821\tval-rmse:1.205821\n",
      "[725]\ttrain-rmse:1.056707\tval-rmse:1.205801\n",
      "[726]\ttrain-rmse:1.056555\tval-rmse:1.205808\n",
      "[727]\ttrain-rmse:1.056378\tval-rmse:1.205823\n",
      "[728]\ttrain-rmse:1.056293\tval-rmse:1.205806\n",
      "[729]\ttrain-rmse:1.056196\tval-rmse:1.205791\n",
      "[730]\ttrain-rmse:1.056106\tval-rmse:1.205794\n",
      "[731]\ttrain-rmse:1.056005\tval-rmse:1.205810\n",
      "[732]\ttrain-rmse:1.055884\tval-rmse:1.205822\n",
      "[733]\ttrain-rmse:1.055783\tval-rmse:1.205812\n",
      "[734]\ttrain-rmse:1.055729\tval-rmse:1.205804\n",
      "[735]\ttrain-rmse:1.055587\tval-rmse:1.205791\n",
      "[736]\ttrain-rmse:1.055515\tval-rmse:1.205777\n",
      "[737]\ttrain-rmse:1.055352\tval-rmse:1.205765\n",
      "[738]\ttrain-rmse:1.055240\tval-rmse:1.205760\n",
      "[739]\ttrain-rmse:1.055081\tval-rmse:1.205757\n",
      "[740]\ttrain-rmse:1.054968\tval-rmse:1.205764\n",
      "[741]\ttrain-rmse:1.054813\tval-rmse:1.205761\n",
      "[742]\ttrain-rmse:1.054640\tval-rmse:1.205768\n",
      "[743]\ttrain-rmse:1.054432\tval-rmse:1.205770\n",
      "[744]\ttrain-rmse:1.054220\tval-rmse:1.205772\n",
      "[745]\ttrain-rmse:1.054128\tval-rmse:1.205757\n",
      "[746]\ttrain-rmse:1.053946\tval-rmse:1.205740\n",
      "[747]\ttrain-rmse:1.053843\tval-rmse:1.205733\n",
      "[748]\ttrain-rmse:1.053689\tval-rmse:1.205725\n",
      "[749]\ttrain-rmse:1.053625\tval-rmse:1.205718\n",
      "[750]\ttrain-rmse:1.053452\tval-rmse:1.205718\n",
      "[751]\ttrain-rmse:1.053352\tval-rmse:1.205706\n",
      "[752]\ttrain-rmse:1.053280\tval-rmse:1.205687\n",
      "[753]\ttrain-rmse:1.053142\tval-rmse:1.205682\n",
      "[754]\ttrain-rmse:1.053035\tval-rmse:1.205694\n",
      "[755]\ttrain-rmse:1.052961\tval-rmse:1.205677\n",
      "[756]\ttrain-rmse:1.052829\tval-rmse:1.205689\n",
      "[757]\ttrain-rmse:1.052672\tval-rmse:1.205701\n",
      "[758]\ttrain-rmse:1.052638\tval-rmse:1.205694\n",
      "[759]\ttrain-rmse:1.052525\tval-rmse:1.205698\n",
      "[760]\ttrain-rmse:1.052395\tval-rmse:1.205700\n",
      "[761]\ttrain-rmse:1.052331\tval-rmse:1.205684\n",
      "[762]\ttrain-rmse:1.052149\tval-rmse:1.205694\n",
      "[763]\ttrain-rmse:1.052079\tval-rmse:1.205694\n",
      "[764]\ttrain-rmse:1.052000\tval-rmse:1.205691\n",
      "[765]\ttrain-rmse:1.051933\tval-rmse:1.205689\n",
      "[766]\ttrain-rmse:1.051854\tval-rmse:1.205688\n",
      "[767]\ttrain-rmse:1.051754\tval-rmse:1.205696\n",
      "[768]\ttrain-rmse:1.051707\tval-rmse:1.205693\n",
      "[769]\ttrain-rmse:1.051628\tval-rmse:1.205675\n",
      "[770]\ttrain-rmse:1.051418\tval-rmse:1.205652\n",
      "[771]\ttrain-rmse:1.051298\tval-rmse:1.205668\n",
      "[772]\ttrain-rmse:1.051243\tval-rmse:1.205671\n",
      "[773]\ttrain-rmse:1.051021\tval-rmse:1.205665\n",
      "[774]\ttrain-rmse:1.050844\tval-rmse:1.205662\n",
      "[775]\ttrain-rmse:1.050721\tval-rmse:1.205674\n",
      "[776]\ttrain-rmse:1.050574\tval-rmse:1.205678\n",
      "[777]\ttrain-rmse:1.050331\tval-rmse:1.205667\n",
      "[778]\ttrain-rmse:1.050235\tval-rmse:1.205666\n",
      "[779]\ttrain-rmse:1.050125\tval-rmse:1.205669\n",
      "[780]\ttrain-rmse:1.050007\tval-rmse:1.205676\n",
      "[781]\ttrain-rmse:1.049959\tval-rmse:1.205675\n",
      "[782]\ttrain-rmse:1.049844\tval-rmse:1.205687\n",
      "[783]\ttrain-rmse:1.049639\tval-rmse:1.205678\n",
      "[784]\ttrain-rmse:1.049408\tval-rmse:1.205673\n",
      "[785]\ttrain-rmse:1.049201\tval-rmse:1.205676\n",
      "[786]\ttrain-rmse:1.049128\tval-rmse:1.205678\n",
      "[787]\ttrain-rmse:1.049003\tval-rmse:1.205692\n",
      "[788]\ttrain-rmse:1.048920\tval-rmse:1.205686\n",
      "[789]\ttrain-rmse:1.048746\tval-rmse:1.205676\n",
      "[790]\ttrain-rmse:1.048662\tval-rmse:1.205676\n",
      "[791]\ttrain-rmse:1.048569\tval-rmse:1.205680\n",
      "[792]\ttrain-rmse:1.048473\tval-rmse:1.205682\n",
      "[793]\ttrain-rmse:1.048437\tval-rmse:1.205676\n",
      "[794]\ttrain-rmse:1.048263\tval-rmse:1.205691\n",
      "[795]\ttrain-rmse:1.048100\tval-rmse:1.205685\n",
      "[796]\ttrain-rmse:1.047890\tval-rmse:1.205685\n",
      "[797]\ttrain-rmse:1.047663\tval-rmse:1.205677\n",
      "[798]\ttrain-rmse:1.047516\tval-rmse:1.205682\n",
      "[799]\ttrain-rmse:1.047394\tval-rmse:1.205696\n",
      "[800]\ttrain-rmse:1.047286\tval-rmse:1.205692\n",
      "[801]\ttrain-rmse:1.047106\tval-rmse:1.205703\n",
      "[802]\ttrain-rmse:1.047040\tval-rmse:1.205703\n",
      "[803]\ttrain-rmse:1.046931\tval-rmse:1.205723\n",
      "[804]\ttrain-rmse:1.046825\tval-rmse:1.205739\n",
      "[805]\ttrain-rmse:1.046735\tval-rmse:1.205717\n",
      "[806]\ttrain-rmse:1.046636\tval-rmse:1.205709\n",
      "[807]\ttrain-rmse:1.046483\tval-rmse:1.205713\n",
      "[808]\ttrain-rmse:1.046413\tval-rmse:1.205714\n",
      "[809]\ttrain-rmse:1.046318\tval-rmse:1.205715\n",
      "[810]\ttrain-rmse:1.046130\tval-rmse:1.205719\n",
      "[811]\ttrain-rmse:1.046049\tval-rmse:1.205723\n",
      "[812]\ttrain-rmse:1.045863\tval-rmse:1.205723\n",
      "[813]\ttrain-rmse:1.045703\tval-rmse:1.205712\n",
      "[814]\ttrain-rmse:1.045547\tval-rmse:1.205700\n",
      "[815]\ttrain-rmse:1.045373\tval-rmse:1.205710\n",
      "[816]\ttrain-rmse:1.045202\tval-rmse:1.205712\n",
      "[817]\ttrain-rmse:1.045115\tval-rmse:1.205721\n",
      "[818]\ttrain-rmse:1.045038\tval-rmse:1.205724\n",
      "[819]\ttrain-rmse:1.044925\tval-rmse:1.205719\n",
      "[820]\ttrain-rmse:1.044762\tval-rmse:1.205731\n",
      "[821]\ttrain-rmse:1.044616\tval-rmse:1.205732\n",
      "[822]\ttrain-rmse:1.044436\tval-rmse:1.205746\n",
      "[823]\ttrain-rmse:1.044337\tval-rmse:1.205746\n",
      "[824]\ttrain-rmse:1.044234\tval-rmse:1.205741\n",
      "[825]\ttrain-rmse:1.044101\tval-rmse:1.205753\n",
      "[826]\ttrain-rmse:1.043907\tval-rmse:1.205747\n",
      "[827]\ttrain-rmse:1.043821\tval-rmse:1.205746\n",
      "[828]\ttrain-rmse:1.043695\tval-rmse:1.205750\n",
      "[829]\ttrain-rmse:1.043595\tval-rmse:1.205745\n",
      "[830]\ttrain-rmse:1.043574\tval-rmse:1.205736\n",
      "[831]\ttrain-rmse:1.043423\tval-rmse:1.205740\n",
      "[832]\ttrain-rmse:1.043328\tval-rmse:1.205741\n",
      "[833]\ttrain-rmse:1.043200\tval-rmse:1.205747\n",
      "[834]\ttrain-rmse:1.043035\tval-rmse:1.205767\n",
      "[835]\ttrain-rmse:1.042963\tval-rmse:1.205774\n",
      "[836]\ttrain-rmse:1.042798\tval-rmse:1.205764\n",
      "[837]\ttrain-rmse:1.042653\tval-rmse:1.205757\n",
      "[838]\ttrain-rmse:1.042540\tval-rmse:1.205747\n",
      "[839]\ttrain-rmse:1.042409\tval-rmse:1.205736\n",
      "[840]\ttrain-rmse:1.042220\tval-rmse:1.205739\n",
      "[841]\ttrain-rmse:1.042048\tval-rmse:1.205738\n",
      "[842]\ttrain-rmse:1.041990\tval-rmse:1.205732\n",
      "[843]\ttrain-rmse:1.041860\tval-rmse:1.205750\n",
      "[844]\ttrain-rmse:1.041691\tval-rmse:1.205772\n",
      "[845]\ttrain-rmse:1.041626\tval-rmse:1.205771\n",
      "[846]\ttrain-rmse:1.041479\tval-rmse:1.205790\n",
      "[847]\ttrain-rmse:1.041411\tval-rmse:1.205800\n",
      "[848]\ttrain-rmse:1.041301\tval-rmse:1.205808\n",
      "[849]\ttrain-rmse:1.041170\tval-rmse:1.205803\n",
      "[850]\ttrain-rmse:1.041033\tval-rmse:1.205821\n",
      "[851]\ttrain-rmse:1.040885\tval-rmse:1.205834\n",
      "[852]\ttrain-rmse:1.040782\tval-rmse:1.205829\n",
      "[853]\ttrain-rmse:1.040705\tval-rmse:1.205830\n",
      "[854]\ttrain-rmse:1.040661\tval-rmse:1.205823\n",
      "[855]\ttrain-rmse:1.040594\tval-rmse:1.205811\n",
      "[856]\ttrain-rmse:1.040472\tval-rmse:1.205813\n",
      "[857]\ttrain-rmse:1.040378\tval-rmse:1.205821\n",
      "[858]\ttrain-rmse:1.040317\tval-rmse:1.205822\n",
      "[859]\ttrain-rmse:1.040174\tval-rmse:1.205821\n",
      "[860]\ttrain-rmse:1.040106\tval-rmse:1.205822\n",
      "[861]\ttrain-rmse:1.040047\tval-rmse:1.205815\n",
      "[862]\ttrain-rmse:1.039976\tval-rmse:1.205811\n",
      "[863]\ttrain-rmse:1.039845\tval-rmse:1.205805\n",
      "[864]\ttrain-rmse:1.039711\tval-rmse:1.205804\n",
      "[865]\ttrain-rmse:1.039659\tval-rmse:1.205800\n",
      "[866]\ttrain-rmse:1.039558\tval-rmse:1.205801\n",
      "[867]\ttrain-rmse:1.039407\tval-rmse:1.205807\n",
      "[868]\ttrain-rmse:1.039290\tval-rmse:1.205817\n",
      "[869]\ttrain-rmse:1.039068\tval-rmse:1.205824\n",
      "[870]\ttrain-rmse:1.038976\tval-rmse:1.205838\n",
      "[871]\ttrain-rmse:1.038810\tval-rmse:1.205855\n",
      "[872]\ttrain-rmse:1.038738\tval-rmse:1.205840\n",
      "[873]\ttrain-rmse:1.038584\tval-rmse:1.205829\n",
      "[874]\ttrain-rmse:1.038449\tval-rmse:1.205838\n",
      "[875]\ttrain-rmse:1.038328\tval-rmse:1.205828\n",
      "[876]\ttrain-rmse:1.038208\tval-rmse:1.205823\n",
      "[877]\ttrain-rmse:1.038106\tval-rmse:1.205828\n",
      "[878]\ttrain-rmse:1.037970\tval-rmse:1.205840\n",
      "[879]\ttrain-rmse:1.037897\tval-rmse:1.205840\n",
      "[880]\ttrain-rmse:1.037770\tval-rmse:1.205832\n",
      "[881]\ttrain-rmse:1.037567\tval-rmse:1.205861\n",
      "[882]\ttrain-rmse:1.037490\tval-rmse:1.205855\n",
      "[883]\ttrain-rmse:1.037423\tval-rmse:1.205859\n",
      "[884]\ttrain-rmse:1.037337\tval-rmse:1.205865\n",
      "[885]\ttrain-rmse:1.037169\tval-rmse:1.205855\n",
      "[886]\ttrain-rmse:1.037063\tval-rmse:1.205862\n",
      "[887]\ttrain-rmse:1.037007\tval-rmse:1.205860\n",
      "[888]\ttrain-rmse:1.036971\tval-rmse:1.205858\n",
      "[889]\ttrain-rmse:1.036769\tval-rmse:1.205862\n",
      "[890]\ttrain-rmse:1.036636\tval-rmse:1.205882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.301873235152\n",
      "0.318391809956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping. Best iteration:\n",
      "[770]\ttrain-rmse:1.051418\tval-rmse:1.205652\n",
      "\n"
     ]
    }
   ],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"reg:linear\"\n",
    "params[\"eta\"] = 0.005\n",
    "params[\"min_child_weight\"] = 6\n",
    "params[\"subsample\"] = 0.7\n",
    "params[\"colsample_bytree\"] = 0.7\n",
    "# params[\"max_delta_step\"] = 10\n",
    "params[\"silent\"] = 1\n",
    "params[\"max_depth\"] = 9 #7\n",
    "# params[\"gamma\"] = 0\n",
    "\n",
    "\n",
    "plst = list(params.items())\n",
    "\n",
    "max_rounds = 2000\n",
    "# xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(y_stack, n_folds=3,\n",
    "                      shuffle=True,\n",
    "                      random_state=np.random.randint(0,100))\n",
    "train, valid= iter(skf).next()\n",
    "X_train_k = X_stack[train]\n",
    "X_valid_k = X_stack[valid]\n",
    "\n",
    "y_train_k = y_stack[train]\n",
    "y_valid_k = y_stack[valid]\n",
    "\n",
    "\n",
    "#create a train and validation dmatrices \n",
    "xgtrain = xgb.DMatrix(X_train_k, label=y_train_k)\n",
    "xgval = xgb.DMatrix(X_valid_k, label=y_valid_k)\n",
    "\n",
    "#train using early stopping and predict\n",
    "watchlist = [(xgtrain, 'train'),(xgval, 'val')]\n",
    "model = xgb.train(plst, xgtrain, max_rounds, watchlist, early_stopping_rounds=120)\n",
    "\n",
    "preds_val = model.predict(xgval)\n",
    "\n",
    "s = metrics.normalized_gini(y_valid_k, preds_val)\n",
    "\n",
    "print s\n",
    "\n",
    "\n",
    "print metrics.normalized_gini(\n",
    "    y_valid_k, \n",
    "    LinearRegression().fit(X_train_k, y_train_k).predict(X_valid_k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2082]\ttrain-rmse:1.154127\tval-rmse:1.200988\n",
      "\n",
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2759]\ttrain-rmse:1.142831\tval-rmse:1.192744\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3831564061\n",
      "0.38269873267"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2550]\ttrain-rmse:1.146314\tval-rmse:1.196913\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.383180011154"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[3556]\ttrain-rmse:1.130771\tval-rmse:1.191296\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.389076250528"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[3373]\ttrain-rmse:1.133703\tval-rmse:1.192363\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.398500634505"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2452]\ttrain-rmse:1.147736\tval-rmse:1.197065\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.387681762343"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2141]\ttrain-rmse:1.153572\tval-rmse:1.195032\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.376591462306"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[2983]\ttrain-rmse:1.141272\tval-rmse:1.184672\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.4018301357\n"
     ]
    }
   ],
   "source": [
    "# normal\n",
    "from transformers import *\n",
    "\n",
    "params = {}\n",
    "params[\"objective\"] = \"reg:linear\"\n",
    "params[\"eta\"] = 0.005\n",
    "params[\"min_child_weight\"] = 6\n",
    "params[\"subsample\"] = 0.7\n",
    "params[\"colsample_bytree\"] = 0.7\n",
    "# params[\"max_delta_step\"] = 1\n",
    "params[\"silent\"] = 1\n",
    "params[\"max_depth\"] = 5 #7\n",
    "# params[\"scale_pos_weight\"] = 1.0\n",
    "# params[\"gamma\"] = 0\n",
    "\n",
    "\n",
    "plst = list(params.items())\n",
    "\n",
    "max_rounds = 5000\n",
    "# xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(y_binned, n_folds=8,\n",
    "                      shuffle=True,\n",
    "                      random_state=np.random.randint(0,100))\n",
    "qq_scores = []\n",
    "for train, valid in skf:\n",
    "# train, valid= iter(skf).next()\n",
    "    X_train_k = X_train[train]\n",
    "    X_valid_k = X_train[valid]\n",
    "#     y_train_k = GeneralTformer(lambda x: np.log(x)).transform(y_train[train])\n",
    "#     y_valid_k = GeneralTformer(lambda x: np.log(x)).transform(y_train[valid])\n",
    "    \n",
    "    y_train_k = y_binned[train]\n",
    "    y_valid_k = y_binned[valid]\n",
    "    \n",
    "    y_valid_k_orig = y_train[valid]\n",
    "\n",
    "\n",
    "    #create a train and validation dmatrices \n",
    "    xgtrain = xgb.DMatrix(X_train_k, label=y_train_k)\n",
    "    xgval = xgb.DMatrix(X_valid_k, label=y_valid_k)\n",
    "\n",
    "    #train using early stopping and predict\n",
    "    watchlist = [(xgtrain, 'train'),(xgval, 'val')]\n",
    "    model = xgb.train(plst, xgtrain, max_rounds, watchlist, \n",
    "                      early_stopping_rounds=100,\n",
    "                      verbose_eval=False)\n",
    "\n",
    "    preds_val = model.predict(xgval)\n",
    "\n",
    "    s = metrics.normalized_gini(y_valid_k_orig, preds_val)\n",
    "    print s\n",
    "    qq_scores.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-map:1.000000\tval-map:1.000000\n",
      "\n",
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-map:1.000000\tval-map:1.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.311622989463\n",
      "0.356664424286"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-map:1.000000\tval-map:1.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.327004161352"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-map:1.000000\tval-map:1.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.330322489111"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-map:1.000000\tval-map:1.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.301414981868"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-map:1.000000\tval-map:1.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.326850757251"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-map:1.000000\tval-map:1.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.324704630157"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 100 rounds.\n",
      "Stopping. Best iteration:\n",
      "[0]\ttrain-map:1.000000\tval-map:1.000000\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.322748426445\n"
     ]
    }
   ],
   "source": [
    "# rank\n",
    "from transformers import *\n",
    "\n",
    "params = {}\n",
    "params[\"objective\"] = \"rank:pairwise\"\n",
    "params[\"eta\"] = 0.0005\n",
    "# params[\"min_child_weight\"] = 6\n",
    "params[\"subsample\"] = 0.7\n",
    "params[\"colsample_bytree\"] = 0.7\n",
    "# params[\"max_delta_step\"] = 1\n",
    "params[\"silent\"] = 1\n",
    "params[\"max_depth\"] = 5 #7\n",
    "# params[\"scale_pos_weight\"] = 1.0\n",
    "# params[\"gamma\"] = 0\n",
    "\n",
    "\n",
    "plst = list(params.items())\n",
    "\n",
    "max_rounds = 5000\n",
    "# xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(y_binned, n_folds=8,\n",
    "                      shuffle=True,\n",
    "                      random_state=np.random.randint(0,100))\n",
    "qq_scores = []\n",
    "for train, valid in skf:\n",
    "# train, valid= iter(skf).next()\n",
    "    X_train_k = X_train[train]\n",
    "    X_valid_k = X_train[valid]\n",
    "#     y_train_k = GeneralTformer(lambda x: np.log(x)).transform(y_train[train])\n",
    "#     y_valid_k = GeneralTformer(lambda x: np.log(x)).transform(y_train[valid])\n",
    "    \n",
    "    y_train_k = y_binned[train]\n",
    "    y_valid_k = y_binned[valid]\n",
    "    \n",
    "    y_valid_k_orig = y_train[valid]\n",
    "\n",
    "\n",
    "    #create a train and validation dmatrices \n",
    "    xgtrain = xgb.DMatrix(X_train_k, label=y_train_k)\n",
    "    xgval = xgb.DMatrix(X_valid_k, label=y_valid_k)\n",
    "\n",
    "    #train using early stopping and predict\n",
    "    watchlist = [(xgtrain, 'train'),(xgval, 'val')]\n",
    "    model = xgb.train(plst, xgtrain, max_rounds, watchlist, \n",
    "                      early_stopping_rounds=100,\n",
    "                      verbose_eval=False)\n",
    "\n",
    "    preds_val = model.predict(xgval)\n",
    "\n",
    "    s = metrics.normalized_gini(y_valid_k_orig, preds_val)\n",
    "    print s\n",
    "    qq_scores.append(s)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3116229894629838, 0.35666442428583095, 0.32700416135203253, 0.33032248911117384, 0.3014149818678528, 0.32685075725051166, 0.3247046301574321, 0.32274842644538476]\n",
      "0.325166607492\n"
     ]
    }
   ],
   "source": [
    "print qq_scores\n",
    "print np.mean(qq_scores)\n",
    "\n",
    "# X_train\n",
    "\n",
    "# inverse log shit\n",
    "# [0.39597194582097206, 0.4020202827906678, 0.3824776199022992, 0.3900430760258498, 0.35743296939952146, 0.39191686671188475, 0.3482977003794325, 0.381895996324256]\n",
    "# 0.381257057169\n",
    "# [912]\ttrain-rmse:3.202105\tval-rmse:3.772708\n",
    "\n",
    "# X_fa10\n",
    "# [708]\ttrain-rmse:3.526680\tval-rmse:3.874051\n",
    "\n",
    "# np.c_[X_fa10_train, X_train]\n",
    "# [986]\ttrain-rmse:3.106901\tval-rmse:3.801840\n",
    "\n",
    "# X_fa40_train\n",
    "# [1071]\ttrain-rmse:3.103334\tval-rmse:3.839233\n",
    "\n",
    "#np.c_[X_fa40_train, X_train]\n",
    "#[1222]\ttrain-rmse:2.904194\tval-rmse:3.807806\n",
    "\n",
    "\n",
    "# [0.38880619454668014, 0.38172734639160605, 0.37474434984100036]\n",
    "# 0.381759296926\n",
    "\n",
    "# normal 8 fold\n",
    "# [0.3731329094124217, 0.3866396348815107, 0.408353381366699, 0.39934839719669324, 0.3876562009578491, 0.38201529516777405, 0.381116988214003, 0.3726814279504832]\n",
    "# 0.386368029393\n",
    "\n",
    "# inverse log shit\n",
    "# [0.39597194582097206, 0.4020202827906678, 0.3824776199022992, 0.3900430760258498, 0.35743296939952146, 0.39191686671188475, 0.3482977003794325, 0.381895996324256]\n",
    "# 0.381257057169\n",
    "\n",
    "# log\n",
    "# [0.4061604139576095, 0.39974187168297554, 0.3695108303351937, 0.3766801117504891, 0.39626831898568365, 0.39644198930580565, 0.37456796456045366, 0.3822501999865891]\n",
    "# 0.387702712571\n",
    "\n",
    "# stupid np.log(2-1./np.log(x+1))\n",
    "# [0.3738971433322404, 0.3820780018728879, 0.37198882562261215, 0.36405897469707776, 0.3517941599099296, 0.38915260748210284, 0.3889183047847193, 0.37215531550206715]\n",
    "# 0.37425541665\n",
    "\n",
    "# binned\n",
    "# [0.3831564061001914, 0.38269873267014587, 0.3831800111542416, 0.38907625052759365, 0.39850063450478496, 0.387681762343047, 0.3765914623062067, 0.4018301357001798]\n",
    "# 0.387839424413"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.126527400315\n",
      "CPU times: user 807 ms, sys: 693 ms, total: 1.5 s\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "q = cross_val_score(DecisionTreeRegressor(), \n",
    "                    X_train, y_train, \n",
    "                    scoring=metrics.gini_score,\n",
    "                    cv=skf, n_jobs=-1)\n",
    "print np.mean(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Will train until val error hasn't decreased in 30 rounds.\n",
      "[0]\ttrain-merror:0.603477\tval-merror:0.628669\n",
      "[1]\ttrain-merror:0.595741\tval-merror:0.627140\n",
      "[2]\ttrain-merror:0.593329\tval-merror:0.625846\n",
      "[3]\ttrain-merror:0.590652\tval-merror:0.626081\n",
      "[4]\ttrain-merror:0.586240\tval-merror:0.625257\n",
      "[5]\ttrain-merror:0.581328\tval-merror:0.624551\n",
      "[6]\ttrain-merror:0.580005\tval-merror:0.623375\n",
      "[7]\ttrain-merror:0.579181\tval-merror:0.622493\n",
      "[8]\ttrain-merror:0.575828\tval-merror:0.623022\n",
      "[9]\ttrain-merror:0.571975\tval-merror:0.623610\n",
      "[10]\ttrain-merror:0.569739\tval-merror:0.622963\n",
      "[11]\ttrain-merror:0.566739\tval-merror:0.623963\n",
      "[12]\ttrain-merror:0.564504\tval-merror:0.622787\n",
      "[13]\ttrain-merror:0.560915\tval-merror:0.623610\n",
      "[14]\ttrain-merror:0.557445\tval-merror:0.623375\n",
      "[15]\ttrain-merror:0.554680\tval-merror:0.623846\n",
      "[16]\ttrain-merror:0.552591\tval-merror:0.624610\n",
      "[17]\ttrain-merror:0.551003\tval-merror:0.623728\n",
      "[18]\ttrain-merror:0.548621\tval-merror:0.625316\n",
      "[19]\ttrain-merror:0.545826\tval-merror:0.624551\n",
      "[20]\ttrain-merror:0.543355\tval-merror:0.625257\n",
      "[21]\ttrain-merror:0.541032\tval-merror:0.625493\n",
      "[22]\ttrain-merror:0.538855\tval-merror:0.625375\n",
      "[23]\ttrain-merror:0.536473\tval-merror:0.625728\n",
      "[24]\ttrain-merror:0.535031\tval-merror:0.625022\n",
      "[25]\ttrain-merror:0.531825\tval-merror:0.623904\n",
      "[26]\ttrain-merror:0.530796\tval-merror:0.623846\n",
      "[27]\ttrain-merror:0.528825\tval-merror:0.623669\n",
      "[28]\ttrain-merror:0.527266\tval-merror:0.623316\n",
      "[29]\ttrain-merror:0.524766\tval-merror:0.623846\n",
      "[30]\ttrain-merror:0.521884\tval-merror:0.623963\n",
      "[31]\ttrain-merror:0.519913\tval-merror:0.624316\n",
      "[32]\ttrain-merror:0.518178\tval-merror:0.623904\n",
      "[33]\ttrain-merror:0.514766\tval-merror:0.623552\n",
      "[34]\ttrain-merror:0.513883\tval-merror:0.622669\n",
      "[35]\ttrain-merror:0.511177\tval-merror:0.622199\n",
      "[36]\ttrain-merror:0.508736\tval-merror:0.623022\n",
      "[37]\ttrain-merror:0.507000\tval-merror:0.621728\n",
      "[38]\ttrain-merror:0.504677\tval-merror:0.622258\n",
      "[39]\ttrain-merror:0.503177\tval-merror:0.621493\n",
      "[40]\ttrain-merror:0.501765\tval-merror:0.621375\n",
      "[41]\ttrain-merror:0.500000\tval-merror:0.621963\n",
      "[42]\ttrain-merror:0.498294\tval-merror:0.621905\n",
      "[43]\ttrain-merror:0.495970\tval-merror:0.621434\n",
      "[44]\ttrain-merror:0.494323\tval-merror:0.622787\n",
      "[45]\ttrain-merror:0.491794\tval-merror:0.622905\n",
      "[46]\ttrain-merror:0.490999\tval-merror:0.622905\n",
      "[47]\ttrain-merror:0.488235\tval-merror:0.623257\n",
      "[48]\ttrain-merror:0.485470\tval-merror:0.623434\n",
      "[49]\ttrain-merror:0.482146\tval-merror:0.623081\n",
      "[50]\ttrain-merror:0.480911\tval-merror:0.622493\n",
      "[51]\ttrain-merror:0.478028\tval-merror:0.623022\n",
      "[52]\ttrain-merror:0.476646\tval-merror:0.622493\n",
      "[53]\ttrain-merror:0.475322\tval-merror:0.623552\n",
      "[54]\ttrain-merror:0.473851\tval-merror:0.624434\n",
      "[55]\ttrain-merror:0.471087\tval-merror:0.625199\n",
      "[56]\ttrain-merror:0.469969\tval-merror:0.625904\n",
      "[57]\ttrain-merror:0.468086\tval-merror:0.625140\n",
      "[58]\ttrain-merror:0.465792\tval-merror:0.624904\n",
      "[59]\ttrain-merror:0.463763\tval-merror:0.624257\n",
      "[60]\ttrain-merror:0.461527\tval-merror:0.623728\n",
      "[61]\ttrain-merror:0.459880\tval-merror:0.623963\n",
      "[62]\ttrain-merror:0.459262\tval-merror:0.624434\n",
      "[63]\ttrain-merror:0.457556\tval-merror:0.624316\n",
      "[64]\ttrain-merror:0.455792\tval-merror:0.625434\n",
      "[65]\ttrain-merror:0.453850\tval-merror:0.626022\n",
      "[66]\ttrain-merror:0.452938\tval-merror:0.625728\n",
      "[67]\ttrain-merror:0.451468\tval-merror:0.626610\n",
      "[68]\ttrain-merror:0.450938\tval-merror:0.626728\n",
      "[69]\ttrain-merror:0.448497\tval-merror:0.626022\n",
      "[70]\ttrain-merror:0.446379\tval-merror:0.625846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.191175929337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stopping. Best iteration:\n",
      "[40]\ttrain-merror:0.501765\tval-merror:0.621375\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# gbm categorical\n",
    "\n",
    "params = {}\n",
    "params[\"objective\"] = \"multi:softmax\"\n",
    "params[\"num_class\"] = len(np.unique(y_binned))\n",
    "params[\"eta\"] = 0.3\n",
    "params[\"min_child_weight\"] = 6\n",
    "params[\"subsample\"] = 1.0\n",
    "params[\"colsample_bytree\"] = 1.0\n",
    "# params[\"max_delta_step\"] = 10\n",
    "params[\"silent\"] = 1\n",
    "params[\"max_depth\"] = 6 #7\n",
    "# params[\"gamma\"] = 0\n",
    "\n",
    "\n",
    "plst = list(params.items())\n",
    "\n",
    "max_rounds = 2000\n",
    "# xgtest = xgb.DMatrix(X_test)\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(y_binned, n_folds=3,\n",
    "                      shuffle=True,\n",
    "                      random_state=np.random.randint(0,100))\n",
    "train, valid= iter(skf).next()\n",
    "X_train_k = X_train[train]\n",
    "X_valid_k = X_train[valid]\n",
    "y_train_k = (y_binned-1)[train]\n",
    "y_valid_k = (y_binned-1)[valid]\n",
    "\n",
    "\n",
    "# create a train and validation dmatrices \n",
    "xgtrain = xgb.DMatrix(X_train_k, label=y_train_k)\n",
    "xgval = xgb.DMatrix(X_valid_k, label=y_valid_k)\n",
    "\n",
    "# train using early stopping and predict\n",
    "watchlist = [(xgtrain, 'train'),(xgval, 'val')]\n",
    "model = xgb.train(plst, xgtrain, max_rounds, watchlist, early_stopping_rounds=30)\n",
    "\n",
    "preds_val = model.predict(xgval)\n",
    "\n",
    "s = metrics.normalized_gini(y_valid_k, preds_val)\n",
    "print s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_binned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "reload(rpfnn)\n",
    "clf = rpfnn.rpfnn(leaf_size=50, no_trees=10, num_neighbors=10)\n",
    "\n",
    "param_dist = {\"leaf_size\": sp_randint(20, 500),\n",
    "              \"no_trees\": sp_randint(10, 400),\n",
    "              \"num_neighbors\": sp_randint(1, 200),\n",
    "             }\n",
    "\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, \n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,\n",
    "                                   cv=skf,\n",
    "                                   scoring=metrics.gini_score,\n",
    "                                   n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print random_search.best_score_\n",
    "print random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Param Finding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV, RandomizedSearchCV\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "k = 3\n",
    "skf = StratifiedKFold(y_binned, n_folds=k,\n",
    "                          shuffle=True,\n",
    "                          random_state=np.random.randint(0,100))\n",
    "\n",
    "# clf = RandomForestRegressor()\n",
    "clf = ExtraTreesRegressor()\n",
    "\n",
    "\n",
    "param_dist = {\"n_estimators\": sp_randint(10, 200),\n",
    "              \"max_features\": ['auto', 'sqrt', 'log2'],\n",
    "              \"max_depth\": [None] + range(1, 15),\n",
    "              \"min_samples_split\": sp_randint(2, 10),\n",
    "              \"min_samples_leaf\": sp_randint(1, 10),\n",
    "              \"min_weight_fraction_leaf\": sp_uniform(0,0.5),\n",
    "              \"max_leaf_nodes\": [None] + range(2, 20),\n",
    "              \"bootstrap\": [True, False],\n",
    "             }\n",
    "\n",
    "\n",
    "\n",
    "n_iter_search = 256\n",
    "random_search = RandomizedSearchCV(clf, \n",
    "                                   param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search,\n",
    "                                   cv=skf,\n",
    "                                   scoring=metrics.gini_score,\n",
    "                                   n_jobs=-1,\n",
    "                                   verbose=2\n",
    "                                  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "toc = time() - tic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Time:', toc\n",
    "print random_search.best_score_\n",
    "print random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF:\n",
    "Time: 4913.59136486\n",
    "0.348998319125\n",
    "{'max_leaf_nodes': None, 'bootstrap': False, 'min_samples_leaf': 8, 'n_estimators': 135, 'min_samples_split': 8, 'min_weight_fraction_leaf': 0.1810718592717216, 'max_features': 'sqrt', 'max_depth': 12}\n",
    "\n",
    "ET:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tic = time()\n",
    "stack.extra_data['y_binned'] = y_binned\n",
    "stack.fit(X_train, y_train)\n",
    "toc = time() - tic\n",
    "\n",
    "print toc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds = stack.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "preds_pd = pd.DataFrame({\"Id\": test_ind, \"Hazard\": preds})\n",
    "preds_pd = preds_pd.set_index('Id')\n",
    "preds_pd.to_csv('submissions/stack_random_crap.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
